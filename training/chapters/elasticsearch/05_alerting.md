---
layout: cover
---

# Syst√®mes d'Alertes

Surveillance proactive et notifications automatiques avec Elasticsearch Alerting

---

# Objectifs d'Apprentissage

√Ä la fin de cette section, vous serez capable de:

- Comprendre les m√©canismes d'alerting d'Elasticsearch (Watcher et Kibana Rules)
- Cr√©er des alertes bas√©es sur des conditions de cluster, m√©triques, ou requ√™tes
- Configurer des actions d'alerte (email, webhook, Slack, index)
- Utiliser les dashboards de monitoring Kibana pour g√©rer les alertes actives

---

# Pourquoi l'Alerting est Critique

L'alerting proactif permet de d√©tecter et r√©agir aux probl√®mes avant qu'ils n'impactent les utilisateurs.

**Sc√©narios d'alerting courants**:
- üî¥ Cluster passe en statut RED (perte de donn√©es)
- üü° Cluster passe en statut YELLOW (perte de HA)
- üíæ Utilisation disque >85% (watermark LOW)
- üß† Heap JVM >85% (risque OutOfMemoryError)
- üìä Taux d'erreur >5% (d√©gradation service)
- ‚è±Ô∏è Latence p95 >1s (exp√©rience utilisateur)
- üö´ Thread pool rejections >100/min (surcharge)

---

# Pourquoi l'Alerting est Critique

**B√©n√©fices**:
- ‚úÖ D√©tection pr√©coce des incidents
- ‚úÖ R√©duction du MTTR (Mean Time To Recovery)
- ‚úÖ Pr√©vention des pannes majeures
- ‚úÖ Conformit√© aux SLA

---

# Elasticsearch Alerting: Deux Solutions

Elasticsearch propose deux syst√®mes d'alerting compl√©mentaires.

## 1. Watcher (Elasticsearch Alerting API)

**Caract√©ristiques**:
- Bas√© sur des requ√™tes Elasticsearch (DSL JSON)
- Ex√©cution programm√©e (schedule)
- Tr√®s flexible et puissant
- Configuration via API REST

**Cas d'usage**: Alertes complexes bas√©es sur agr√©gations, transformations de donn√©es, logique m√©tier avanc√©e.

---

# Elasticsearch Alerting: Deux Solutions

## 2. Kibana Rules & Connectors

**Caract√©ristiques**:
- Interface graphique dans Kibana
- Rule types pr√©d√©finis (Elasticsearch query, index threshold, etc.)
- Int√©gration avec Kibana Stack Monitoring
- Plus simple √† configurer

**Cas d'usage**: Alertes standard sur m√©triques Elasticsearch, logs, APM.

**Recommendation**: Utilisez Kibana Rules pour les cas simples, Watcher pour les cas complexes.

---

# Anatomie d'une Alerte Watcher

Une [alerte Watcher](https://www.elastic.co/guide/en/elasticsearch/reference/current/xpack-alerting.html) se compose de 5 √©l√©ments principaux.

```
Watch = Trigger + Input + Condition + Transform + Actions
```

**1. Trigger** (quand ex√©cuter):
```json
"trigger": {
  "schedule": {
    "interval": "1m"  // Toutes les minutes
  }
}
```

**2. Input** (collecter donn√©es):
```json
"input": {
  "search": {
    "request": {
      "indices": ["logs-*"],
      "body": {
        "query": { "range": { "@timestamp": { "gte": "now-5m" }}}
      }
    }
  }
}
```

---

# Anatomie d'une Alerte Watcher

**3. Condition** (√©valuer si alerte):
```json
"condition": {
  "compare": {
    "ctx.payload.hits.total": {
      "gte": 100  // Si ‚â•100 erreurs
    }
  }
}
```

**4. Transform** (optionnel, transformer donn√©es):
```json
"transform": {
  "script": "return ['error_count': ctx.payload.hits.total]"
}
```

---

# Anatomie d'une Alerte Watcher

**5. Actions** (notifier):
```json
"actions": {
  "send_email": {
    "email": {
      "to": "ops@company.com",
      "subject": "Erreurs d√©tect√©es: {{ctx.payload.hits.total}}"
    }
  }
}
```

---

# Trigger: Planification d'Ex√©cution

Le [trigger](https://www.elastic.co/guide/en/elasticsearch/reference/current/trigger.html) d√©finit quand la watch est ex√©cut√©e.

**Schedule types**:

**1. Interval** (p√©riodique):
```json
"trigger": {
  "schedule": {
    "interval": "30s"  // 30s, 5m, 1h, 1d
  }
}
```

**2. Cron** (horaires sp√©cifiques):
```json
"trigger": {
  "schedule": {
    "cron": "0 0 12 * * ?"  // Tous les jours √† midi
  }
}
```

---

# Trigger: Planification d'Ex√©cution

**3. Hourly** (toutes les heures):
```json
"trigger": {
  "schedule": {
    "hourly": {
      "minute": [0, 30]  // √Ä xx:00 et xx:30
    }
  }
}
```

**4. Daily** (tous les jours):
```json
"trigger": {
  "schedule": {
    "daily": {
      "at": ["08:00", "20:00"]  // √Ä 8h et 20h
    }
  }
}
```

---

# Input: Collecte de Donn√©es

L'[input](https://www.elastic.co/guide/en/elasticsearch/reference/current/input.html) r√©cup√®re les donn√©es √† analyser pour l'alerte.

**Types d'input principaux**:

**1. Search Input** (requ√™te Elasticsearch):
```json
"input": {
  "search": {
    "request": {
      "indices": ["logs-*"],
      "body": {
        "query": {
          "bool": {
            "must": [
              { "range": { "@timestamp": { "gte": "now-5m" }}},
              { "term": { "level": "error" }}
            ]
          }
        },
        "aggs": {
          "error_count": {
            "value_count": { "field": "message.keyword" }
          }
        }
      }
    }
  }
}
```

---

# Input: Collecte de Donn√©es

**2. HTTP Input** (API externe):
```json
"input": {
  "http": {
    "request": {
      "url": "https://api.example.com/metrics",
      "method": "GET",
      "headers": {
        "Authorization": "Bearer {{ctx.metadata.token}}"
      }
    }
  }
}
```

---

# Input: Collecte de Donn√©es

**3. Chain Input** (combiner plusieurs inputs):
```json
"input": {
  "chain": {
    "inputs": [
      { "first": { "search": {...}}},
      { "second": { "http": {...}}}
    ]
  }
}
```

---

# Condition: √âvaluation de l'Alerte

La [condition](https://www.elastic.co/guide/en/elasticsearch/reference/current/condition.html) d√©termine si les actions doivent √™tre d√©clench√©es.

**Types de conditions**:

**1. Compare** (comparaison simple):
```json
"condition": {
  "compare": {
    "ctx.payload.hits.total": {
      "gte": 100
    }
  }
}
```

---

# Condition: √âvaluation de l'Alerte

**2. Array Compare** (tous/au moins un √©l√©ment):
```json
"condition": {
  "array_compare": {
    "ctx.payload.aggregations.nodes.buckets": {
      "path": "heap_percent",
      "gte": {
        "value": 85
      }
    }
  }
}
```

---

# Condition: √âvaluation de l'Alerte

**3. Script** (logique personnalis√©e):
```json
"condition": {
  "script": {
    "source": "return ctx.payload.hits.total > 100 && ctx.payload.aggregations.error_count.value > 50"
  }
}
```

**4. Always** (toujours d√©clencher):
```json
"condition": {
  "always": {}
}
```

**5. Never** (d√©sactiver temporairement):
```json
"condition": {
  "never": {}
}
```

---

# Condition: √âvaluation de l'Alerte

**Context variables**:
- `ctx.trigger.scheduled_time`: Heure pr√©vue d'ex√©cution
- `ctx.execution_time`: Heure r√©elle d'ex√©cution
- `ctx.payload`: Donn√©es de l'input
- `ctx.metadata`: M√©tadonn√©es de la watch

---

# Actions: Notifications et R√©ponses

Les [actions](https://www.elastic.co/guide/en/elasticsearch/reference/current/actions.html) d√©finissent la r√©ponse quand une alerte se d√©clenche.

**Types d'actions**:

**1. Email** (notification par email):
```json
"actions": {
  "send_email": {
    "email": {
      "to": ["ops@company.com", "oncall@company.com"],
      "subject": "ALERT: Cluster {{ctx.metadata.cluster}} - Heap >85%",
      "body": {
        "text": "Heap usage: {{ctx.payload.aggregations.avg_heap.value}}%\nNodes affected: {{ctx.payload.hits.total}}"
      }
    }
  }
}
```

---

# Actions: Notifications et R√©ponses

**2. Webhook** (HTTP POST vers API externe):
```json
"actions": {
  "notify_slack": {
    "webhook": {
      "url": "https://hooks.slack.com/services/YOUR/WEBHOOK/URL",
      "method": "POST",
      "body": "{\"text\":\"Heap alert: {{ctx.payload.aggregations.avg_heap.value}}%\"}"
    }
  }
}
```

**3. Index** (indexer l'alerte dans Elasticsearch):
```json
"actions": {
  "log_alert": {
    "index": {
      "index": "alerts-history",
      "doc_id": "{{ctx.watch_id}}-{{ctx.execution_time}}"
    }
  }
}
```

---

# Actions: Notifications et R√©ponses

**4. Logging** (√©crire dans les logs Elasticsearch):
```json
"actions": {
  "log_action": {
    "logging": {
      "level": "error",
      "text": "Heap alert triggered: {{ctx.payload.aggregations.avg_heap.value}}%"
    }
  }
}
```

**Throttling**: √âviter les alertes en rafale:
```json
"throttle_period": "15m"  // Max 1 alerte toutes les 15min
```

---

# Exemple Complet: Alerte Heap >85%

**Watch compl√®te** pour surveiller le heap JVM:

```json
PUT _watcher/watch/heap_usage_alert
{
  "metadata": {
    "cluster": "production",
    "threshold": 85
  },
  "trigger": {
    "schedule": {
      "interval": "1m"
    }
  },
  "input": {
    "search": {
      "request": {
        "indices": [".monitoring-es-*"],
        "body": {
          "query": {
            "bool": {
              "must": [
                { "range": { "@timestamp": { "gte": "now-2m" }}},
                { "range": { "node_stats.jvm.mem.heap_used_percent": { "gte": 85 }}}
              ]
            }
          },
          "aggs": {
            "nodes_high_heap": {
              "terms": {
                "field": "node_stats.node_id",
                "size": 10
              },
              "aggs": {
                "avg_heap": {
                  "avg": { "field": "node_stats.jvm.mem.heap_used_percent" }
                }
              }
            }
          }
        }
      }
    }
  },
  "condition": {
    "compare": {
      "ctx.payload.hits.total": {
        "gt": 0
      }
    }
  },
  "actions": {
    "notify_ops": {
      "email": {
        "to": "ops@company.com",
        "subject": "[CRITICAL] Heap usage >85% on {{ctx.payload.aggregations.nodes_high_heap.buckets.size}} nodes",
        "body": {
          "text": "Heap alert triggered at {{ctx.execution_time}}.\n\nAffected nodes:\n{{#ctx.payload.aggregations.nodes_high_heap.buckets}}- Node {{key}}: {{avg_heap.value}}%\n{{/ctx.payload.aggregations.nodes_high_heap.buckets}}"
        }
      },
      "throttle_period": "15m"
    },
    "log_to_index": {
      "index": {
        "index": "watcher-alerts",
        "doc_id": "{{ctx.watch_id}}-{{ctx.execution_time}}"
      }
    }
  }
}
```

**Tester la watch**:
```bash
POST _watcher/watch/heap_usage_alert/_execute
```

---

# Kibana Rules: Alerting Simplifi√©

Les [Kibana Rules](https://www.elastic.co/guide/en/kibana/current/alerting-getting-started.html) offrent une interface graphique pour cr√©er des alertes.

**Acc√®s**: Kibana ‚Üí Stack Management ‚Üí Rules and Connectors

**Rule types disponibles**:

| Rule Type | Usage | Exemple |
|-----------|-------|---------|
| **Elasticsearch query** | Alerte bas√©e sur requ√™te ES | Nombre d'erreurs >100 |
| **Index threshold** | Seuil sur agr√©gation | Moyenne heap >85% |
| **ES query** | Requ√™te ES avec condition | Documents manquants |
| **Metrics threshold** | Seuils sur m√©triques APM/infra | CPU >80% |

---

# Kibana Rules: Alerting Simplifi√©

**Workflow de cr√©ation**:
1. **Define rule**: Nom, type, query/condition
2. **Add connector**: Slack, email, PagerDuty, webhook
3. **Configure action**: Message template, variables
4. **Set schedule**: Interval d'√©valuation (1m, 5m, etc.)
5. **Save and enable**: Activer la rule

**Avantages vs Watcher**:
- ‚úÖ Interface graphique (pas de JSON)
- ‚úÖ Int√©gration native avec Kibana visualizations
- ‚úÖ Gestion centralis√©e des connectors (r√©utilisables)
- ‚ö†Ô∏è Moins flexible que Watcher pour logique complexe

---

# Connectors: Int√©grations Externes

Les [Connectors](https://www.elastic.co/guide/en/kibana/current/action-types.html) permettent d'envoyer des notifications vers des syst√®mes externes.

**Connectors populaires**:

**1. Slack**:
```
- Type: Webhook
- URL: https://hooks.slack.com/services/YOUR/WEBHOOK
- Message: Utilise Markdown Slack
```

**2. Email (SMTP)**:
```
- Host: smtp.gmail.com
- Port: 587
- Username/Password: Credentials SMTP
- From: alerts@company.com
```

**3. PagerDuty**:
```
- Integration Key: Cl√© API PagerDuty
- Severity: critical, error, warning, info
```

**4. Webhook (g√©n√©rique)**:
```
- URL: API externe
- Method: POST, PUT, etc.
- Headers: Authorization, Content-Type
- Body: JSON payload
```

**5. Index** (Elasticsearch):
```
- Index: alerts-history-*
- Document: JSON avec d√©tails alerte
```

**Configuration**: Stack Management ‚Üí Rules and Connectors ‚Üí Connectors ‚Üí Create connector

**Best practice**: Cr√©ez un connector par type de notification (1 pour Slack prod, 1 pour Slack dev, etc.)

---

# Kibana Stack Monitoring Alerts

Kibana propose des [alertes pr√©d√©finies pour Stack Monitoring](https://www.elastic.co/guide/en/kibana/current/kibana-alerts.html).

**Alertes disponibles** (pr√™tes √† l'emploi):

| Alerte | Condition | Criticit√© |
|--------|-----------|-----------|
| **Cluster health** | Status = yellow ou red | Critique |
| **Node disk usage** | Disk >80% | Warning |
| **CPU usage** | CPU >95% pour 5min | Warning |
| **Memory usage (JVM)** | Heap >85% pour 5min | Critique |
| **Missing monitoring data** | Pas de donn√©es >15min | Warning |
| **License expiration** | License expire <30j | Warning |
| **Large shard size** | Shard >50GB | Info |

**Activation**:
1. Stack Management ‚Üí Rules and Connectors
2. Cliquez sur "Create rule"
3. S√©lectionnez "Elasticsearch health" ou autre rule type
4. Configurez les seuils et connector
5. Save and enable

**Alert history**: Kibana ‚Üí Stack Monitoring ‚Üí Alerts

**Visualisation**:
- üî¥ Alertes actives (currently firing)
- üü¢ Alertes r√©cup√©r√©es (recovered)
- ‚ö™ Alertes en erreur (execution error)

---

# Gestion du Cycle de Vie des Alertes

**√âtats d'une alerte**:
1. **Active** (firing): Condition remplie, actions d√©clench√©es
2. **OK** (recovered): Condition revenue √† la normale
3. **Pending**: En cours d'√©valuation
4. **Error**: Erreur d'ex√©cution (query invalide, connector indisponible)

**Historique des alertes**:
```bash
GET .watcher-history-*/_search
{
  "query": {
    "match": {
      "watch_id": "heap_usage_alert"
    }
  },
  "sort": [{ "@timestamp": "desc" }],
  "size": 100
}
```

**D√©sactiver temporairement**:
```bash
PUT _watcher/watch/heap_usage_alert/_deactivate
# R√©activer:
PUT _watcher/watch/heap_usage_alert/_activate
```

---

# Gestion du Cycle de Vie des Alertes

**Supprimer une watch**:
```bash
DELETE _watcher/watch/heap_usage_alert
```

**Monitoring des watches**:
```bash
GET _watcher/stats
{
  "watcher_state": "started",
  "watch_count": 10,
  "execution_thread_pool": {
    "queue_size": 0,
    "max_size": 1000
  }
}
```

---

# Best Practices d'Alerting

**1. √âviter l'Alert Fatigue**:
- ‚ö†Ô∏è Trop d'alertes ‚Üí √©quipe les ignore
- ‚úÖ Alertez uniquement sur les m√©triques critiques
- ‚úÖ Utilisez throttling (15-30min minimum)
- ‚úÖ Groupez les alertes similaires

**2. Actionnable**:
- ‚ùå "Cluster unhealthy" (trop vague)
- ‚úÖ "Heap >85% on node-1, consider scaling or optimizing queries"

**3. Contexte**:
- ‚úÖ Incluez des liens vers dashboards Kibana
- ‚úÖ Incluez des commandes de diagnostic
- ‚úÖ Incluez l'historique (trend)

**4. Escalation**:
```
1. Warning (>75%) ‚Üí Log + Slack
2. Critical (>85%) ‚Üí Email ops + PagerDuty
3. Emergency (>95%) ‚Üí PagerDuty + Appel t√©l√©phonique
```

**5. Testing**:
- ‚úÖ Testez chaque alerte avec `_execute`
- ‚úÖ Validez que les connectors fonctionnent
- ‚úÖ V√©rifiez les templates de message

**6. Documentation**:
- ‚úÖ Playbook pour chaque alerte (que faire quand elle se d√©clenche)
- ‚úÖ Fr√©quence attendue (si alerte fr√©quente = normal ou probl√®me)

---

# R√©sum√©

## Points Cl√©s

- Les **syst√®mes d'alerting** (Watcher et Kibana Rules) permettent la surveillance proactive du cluster
- Une **watch Watcher** se compose de: trigger, input, condition, transform, actions
- Les **Kibana Rules** offrent une interface graphique pour des alertes simples
- Les **connectors** (Slack, email, PagerDuty, webhook) permettent des notifications externes
- Les **Stack Monitoring alerts** fournissent des alertes pr√©d√©finies pour Elasticsearch
- **Best practices**: √©viter alert fatigue, rendre les alertes actionnables, documenter les playbooks

## Formules et Exemples

**Trigger cron**: `"0 0 12 * * ?"` = Tous les jours √† midi
**Condition seuil**: `"ctx.payload.hits.total": {"gte": 100}` = Si ‚â•100 r√©sultats
**Throttling**: `"throttle_period": "15m"` = Max 1 alerte/15min
**Context variable**: `{{ctx.payload.aggregations.avg_heap.value}}` = Valeur agr√©gation

---

# Exercices Pratiques

Passez maintenant au **cahier d'exercices** pour mettre en pratique ces concepts.

**Labs √† r√©aliser**:
- Lab 5.1: Cr√©ation d'une alerte simple (cluster health)
- Lab 5.2: Configuration des actions d'alerte (webhook, index)
- Bonus 5.A: Alerte Watcher avanc√©e avec agr√©gations complexes

Temps estim√©: **45-60 minutes**

**Ces exercices couvrent**:
- Cr√©ation de Kibana Rules avec interface graphique
- Configuration de connectors (Slack, webhook)
- Cr√©ation de Watcher avec JSON
- Test et validation des alertes
