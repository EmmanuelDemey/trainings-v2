---
layout: cover
---

# ElastAlert 2

Alerting Open-Source pour Elasticsearch

---

# Qu'est-ce qu'ElastAlert 2 ?

* Framework open-source pour cr√©er des alertes sur Elasticsearch/OpenSearch
* Continuation du projet original `Yelp/elastalert` (maintenu activement)
* Alternative gratuite √† Watcher/Kibana Alerting
* √âcrit en Python, l√©ger et flexible

**Cas d'usage principaux :**
* D√©tection d'anomalies et de spikes
* Monitoring de s√©curit√© (SOC/SIEM)
* Alerting sur logs applicatifs
* Surveillance d'infrastructure

---

# ElastAlert 2 vs Kibana Alerting

| Crit√®re | ElastAlert 2 | Kibana Alerting |
|---------|--------------|-----------------|
| **Licence** | Open-source (Apache 2.0) | Gratuit (Basic) / Payant (Gold+) |
| **D√©ploiement** | Externe (Python) | Int√©gr√© √† Kibana |
| **Configuration** | Fichiers YAML | UI Web / API |
| **Types de r√®gles** | 10+ types pr√©d√©finis | Moins nombreux en Basic |
| **Complexit√©** | Plus technique | Plus accessible |
| **Flexibilit√©** | Tr√®s extensible | Limit√©e en Basic |

**Recommandation Ops :** ElastAlert2 pour environnements sans licence Gold/Platinum

---

# Architecture ElastAlert 2

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         ElastAlert 2 Service            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ   Rule Engine                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Charge les r√®gles (.yaml)      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Ex√©cute les requ√™tes ES        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - √âvalue les conditions          ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                  ‚Üì                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ   Alerter Manager                 ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Email, Slack, PagerDuty        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Webhooks personnalis√©s         ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚Üï                    ‚Üï
    Elasticsearch          Alerting
    (Data Source)         (Destinations)
```

---

# Installation - M√©thode 1 : pip

```bash
# Installation via pip
pip install elastalert2

# Cr√©ation du fichier de configuration
cp config.yaml.example config.yaml

# Configuration minimale
cat > config.yaml <<EOF
rules_folder: rules
run_every:
  minutes: 1
buffer_time:
  minutes: 15
es_host: localhost
es_port: 9200
writeback_index: elastalert_status
EOF

# Cr√©ation de l'index ElastAlert
elastalert-create-index --config config.yaml
```

---

# Installation - M√©thode 2 : Docker

```bash
# Cr√©er la structure de dossiers
mkdir -p elastalert/{config,rules}

# Cr√©er le docker-compose.yml
cat > docker-compose.yml <<EOF
version: '3'
services:
  elastalert:
    image: jertel/elastalert2:latest
    container_name: elastalert2
    volumes:
      - ./elastalert/config:/opt/elastalert/config
      - ./elastalert/rules:/opt/elastalert/rules
    restart: unless-stopped
EOF

# D√©marrage
docker-compose up -d
```

---

# Configuration Globale (config.yaml)

```yaml
# Configuration Elasticsearch
es_host: elasticsearch
es_port: 9200
es_username: elastic
es_password: changeme

# S√©curit√© SSL/TLS
use_ssl: true
verify_certs: true
ca_certs: /path/to/ca.crt

# Gestion des r√®gles
rules_folder: rules
run_every:
  minutes: 1

# Buffer de recherche
buffer_time:
  minutes: 15

# Index de m√©tadonn√©es
writeback_index: elastalert_status
alert_time_limit:
  days: 2
```

---

# Types de R√®gles - Vue d'ensemble

ElastAlert 2 propose **10 types de r√®gles** pr√©d√©finis :

1. **frequency** - N √©v√©nements dans un intervalle de temps
2. **spike** - Augmentation/diminution soudaine du volume
3. **flatline** - Absence d'√©v√©nements pendant une p√©riode
4. **change** - Changement de valeur d'un champ
5. **blacklist/whitelist** - Liste noire/blanche de valeurs
6. **new_term** - Nouvelle valeur jamais vue
7. **cardinality** - Cardinalit√© d'un champ
8. **metric_aggregation** - Seuils sur m√©triques agr√©g√©es
9. **percentage_match** - Pourcentage de correspondance
10. **any** - Au moins un r√©sultat

---

# R√®gle Type 1 : Frequency

**Objectif :** Alerter quand N √©v√©nements matchent dans un intervalle de temps

```yaml
name: "Trop d'erreurs 500"
type: frequency
index: logs-*

# Nombre d'occurrences
num_events: 5
timeframe:
  minutes: 5

# Requ√™te Elasticsearch
filter:
- query:
    query_string:
      query: "http.response.status_code:500"

# Alerting
alert:
  - email
email:
  - "ops-team@example.com"
```

---

# R√®gle Type 2 : Spike

**Objectif :** D√©tecter une augmentation/diminution anormale du volume

```yaml
name: "Spike de connexions √©chou√©es"
type: spike
index: auth-logs-*

# Seuil d'augmentation (300%)
spike_height: 3
spike_type: "up"

# Fen√™tres de comparaison
timeframe:
  minutes: 15
threshold_ref: 10
threshold_cur: 30

filter:
- query:
    match:
      event.outcome: "failure"

alert:
  - slack
slack_webhook_url: "https://hooks.slack.com/services/YOUR/WEBHOOK/URL"
```

---

# R√®gle Type 3 : Flatline

**Objectif :** Alerter en cas d'absence d'√©v√©nements (service down)

```yaml
name: "Aucun heartbeat re√ßu"
type: flatline
index: heartbeat-*

# Seuil d'√©v√©nements minimum attendu
threshold: 1
timeframe:
  minutes: 5

# Groupement par service
query_key: "service.name"

filter:
- query:
    match:
      event.type: "heartbeat"

alert:
  - pagerduty
pagerduty_service_key: "YOUR_SERVICE_KEY"
pagerduty_client_name: "ElastAlert"
```

---

# R√®gle Type 4 : Blacklist

**Objectif :** D√©tecter des valeurs interdites

```yaml
name: "Acc√®s depuis IP blacklist√©e"
type: blacklist
index: nginx-access-*

# Champ √† surveiller
compare_key: "source.ip"

# Liste noire
blacklist:
  - "192.168.1.100"
  - "10.0.0.50"

filter:
- query:
    match:
      http.response.status_code: 200

alert:
  - email
  - slack
email: ["security@example.com"]
```

---

# R√®gle Type 5 : New Term

**Objectif :** D√©tecter de nouvelles valeurs jamais vues auparavant

```yaml
name: "Nouveau user-agent jamais vu"
type: new_term
index: web-access-*

# Champ √† surveiller
fields:
  - "user_agent.original"

# Fen√™tre d'apprentissage
terms_window_size:
  days: 30

filter:
- query:
    exists:
      field: "user_agent.original"

alert:
  - jira
jira_server: "https://jira.example.com"
jira_project: "SEC"
jira_issuetype: "Task"
```

---

# R√®gle Type 6 : Metric Aggregation

**Objectif :** Alerter sur des seuils de m√©triques agr√©g√©es

```yaml
name: "Temps de r√©ponse moyen √©lev√©"
type: metric_aggregation
index: apm-*

# M√©trique
metric_agg_key: "transaction.duration.us"
metric_agg_type: "avg"

# Seuil
max_threshold: 500000  # 500ms en microseconds

# Fen√™tre
buffer_time:
  minutes: 10

filter:
- query:
    match:
      processor.event: "transaction"

alert:
  - slack
```

---

# Alerters Disponibles

ElastAlert 2 supporte **30+ destinations d'alerting** :

**Communication :**
- Email (SMTP)
- Slack
- Microsoft Teams
- Telegram
- Discord
- Mattermost

**Ticketing & Incident Management :**
- PagerDuty
- Jira
- ServiceNow
- Opsgenie

**Stockage & Logging :**
- Elasticsearch (index)
- HTTP POST (webhook)
- Command (script custom)

**Cloud & Monitoring :**
- AWS SNS/SES
- Datadog
- TheHive

---

# Configuration Alerter : Email

```yaml
name: "Alerte Email"
type: frequency
# ... r√®gle ...

# Configuration SMTP
alert:
  - email

# Destinataires
email:
  - "ops@example.com"
  - "admin@example.com"

# Serveur SMTP
smtp_host: "smtp.gmail.com"
smtp_port: 587
smtp_ssl: false
smtp_auth_file: "/path/to/smtp_auth.yaml"

# Formatage
email_subject: "ALERT: {0}"
email_body: |
  R√®gle d√©clench√©e: {0}
  Timestamp: {1}
  Nombre d'√©v√©nements: {2}
```

---

# Configuration Alerter : Slack

```yaml
name: "Alerte Slack"
type: spike
# ... r√®gle ...

alert:
  - slack

# Webhook URL
slack_webhook_url: "https://hooks.slack.com/services/T00/B00/XXX"

# Personnalisation
slack_username_override: "ElastAlert Bot"
slack_emoji_override: ":bell:"
slack_channel_override: "#ops-alerts"

# Message enrichi
slack_msg_color: "danger"
slack_title: "Spike d√©tect√© !"
slack_title_link: "https://kibana.example.com/app/discover"
```

---

# Configuration Alerter : PagerDuty

```yaml
name: "Incident PagerDuty"
type: flatline
# ... r√®gle ...

alert:
  - pagerduty

# Service Key (Events API v2)
pagerduty_service_key: "YOUR_INTEGRATION_KEY"

# M√©tadonn√©es
pagerduty_client_name: "ElastAlert2"
pagerduty_event_type: "trigger"  # trigger, resolve, acknowledge

# Payload personnalis√©
pagerduty_v2_payload_custom_details:
  service: "{match[service.name]}"
  host: "{match[host.name]}"
  message: "{match[message]}"
```

---

# Configuration Alerter : Webhook Custom

```yaml
name: "Webhook personnalis√©"
type: any
# ... r√®gle ...

alert:
  - post

# URL du webhook
http_post_url: "https://api.myapp.com/alerts"

# Headers HTTP
http_post_headers:
  Content-Type: "application/json"
  Authorization: "Bearer YOUR_TOKEN"

# Payload JSON
http_post_payload:
  alert_name: "{rule[name]}"
  timestamp: "{match[timestamp]}"
  severity: "high"
  data: "{match}"

# Options
http_post_all_values: true
http_post_timeout: 10
```

---

# Enrichissement des Alertes

ElastAlert 2 permet d'**enrichir les alertes** avec des donn√©es suppl√©mentaires :

```yaml
name: "Alerte enrichie"
type: frequency
# ... r√®gle ...

# 1. Enrichissement via lookup Elasticsearch
enhancement:
  - "elastalert_modules.enhancement.DropMatchException"

# 2. Ajout de champs statiques
static_fields:
  environment: "production"
  team: "platform"

# 3. Variables dans le message
alert_text: |
  Alerte: {rule[name]}
  Environnement: {match[env]}
  Service: {match[service.name]}
  Host: {match[host.name]}
  Message: {match[message]}
  Nombre total: {num_hits}
```

---

# Agr√©gation d'Alertes (Alert Aggregation)

√âviter le spam d'alertes avec l'**agr√©gation** :

```yaml
name: "Alertes agr√©g√©es"
type: frequency
# ... r√®gle ...

# Agr√©gation par service
aggregation:
  schedule: "*/5 * * * *"  # Toutes les 5 minutes

# Ou agr√©gation par cl√©
query_key: "service.name"
aggregation_key: "host.name"

# Ou simple agr√©gation temporelle
realert:
  minutes: 10  # Ne pas r√©alerter avant 10 min

# Summary uniquement
summary_table_fields:
  - "service.name"
  - "host.name"
  - "message"
```

---

# Gestion des Erreurs et Debugging

**Tester une r√®gle avant d√©ploiement :**

```bash
# Test d'une r√®gle sp√©cifique
elastalert-test-rule \
  --config config.yaml \
  rules/mon_alerte.yaml

# Test avec affichage des r√©sultats
elastalert-test-rule \
  --config config.yaml \
  --alert \
  --days 1 \
  rules/mon_alerte.yaml
```

**Logs et troubleshooting :**

```yaml
# Dans config.yaml
logging:
  version: 1
  incremental: false
  formatters:
    verbose:
      format: '%(asctime)s %(levelname)s %(name)s %(message)s'
  handlers:
    console:
      class: logging.StreamHandler
      formatter: verbose
  loggers:
    elastalert:
      level: DEBUG
```

---

# Monitoring ElastAlert 2

**Index de statut :** `elastalert_status`

```json
GET elastalert_status*/_search
{
  "sort": [{"@timestamp": "desc"}],
  "size": 10
}
```

**M√©triques importantes :**
- `elastalert_status` - √âtat des r√®gles
- `elastalert_error` - Erreurs d'ex√©cution
- `silence` - R√®gles silenc√©es
- `elastalert` - Alertes d√©clench√©es

**Dashboard Kibana recommand√© :**
- Nombre d'alertes par r√®gle (Timeline)
- Taux d'erreur des r√®gles (Pie chart)
- Temps d'ex√©cution des r√®gles (Line chart)

---

# Best Practices - Configuration

**Performance :**
- Limiter `buffer_time` au strict n√©cessaire (√©viter > 1h)
- Utiliser `use_count_query: true` pour les grandes volum√©tries
- Optimiser les filtres Elasticsearch
- Activer `use_strftime_index: true` pour les index temporels

**Fiabilit√© :**
- Toujours tester les r√®gles avec `elastalert-test-rule`
- Configurer `alert_time_limit` pour √©viter les alertes dupliqu√©es
- Utiliser `realert` pour limiter le spam
- Monitor l'index `elastalert_status`

**S√©curit√© :**
- Stocker les credentials dans des fichiers s√©par√©s
- Utiliser SSL/TLS pour Elasticsearch
- Restreindre les permissions du user ElastAlert (read-only sur les index)
- Ne jamais commiter les fichiers de config avec secrets

---

# Best Practices - R√®gles

**Conception :**
1. **Nommage explicite** : `prod-api-errors-500-spike`
2. **Documentation** : Ajouter des commentaires dans les YAML
3. **Ownership** : Sp√©cifier l'√©quipe responsable
4. **Priorit√©** : Utiliser des tags (`priority: P1`)

**√âviter les faux positifs :**
```yaml
# Filtres stricts
filter:
  - query:
      bool:
        must:
          - match: {env: "production"}
        must_not:
          - match: {service.name: "test-*"}

# Seuils appropri√©s
num_events: 10  # Pas trop bas
timeframe:
  minutes: 5    # Fen√™tre adapt√©e
```

---

# Best Practices - D√©ploiement

**D√©ploiement Production :**

```yaml
# docker-compose.yml
version: '3.8'
services:
  elastalert:
    image: jertel/elastalert2:2.15.0  # Version fixe
    container_name: elastalert2
    restart: always
    volumes:
      - ./config:/opt/elastalert/config:ro
      - ./rules:/opt/elastalert/rules:ro
      - ./smtp_auth.yaml:/opt/elastalert/smtp_auth.yaml:ro
    environment:
      - TZ=Europe/Paris
    healthcheck:
      test: ["CMD", "python", "-m", "elastalert.elastalert", "--config", "/opt/elastalert/config/config.yaml", "--es-debug-trace"]
      interval: 60s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
```

---

# Cas d'Usage 1 : S√©curit√©

**D√©tection de brute-force SSH :**

```yaml
name: "SSH Brute Force Attack"
type: frequency
index: auth-logs-*

num_events: 5
timeframe:
  minutes: 5

query_key: "source.ip"

filter:
  - query:
      bool:
        must:
          - match: {event.action: "ssh_login"}
          - match: {event.outcome: "failure"}

alert:
  - email
  - slack
email: ["security@example.com"]
slack_webhook_url: "https://hooks.slack.com/..."
alert_subject: "SECURITY: SSH Brute Force from {0}"
```

---

# Cas d'Usage 2 : Performance

**D√©gradation du temps de r√©ponse API :**

```yaml
name: "API Response Time Degradation"
type: metric_aggregation
index: apm-transactions-*

metric_agg_key: "transaction.duration.us"
metric_agg_type: "percentiles"
metric_agg_script:
  script: "doc['transaction.duration.us'].value"

percentile_range: 95
max_threshold: 1000000  # 1 seconde (en microseconds)

buffer_time:
  minutes: 5

query_key: "transaction.name"

filter:
  - term:
      processor.event: "transaction"

alert:
  - pagerduty
```

---

# Cas d'Usage 3 : Business Metrics

**Alerte sur chute de commandes :**

```yaml
name: "Drop in Orders"
type: flatline
index: orders-*

threshold: 10
timeframe:
  minutes: 15

use_count_query: true

filter:
  - query:
      bool:
        must:
          - match: {order.status: "completed"}
          - range:
              order.amount:
                gte: 0

alert:
  - email
  - slack
email_reply_to: "business-alerts@example.com"
alert_text: |
  ‚ö†Ô∏è Chute significative des commandes d√©tect√©e !

  P√©riode: {0}
  Seuil attendu: {1} commandes / 15 min
  V√©rifier le syst√®me de paiement.
```

---

# Cas d'Usage 4 : Infrastructure

**Surveillance du disk usage :**

```yaml
name: "High Disk Usage"
type: metric_aggregation
index: metricbeat-*

metric_agg_key: "system.filesystem.used.pct"
metric_agg_type: "max"

max_threshold: 0.85  # 85%

buffer_time:
  minutes: 5

query_key: "host.name"

filter:
  - term:
      metricset.name: "filesystem"
  - term:
      system.filesystem.mount_point: "/"

alert:
  - opsgenie
opsgenie_key: "YOUR_API_KEY"
opsgenie_priority: "P2"
```

---

# Migration Watcher ‚Üí ElastAlert 2

**Comparaison des concepts :**

| Watcher | ElastAlert 2 |
|---------|--------------|
| Watch | Rule (fichier YAML) |
| Trigger (schedule) | `run_every` |
| Input (search) | `filter` + `index` |
| Condition (compare) | `type` + seuils |
| Actions | `alert` + alerters |
| .watcher-history | elastalert_status |

**Exemple de migration :**

```yaml
# Watcher ‚Üí ElastAlert 2
# trigger.schedule.interval: "5m" ‚Üí run_every: {minutes: 5}
# input.search ‚Üí filter: [...]
# condition.compare ‚Üí type: frequency + num_events
# actions.email ‚Üí alert: [email]
```

---

# Int√©gration CI/CD

**Validation automatique des r√®gles :**

```yaml
# .gitlab-ci.yml
test-elastalert-rules:
  stage: test
  image: jertel/elastalert2:latest
  script:
    - |
      for rule in rules/*.yaml; do
        echo "Testing $rule..."
        elastalert-test-rule \
          --config config.yaml \
          --days 1 \
          "$rule" || exit 1
      done
  only:
    changes:
      - rules/*.yaml
```

**D√©ploiement automatis√© :**

```bash
# deploy.sh
#!/bin/bash
set -e

# Validation
elastalert-test-rule --config config.yaml rules/*.yaml

# D√©ploiement
rsync -avz rules/ elastalert-server:/opt/elastalert/rules/
ssh elastalert-server "docker restart elastalert2"
```

---

# ElastAlert 2 avec Kubernetes

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: elastalert2
spec:
  replicas: 1
  selector:
    matchLabels:
      app: elastalert2
  template:
    metadata:
      labels:
        app: elastalert2
    spec:
      containers:
      - name: elastalert2
        image: jertel/elastalert2:2.15.0
        volumeMounts:
        - name: config
          mountPath: /opt/elastalert/config
          readOnly: true
        - name: rules
          mountPath: /opt/elastalert/rules
          readOnly: true
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "500m"
      volumes:
      - name: config
        configMap:
          name: elastalert-config
      - name: rules
        configMap:
          name: elastalert-rules
```

---

# Ressources et Documentation

**Documentation officielle :**
- üìñ [ElastAlert 2 Docs](https://elastalert2.readthedocs.io/)
- üíª [GitHub Repository](https://github.com/jertel/elastalert2)
- üê≥ [Docker Hub](https://hub.docker.com/r/jertel/elastalert2)

**Tutoriels 2024-2025 :**
- [Mastering Real-Time Alerting with ElastAlert2](https://www.tothenew.com/blog/setting-up-elastalert2-for-real-time-alerting-on-elasticsearch-indices/) (Avril 2025)
- [Rules and alerts with ElastAlert 2](https://www.bujarra.com/reglas-y-alertas-con-elastalert-2/?lang=en)

**Communaut√© :**
- GitHub Issues & Discussions
- Elastic Community Forums

---
layout: cover
---

# Travaux Pratiques

ElastAlert 2 - Configuration et R√®gles d'Alerting
