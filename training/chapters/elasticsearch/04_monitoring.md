---
layout: cover
---

# Strat√©gie de Monitoring

Surveillance et observabilit√© d'Elasticsearch en production

---

# Objectifs d'Apprentissage

√Ä la fin de cette section, vous serez capable de:

- Utiliser les APIs de monitoring natives pour collecter des m√©triques cluster
- Identifier et surveiller les m√©triques critiques pour la sant√© du cluster
- Configurer et exploiter les interfaces de monitoring Kibana (Stack Monitoring)
- Analyser les logs Elasticsearch pour diagnostiquer les probl√®mes op√©rationnels

---

# Pourquoi Monitorer Elasticsearch ?

Le monitoring proactif est essentiel pour maintenir un cluster Elasticsearch en bonne sant√©.

**Objectifs du monitoring**:
- üéØ **D√©tection pr√©coce**: Identifier les probl√®mes avant impact utilisateur
- üìä **Planification capacit√©**: Anticiper les besoins en ressources
- üîç **Troubleshooting**: Diagnostiquer rapidement les incidents
- üìà **Optimisation**: Identifier les goulots d'√©tranglement de performance
- ‚úÖ **SLA compliance**: V√©rifier le respect des objectifs de disponibilit√©

**Niveaux de monitoring**:
1. **Infrastructure**: CPU, RAM, disque, r√©seau (OS-level)
2. **Cluster**: Sant√©, n≈ìuds, shards, indices (Elasticsearch APIs)
3. **Application**: Latence requ√™tes, taux d'erreur, throughput
4. **Business**: M√©triques m√©tier (volume documents, utilisateurs actifs)

---

# APIs de Monitoring Natives

Elasticsearch fournit plusieurs [APIs de monitoring](https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster.html) pour observer l'√©tat du cluster.

**APIs essentielles**:

| API | Usage | Fr√©quence recommand√©e |
|-----|-------|-----------------------|
| `_cluster/health` | Sant√© globale du cluster | 30s - 1min |
| `_cluster/stats` | Statistiques agr√©g√©es du cluster | 1 - 5min |
| `_nodes/stats` | M√©triques d√©taill√©es par n≈ìud | 30s - 1min |
| `_cat/indices` | √âtat et taille des indices | 1 - 5min |
| `_cat/shards` | Allocation et √©tat des shards | 1 - 5min |
| `_nodes/hot_threads` | CPU threads actifs (debug) | √Ä la demande |
| `_cat/pending_tasks` | T√¢ches master en attente | 30s - 1min |

**Principe g√©n√©ral**: Queries l√©g√®res et fr√©quentes pour d√©tection rapide, queries lourdes moins fr√©quentes.

---

# API Cluster Stats

L'API [_cluster/stats](https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-stats.html) fournit des statistiques agr√©g√©es pour tout le cluster.

**Requ√™te**:
```bash
GET /_cluster/stats
```

**M√©triques cl√©s retourn√©es**:
```json
{
  "cluster_name": "production",
  "nodes": {
    "count": { "total": 10, "data": 7, "master": 3 },
    "os": { "mem": { "total_in_bytes": 687194767360 }},
    "jvm": { "mem": { "heap_used_in_bytes": 123456789 }}
  },
  "indices": {
    "count": 150,
    "docs": { "count": 50000000 },
    "store": { "size_in_bytes": 1099511627776 },
    "shards": { "total": 450, "primaries": 225 }
  }
}
```

**Cas d'usage**: Vue d'ensemble du cluster pour dashboards, calcul de ratios (heap usage rate, storage growth rate).

---

# API Nodes Stats

L'API [_nodes/stats](https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-nodes-stats.html) retourne des m√©triques d√©taill√©es par n≈ìud.

**Requ√™te avec filtres**:
```bash
GET /_nodes/stats/jvm,os,process,indices,fs,thread_pool,breaker
```

**Sections importantes**:
- **jvm**: `mem.heap_used_percent`, `gc.collectors.*.collection_time_in_millis`
- **os**: `cpu.percent`, `mem.used_percent`, `swap.used_in_bytes`
- **process**: `cpu.percent`, `open_file_descriptors`
- **indices**: `indexing.index_total`, `search.query_total`, `search.query_time_in_millis`
- **fs**: `total.available_in_bytes`, `io_stats.total.operations`
- **thread_pool**: `*.rejected` (rejections critiques)
- **breaker**: Circuit breakers d√©clench√©s

**Monitoring key**: `indices.indexing.index_time_in_millis / indices.indexing.index_total` = latence moyenne d'indexation

---

# API Cat Indices et Shards

Les [_cat APIs](https://www.elastic.co/guide/en/elasticsearch/reference/current/cat.html) offrent des vues concises pour op√©rations quotidiennes.

**Cat Indices** (√©tat des indices):
```bash
GET /_cat/indices?v&h=index,health,status,pri,rep,docs.count,store.size&s=store.size:desc
```

R√©sultat:
```
index          health status pri rep docs.count store.size
logs-2023.11   green  open     5   1   15000000      2.5gb
products       yellow open     1   1     100000       50mb
```

**Cat Shards** (localisation et √©tat):
```bash
GET /_cat/shards?v&h=index,shard,prirep,state,node,store&s=store:desc
```

R√©sultat:
```
index     shard prirep state   node    store
logs-2023 0     p      STARTED node-1  512mb
logs-2023 0     r      STARTED node-2  512mb
```

**Cas d'usage**: Identification rapide de shards unassigned, indices volumineux, distribution d√©s√©quilibr√©e.

---

# API Hot Threads (Troubleshooting)

L'API [_nodes/hot_threads](https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-nodes-hot-threads.html) identifie les threads consommant le plus de CPU.

**Requ√™te**:
```bash
GET /_nodes/hot_threads
GET /_nodes/node-1/hot_threads?threads=5&interval=500ms&type=cpu
```

**Param√®tres**:
- `threads`: Nombre de threads √† afficher (d√©faut: 3)
- `interval`: P√©riode d'√©chantillonnage (d√©faut: 500ms)
- `type`: `cpu` (d√©faut), `wait`, `block`

**R√©sultat** (extrait):
```
::: {node-1}{abc123}
   Hot threads at 2023-11-10T10:30:00.000Z, interval=500ms, busiestThreads=5:
   
   99.8% (499ms out of 500ms) cpu usage by thread 'elasticsearch[node-1][search][T#5]'
     org.elasticsearch.search.SearchService.executeQueryPhase()
     org.elasticsearch.search.query.QueryPhase.execute()
```

**Usage**: Diagnostic de pics CPU, identification de requ√™tes co√ªteuses en temps r√©el.

---

# M√©triques Critiques: Cluster Health

La [sant√© du cluster](https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-health.html) est la m√©trique la plus importante √† surveiller.

**Status colors**:
- üü¢ **GREEN**: Tous les shards (primaires + replicas) allou√©s ‚úÖ
- üü° **YELLOW**: Tous primaires allou√©s, certains replicas manquants ‚ö†Ô∏è
- üî¥ **RED**: Au moins un shard primaire manquant ‚ùå PERTE DE DONN√âES

**Requ√™te d√©taill√©e**:
```bash
GET /_cluster/health?level=indices
```

**Alertes √† configurer**:
```yaml
# Seuils d'alerte recommand√©s
cluster.status:
  CRITICAL: status == "red"         # Alerte imm√©diate
  WARNING: status == "yellow"       # Enqu√™te sous 15min
  
unassigned_shards:
  CRITICAL: > 10                    # Action imm√©diate
  WARNING: > 0                      # Enqu√™te
  
active_shards_percent:
  CRITICAL: < 90%                   # Probl√®me d'allocation grave
  WARNING: < 98%                    # Surveillance accrue
```

---

# M√©triques Critiques: CPU et M√©moire

Le monitoring de **CPU** et **m√©moire** est critique pour la stabilit√©.

**CPU monitoring**:
```bash
GET /_nodes/stats/os,process?filter_path=nodes.*.os.cpu,nodes.*.process.cpu
```

**Seuils CPU**:
- ‚úÖ **<60%**: Sain
- ‚ö†Ô∏è **60-80%**: Surveiller, planifier scaling
- ‚ùå **>80%**: Critique, risque de d√©gradation latence
- üö® **>95%**: Cluster surcharg√©, action imm√©diate

**Heap memory monitoring**:
```bash
GET /_nodes/stats/jvm?filter_path=nodes.*.jvm.mem
```

**Seuils Heap**:
- ‚úÖ **<75%**: Sain
- ‚ö†Ô∏è **75-85%**: Surveiller GC frequency
- ‚ùå **>85%**: Risque de OutOfMemoryError
- üö® **>95%**: GC thrashing probable, circuit breakers activ√©s

**Garbage Collection**:
```
gc_collection_time / gc_collection_count = moyenne dur√©e GC
Si moyenne >100ms ‚Üí probl√®me heap ou GC tuning n√©cessaire
```

---

# M√©triques Critiques: Disque et I/O

Le [monitoring disque](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-cluster.html#disk-based-shard-allocation) pr√©vient les pannes dues au remplissage.

**Disk space monitoring**:
```bash
GET /_nodes/stats/fs?filter_path=nodes.*.fs.total
```

**Seuils disque** (disk-based shard allocation):
- ‚úÖ **<85%**: Sain
- ‚ö†Ô∏è **85-90%**: Watermark LOW - aucune allocation de nouveaux shards sur ce n≈ìud
- ‚ùå **90-95%**: Watermark HIGH - relocate shards depuis ce n≈ìud
- üö® **>95%**: Watermark FLOOD - indices en read-only !

**Configuration watermarks**:
```json
PUT /_cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.disk.watermark.low": "85%",
    "cluster.routing.allocation.disk.watermark.high": "90%",
    "cluster.routing.allocation.disk.watermark.flood_stage": "95%"
  }
}
```

**I/O stats**: `fs.io_stats.total.operations`, `fs.io_stats.total.read_time` (latence I/O)

---

# M√©triques Critiques: Indexation et Recherche

Les m√©triques d'**indexing** et **search** mesurent la performance applicative.

**Indexing metrics**:
```bash
GET /_nodes/stats/indices?filter_path=nodes.*.indices.indexing
```

M√©triques cl√©s:
- `indexing.index_total`: Nombre total de documents index√©s
- `indexing.index_time_in_millis`: Temps total d'indexation
- `indexing.index_failed`: Documents √©chou√©s (‚ùå doit √™tre proche de 0)

**Calcul latence moyenne**:
```
avg_indexing_latency = index_time_in_millis / index_total
```

**Search metrics**:
```bash
GET /_nodes/stats/indices?filter_path=nodes.*.indices.search
```

M√©triques cl√©s:
- `search.query_total`: Nombre de requ√™tes
- `search.query_time_in_millis`: Temps total de recherche
- `search.fetch_total`, `search.fetch_time_in_millis`: Phase fetch

**Calcul latence moyenne**:
```
avg_search_latency = query_time_in_millis / query_total
```

---

# M√©triques Critiques: Thread Pool Rejections

Les [thread pool rejections](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-threadpool.html) indiquent une surcharge du cluster.

**Monitoring rejections**:
```bash
GET /_nodes/stats/thread_pool?filter_path=nodes.*.thread_pool.*.rejected
```

**Thread pools √† surveiller**:
- **write**: Rejections d'indexation ‚Üí Cluster surcharg√© en √©criture
- **search**: Rejections de recherche ‚Üí Cluster surcharg√© en lecture
- **get**: Rejections de GET par ID (rare)

**Seuils d'alerte**:
```yaml
thread_pool.*.rejected:
  WARNING: delta > 10/min        # Surcharge ponctuelle
  CRITICAL: delta > 100/min      # Surcharge s√©v√®re
```

**Actions correctives**:
- Court terme: Throttle client-side, augmenter queue_size (temporaire)
- Moyen terme: Optimiser requ√™tes, ajouter n≈ìuds
- Long terme: Revoir architecture, sharding strategy

---

# Kibana Stack Monitoring: Vue d'Ensemble

[Kibana Stack Monitoring](https://www.elastic.co/guide/en/kibana/current/xpack-monitoring.html) fournit une interface graphique pour surveiller Elasticsearch.

**Activation**:
```yaml
# elasticsearch.yml
xpack.monitoring.collection.enabled: true
```

**Pages principales**:
1. **Overview**: Sant√© globale, n≈ìuds actifs, utilisation ressources
2. **Nodes**: D√©tail par n≈ìud (CPU, memory, disk, JVM)
3. **Indices**: Liste indices avec m√©triques (size, docs, search rate)
4. **Advanced**: Logs, thread pools, CCR, Watcher

**Avantages vs APIs brutes**:
- ‚úÖ Visualisation graphique avec historique (time-series)
- ‚úÖ Alertes int√©gr√©es (Elasticsearch Watcher)
- ‚úÖ Corr√©lation entre m√©triques (CPU spike + search latency)
- ‚úÖ Drill-down par n≈ìud/index/shard

**Limite**: Overhead de monitoring (~5-10% resources). Pour clusters critiques, envisager monitoring externe (Prometheus, Datadog).

---

# Kibana Stack Monitoring: Cluster Overview

La page **Cluster Overview** affiche les m√©triques agr√©g√©es en temps r√©el.

**Widgets principaux**:

**1. Cluster Health**
- Status color (green/yellow/red)
- Nombre de n≈ìuds actifs
- Shards (total, primaries, replicas, unassigned)

**2. Search & Indexing Rate**
- Graphique time-series des requ√™tes/sec
- Latence moyenne (p50, p95, p99)
- Taux d'erreur

**3. Resource Usage**
- CPU usage (moyenne cluster)
- JVM Heap (average across nodes)
- Disk usage (total et par n≈ìud)

**4. Alerts**
- Liste des alertes actives (disk watermark, heap high, etc.)

**Configuration refresh**: Par d√©faut 10s, ajustable dans Settings.

---

# Kibana Stack Monitoring: Nodes View

La page **Nodes** permet de surveiller chaque n≈ìud individuellement.

**M√©triques par n≈ìud**:

| M√©trique | Description | Seuil d'alerte |
|----------|-------------|----------------|
| **CPU Usage** | % CPU utilis√© | >80% |
| **JVM Memory** | % heap utilis√© | >85% |
| **Disk Free Space** | Espace disque restant | <15% (85% full) |
| **Load Average** | Charge syst√®me (1m, 5m, 15m) | >cores √ó 1.5 |
| **Shards** | Nombre de shards sur ce n≈ìud | >20/GB heap |

**Graphiques disponibles**:
- CPU usage over time
- JVM heap usage over time
- GC duration and frequency
- Indexing and search latency
- Disk I/O throughput

**Drill-down**: Cliquer sur un n≈ìud pour voir logs, hot threads, stack traces.

---

# Kibana Stack Monitoring: Indices View

La page **Indices** surveille la sant√© et performance de chaque index.

**M√©triques par index**:
- **Health**: green/yellow/red
- **Status**: open/close
- **Document Count**: Nombre de documents
- **Size**: Taille totale (primaires + replicas)
- **Search Rate**: Recherches/sec
- **Indexing Rate**: Documents/sec

**Graphiques time-series**:
- Document count evolution
- Indexing rate (docs/s)
- Search rate (queries/s)
- Search latency (ms)

**Use cases**:
- Identifier les indices √† forte croissance (planification capacit√©)
- D√©tecter les indices non utilis√©s (candidats √† suppression/archivage)
- Surveiller les index en yellow/red (probl√®mes d'allocation)

---

# Analyse des Logs: Emplacements

Elasticsearch g√©n√®re plusieurs types de [logs](https://www.elastic.co/guide/en/elasticsearch/reference/current/logging.html) pour diagnostiquer les probl√®mes.

**Fichiers de logs par d√©faut**:
```
/var/log/elasticsearch/
‚îú‚îÄ‚îÄ <cluster_name>.log              # Log principal
‚îú‚îÄ‚îÄ <cluster_name>_deprecation.log  # Avertissements de d√©pr√©ciation
‚îú‚îÄ‚îÄ <cluster_name>_index_search_slowlog.log
‚îú‚îÄ‚îÄ <cluster_name>_index_indexing_slowlog.log
‚îî‚îÄ‚îÄ gc.log                          # Garbage Collection logs
```

**Niveaux de log**:
- **ERROR**: Erreurs n√©cessitant une action
- **WARN**: Avertissements √† surveiller
- **INFO**: √âv√©nements normaux (startup, config changes)
- **DEBUG**: D√©tails pour troubleshooting (activer temporairement)
- **TRACE**: D√©tails tr√®s verbeux (dev uniquement)

**Configuration dans log4j2.properties**:
```properties
logger.action.name = org.elasticsearch.action
logger.action.level = info
```

---

# Analyse des Logs: Configuration Log4j2

La configuration [Log4j2](https://www.elastic.co/guide/en/elasticsearch/reference/current/logging.html#configuring-logging-levels) contr√¥le le niveau de d√©tail des logs.

**Fichier log4j2.properties**:
```properties
# Niveau global
rootLogger.level = info

# Logger pour un package sp√©cifique
logger.discovery.name = org.elasticsearch.discovery
logger.discovery.level = debug

# Appender pour rotation des logs
appender.rolling.type = RollingFile
appender.rolling.fileName = ${sys:es.logs.base_path}${sys:file.separator}${sys:es.logs.cluster_name}.log
appender.rolling.filePattern = ${sys:es.logs.base_path}${sys:file.separator}${sys:es.logs.cluster_name}-%d{yyyy-MM-dd}-%i.log.gz
appender.rolling.policies.type = Policies
appender.rolling.policies.time.type = TimeBasedTriggeringPolicy
appender.rolling.policies.time.interval = 1
appender.rolling.policies.size.type = SizeBasedTriggeringPolicy
appender.rolling.policies.size.size = 256MB
```

---

# Analyse des Logs: Configuration Log4j2

**Modification dynamique** (sans red√©marrage):
```json
PUT /_cluster/settings
{
  "transient": {
    "logger.org.elasticsearch.discovery": "DEBUG",
    "logger.org.elasticsearch.index.search.slowlog": "TRACE"
  }
}
```

---

# Analyse des Logs: Slow Logs

Les [slow logs](https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-slowlog.html) enregistrent les requ√™tes d√©passant des seuils de latence.

**Configuration par index**:
```json
PUT /my-index/_settings
{
  "index.search.slowlog.threshold.query.warn": "10s",
  "index.search.slowlog.threshold.query.info": "5s",
  "index.search.slowlog.threshold.query.debug": "2s",
  "index.search.slowlog.threshold.query.trace": "500ms",
  
  "index.indexing.slowlog.threshold.index.warn": "10s",
  "index.indexing.slowlog.threshold.index.info": "5s",
  "index.indexing.slowlog.threshold.index.debug": "2s",
  "index.indexing.slowlog.threshold.index.trace": "500ms"
}
```

---

# Analyse des Logs: Slow Logs

**Format du slow log**:
```
[2023-11-10T10:30:15,123][WARN ][i.s.s.query] [node-1] [my-index][0] 
took[5.2s], took_millis[5234], types[], stats[], search_type[QUERY_THEN_FETCH], 
total_shards[5], source[{"query":{"match":{"field":"value"}}}]
```

**Analyse**: Identifier patterns (requ√™tes similaires, m√™me index), optimiser ou ajouter ressources.

---

# Analyse des Logs: Messages d'Erreur Courants

Savoir interpr√©ter les erreurs courantes acc√©l√®re le troubleshooting.

**Erreurs fr√©quentes**:

**1. CircuitBreakerException**
```
\[parent\] Data too large, data for [<http_request>] would be [x], which is larger than the limit of [y]
```
‚Üí Heap satur√©, requ√™te trop gourmande. Actions: R√©duire taille requ√™te, augmenter heap, ajouter n≈ìuds.

**2. EsRejectedExecutionException**
```
rejected execution of org.elasticsearch.transport.TransportService$7@abc on EsThreadPoolExecutor[search, queue capacity = 1000]
```
‚Üí Thread pool satur√©. Actions: Throttle client-side, optimiser requ√™tes, scale cluster.

**3. SearchPhaseExecutionException**
```
Shard failures: [failed shard on node [xyz]: query shard failed]
```
‚Üí √âchec de recherche sur un shard. Actions: V√©rifier logs du n≈ìud concern√©, √©tat du shard.

**4. ClusterBlockException**
```
index [my-index] blocked by: [FORBIDDEN/12/index read-only / allow delete (api)];
```
‚Üí Index en read-only (souvent disk watermark flood). Actions: Lib√©rer espace disque, augmenter watermark.

---

# R√©sum√©

## Points Cl√©s

- Les **APIs natives** (_cluster/health, _nodes/stats, _cat APIs) sont essentielles pour le monitoring temps r√©el
- Les **m√©triques critiques** incluent: cluster health, CPU/memory/disk, indexing/search rates, thread pool rejections
- **Kibana Stack Monitoring** offre une interface graphique compl√®te avec historique et alertes int√©gr√©es
- L'**analyse des logs** (main log, slow logs, GC logs) permet de diagnostiquer les probl√®mes op√©rationnels
- Les **seuils d'alerte** doivent √™tre configur√©s pour d√©tection pr√©coce: heap >85%, disk >85%, CPU >80%

---

# R√©sum√©

## APIs de R√©f√©rence Rapide

| API | M√©trique cl√© | Fr√©quence |
|-----|--------------|-----------|
| `_cluster/health` | status (green/yellow/red) | 30s |
| `_nodes/stats/jvm` | heap_used_percent | 1min |
| `_nodes/stats/os` | cpu.percent | 1min |
| `_cat/indices` | health, store.size | 5min |
| `_nodes/hot_threads` | CPU threads actifs | √Ä la demande |

---

# Exercices Pratiques

Passez maintenant au **cahier d'exercices** pour mettre en pratique ces concepts.

**Labs √† r√©aliser**:
- Lab 4.1: Utilisation des APIs de monitoring natives
- Lab 4.2: Configuration des seuils d'alerte critiques
- Lab 4.3: Exploration de Kibana Stack Monitoring

**Ces exercices couvrent**:
- Requ√™tes sur APIs _cluster/health, _nodes/stats, _cat
- Configuration de slow logs et watermarks
- Navigation dans Kibana Stack Monitoring
- Interpr√©tation de logs et diagnostic de probl√®mes simul√©s
