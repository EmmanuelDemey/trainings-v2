---
layout: cover
---

# Bonnes Pratiques de Production

Architecture, haute disponibilitÃ©, et gestion opÃ©rationnelle

---

# Objectifs d'Apprentissage

Ã€ la fin de ce module, vous serez capable de :

- **Concevoir** une architecture de cluster Elasticsearch pour la production
- **Configurer** la haute disponibilitÃ© avec rÃ©plication et rack awareness
- **Planifier** la disaster recovery avec RPO/RTO appropriÃ©s
- **Appliquer** les checklists opÃ©rationnelles pour dÃ©ploiements et incidents

---

# Pourquoi les Bonnes Pratiques sont Essentielles

Un cluster mal configurÃ© en production peut entraÃ®ner des **pertes de donnÃ©es** et des **interruptions de service**.

**Risques sans bonnes pratiques** :
1. ğŸ’¥ **Split-brain** : Cluster se divise en deux parties indÃ©pendantes (corruption de donnÃ©es)
2. ğŸ“‰ **Performance dÃ©gradÃ©e** : NÅ“uds surchargÃ©s, recherches lentes, indexations bloquÃ©es
3. ğŸ”¥ **Perte de donnÃ©es** : Pas de rÃ©pliques, pas de snapshots, dÃ©faillance matÃ©rielle
4. â±ï¸ **Recovery lent** : Pas de plan de disaster recovery, RTO/RPO non respectÃ©s
5. ğŸ¤· **Incidents non rÃ©solus** : Pas de runbooks, Ã©quipes ops perdues

**Objectif** : Construire un cluster **rÃ©silient**, **performant**, et **maintenable**.

---
layout: section
---

# Partie 1: Patterns d'Architecture de Cluster

SÃ©paration des rÃ´les et dimensionnement

---

# RÃ´les de NÅ“uds Elasticsearch

Elasticsearch permet de spÃ©cialiser les nÅ“uds avec des **rÃ´les** pour optimiser performance et stabilitÃ©.

| RÃ´le | Description | Ressources | Charge de travail |
|------|-------------|------------|-------------------|
| **master** | Gestion du cluster (Ã©tat, shards, indices) | CPU moyen, RAM faible | Faible (mÃ©tadonnÃ©es) |
| **data** | Stockage et recherche de donnÃ©es | CPU Ã©levÃ©, RAM Ã©levÃ©e, Disque rapide | TrÃ¨s Ã©levÃ©e |
| **data_hot** | DonnÃ©es actives (Ã©critures frÃ©quentes) | CPU trÃ¨s Ã©levÃ©, SSD rapide | Indexation intensive |
| **data_warm** | DonnÃ©es anciennes (lectures occasionnelles) | CPU moyen, HDD acceptable | Recherches modÃ©rÃ©es |
| **data_cold** | DonnÃ©es archivÃ©es (lectures rares) | CPU faible, HDD lent | Minimal |
| **ingest** | Transformation de donnÃ©es (pipelines) | CPU Ã©levÃ©, RAM modÃ©rÃ©e | Traitement de donnÃ©es |
| **ml** | Machine Learning (dÃ©tection d'anomalies) | CPU trÃ¨s Ã©levÃ©, RAM trÃ¨s Ã©levÃ©e | ML tasks |
| **coordinating** | Routage de requÃªtes (pas de donnÃ©es) | CPU moyen, RAM modÃ©rÃ©e | AgrÃ©gations distribuÃ©es |

**Configuration** : Dans `elasticsearch.yml`, dÃ©finir `node.roles: [master, data]`

---

# Pattern 1 : Cluster de Production Basique (3-5 NÅ“uds)

**Architecture simple** pour petites Ã  moyennes charges.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3 NÅ“uds Master-eligible + Data         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ M+D  â”‚  â”‚ M+D  â”‚  â”‚ M+D  â”‚          â”‚
â”‚  â”‚ Node1â”‚  â”‚ Node2â”‚  â”‚ Node3â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Configuration par nÅ“ud** :
```yaml
# elasticsearch.yml (sur chaque nÅ“ud)
node.roles: [master, data, ingest]
cluster.initial_master_nodes: ["node1", "node2", "node3"]
discovery.seed_hosts: ["node1:9300", "node2:9300", "node3:9300"]
```

**Avantages** :
- âœ… Simple Ã  configurer et maintenir
- âœ… Haute disponibilitÃ© avec quorum de 3 masters

**InconvÃ©nients** :
- âŒ Pas de sÃ©paration des responsabilitÃ©s (master et data partagent ressources)
- âŒ ScalabilitÃ© limitÃ©e (scaling vertical uniquement)

---

# Pattern 2 : Dedicated Master Nodes (Production RecommandÃ©)

**SÃ©parer les rÃ´les** pour Ã©viter que les tÃ¢ches de gestion impactent les performances de recherche.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3 Dedicated Master Nodes (lÃ©gers)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚ Master â”‚  â”‚ Master â”‚  â”‚ Master â”‚                     â”‚
â”‚  â”‚  Only  â”‚  â”‚  Only  â”‚  â”‚  Only  â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                                                           â”‚
â”‚  6+ Data Nodes (lourds en CPU/RAM/Disque)               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚Data â”‚ â”‚Data â”‚ â”‚Data â”‚ â”‚Data â”‚ â”‚Data â”‚ â”‚Data â”‚      â”‚
â”‚  â”‚ 1   â”‚ â”‚  2  â”‚ â”‚  3  â”‚ â”‚  4  â”‚ â”‚  5  â”‚ â”‚  6  â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Configuration Master Node** :
```yaml
# elasticsearch.yml (master nodes)
node.name: master-1
node.roles: [master]
cluster.initial_master_nodes: ["master-1", "master-2", "master-3"]
discovery.seed_hosts: ["master-1:9300", "master-2:9300", "master-3:9300"]
```

**Configuration Data Node** :
```yaml
# elasticsearch.yml (data nodes)
node.name: data-1
node.roles: [data, ingest]
discovery.seed_hosts: ["master-1:9300", "master-2:9300", "master-3:9300"]
```

**Avantages** :
- âœ… Masters dÃ©diÃ©s = stabilitÃ© du cluster
- âœ… Data nodes peuvent Ãªtre ajoutÃ©s horizontalement
- âœ… Isolation des pannes (data node down â‰  perte de quorum master)

---

# Pattern 3 : Hot-Warm-Cold Architecture (ILM)

**SÃ©paration des donnÃ©es** par Ã¢ge et frÃ©quence d'accÃ¨s pour optimiser coÃ»ts et performances.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HOT Tier (SSD rapides, indexation intensive)                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚ Hot Data â”‚  â”‚ Hot Data â”‚  â”‚ Hot Data â”‚                   â”‚
â”‚  â”‚  Node 1  â”‚  â”‚  Node 2  â”‚  â”‚  Node 3  â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚  DonnÃ©es actives : logs derniÃ¨res 24h                        â”‚
â”‚                                                               â”‚
â”‚  WARM Tier (HDD, lectures modÃ©rÃ©es)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚Warm Data â”‚  â”‚Warm Data â”‚  â”‚Warm Data â”‚                   â”‚
â”‚  â”‚  Node 1  â”‚  â”‚  Node 2  â”‚  â”‚  Node 3  â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚  DonnÃ©es anciennes : logs 1-30 jours                         â”‚
â”‚                                                               â”‚
â”‚  COLD Tier (Stockage Ã©conomique, lectures rares)             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”‚
â”‚  â”‚Cold Data â”‚  â”‚Cold Data â”‚                                 â”‚
â”‚  â”‚  Node 1  â”‚  â”‚  Node 2  â”‚                                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â”‚
â”‚  Archives : logs > 30 jours                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Configuration** :
```yaml
# Hot node
node.roles: [data_hot, data_content]
node.attr.data: hot

# Warm node
node.roles: [data_warm, data_content]
node.attr.data: warm

# Cold node
node.roles: [data_cold]
node.attr.data: cold
```

**ILM Policy** pour migration automatique :
```json
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover": {
            "max_size": "50GB",
            "max_age": "1d"
          }
        }
      },
      "warm": {
        "min_age": "7d",
        "actions": {
          "shrink": { "number_of_shards": 1 },
          "forcemerge": { "max_num_segments": 1 }
        }
      },
      "cold": {
        "min_age": "30d",
        "actions": {
          "searchable_snapshot": {
            "snapshot_repository": "my_backup"
          }
        }
      },
      "delete": {
        "min_age": "90d",
        "actions": {
          "delete": {}
        }
      }
    }
  }
}
```

---

# Pattern 4 : Coordinating Nodes (Large Clusters)

**NÅ“uds de coordination** dÃ©diÃ©s pour rÃ©partir la charge des agrÃ©gations complexes.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Load Balancer                                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚        â”‚         â”‚         â”‚         â”‚                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚Coordinatâ”‚ â”‚Coordinatâ”‚ â”‚Coordinatâ”‚                  â”‚
â”‚  â”‚  ing 1  â”‚ â”‚  ing 2  â”‚ â”‚  ing 3  â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚        â”‚         â”‚         â”‚                            â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                  â”‚                                       â”‚
â”‚         Data Nodes (20+)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Configuration Coordinating Node** :
```yaml
node.roles: []  # Vide = coordinating only
```

**Use Case** : Clusters avec >50 nÅ“uds data oÃ¹ les agrÃ©gations consomment beaucoup de mÃ©moire.

---

# Dimensionnement : RÃ¨gles de Thumb

**Combien de nÅ“uds et quelle taille ?**

| Ressource | Recommandation | Justification |
|-----------|----------------|---------------|
| **Heap JVM** | 50% de RAM, max 31 GB | Au-delÃ  de 32GB, perte de compressed oops |
| **RAM totale** | 2x Heap (reste pour OS cache) | OS cache accÃ©lÃ¨re les lectures disque |
| **CPU** | 8+ cores pour data nodes | Recherches et indexations parallÃ¨les |
| **Disque** | SSD pour hot, HDD pour warm/cold | Latence critique pour indexation |
| **Shards** | 20-50 GB par shard | Trop petits = overhead, trop gros = recovery lent |
| **Shards par nÅ“ud** | < 3000 shards | Au-delÃ , dÃ©gradation performance master |

**Exemple de dimensionnement** :
- 500 GB de donnÃ©es actives
- Shards de 30 GB â†’ 17 shards primaires
- RÃ©plication factor 1 â†’ 17 rÃ©pliques
- Total : 34 shards
- Recommandation : 4-6 data nodes

---
layout: section
---

# Partie 2: Configuration de Haute DisponibilitÃ©

RÃ©plication, rack awareness, et cross-cluster replication

---

# Haute DisponibilitÃ© : Principes

**Haute disponibilitÃ©** = Le cluster continue de fonctionner malgrÃ© des pannes.

**Composants de HA** :
1. **Quorum de masters** : 3+ master-eligible nodes (Ã©viter split-brain)
2. **RÃ©pliques de shards** : 1+ rÃ©pliques par shard primaire
3. **Rack Awareness** : Distribuer rÃ©pliques sur diffÃ©rentes zones de disponibilitÃ©
4. **Load Balancing** : Distribuer requÃªtes sur plusieurs nÅ“uds
5. **Monitoring et Alerting** : DÃ©tecter pannes rapidement

**Formule de quorum** : `(nombre_masters / 2) + 1`
- 3 masters â†’ quorum = 2 (tolÃ©rance : 1 panne)
- 5 masters â†’ quorum = 3 (tolÃ©rance : 2 pannes)

---

# Configuration des RÃ©pliques

**RÃ©pliques** = Copies des shards primaires pour tolÃ©rer pannes et rÃ©partir charge de lecture.

**Configurer le nombre de rÃ©pliques** :

```bash
# Au niveau index
PUT /my-index
{
  "settings": {
    "number_of_shards": 5,
    "number_of_replicas": 2
  }
}

# Modifier un index existant
PUT /my-index/_settings
{
  "number_of_replicas": 2
}
```

**StratÃ©gie de rÃ©plication** :

| Environnement | RÃ©pliques | Justification |
|---------------|-----------|---------------|
| **Dev/Test** | 0 | Performance max, pas de HA requis |
| **Staging** | 1 | Balance entre HA et coÃ»t |
| **Production** | 1-2 | HA standard (2 = tolÃ©rance 2 pannes) |
| **Critique** | 2+ | Mission-critical (finance, santÃ©) |

**Note** : `number_of_replicas = 2` signifie **3 copies totales** (1 primaire + 2 rÃ©pliques)

---

# Rack Awareness (Shard Allocation Awareness)

**ProblÃ¨me** : Si tous les shards primaires et rÃ©pliques sont sur le mÃªme rack/zone â†’ panne du rack = perte de donnÃ©es.

**Solution** : **Rack Awareness** distribue rÃ©pliques sur diffÃ©rentes zones de disponibilitÃ©.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Zone A (Datacenter 1)    Zone B (Datacenter 2)         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚ Primary 0  â”‚            â”‚ Replica 0  â”‚               â”‚
â”‚  â”‚ Replica 1  â”‚            â”‚ Primary 1  â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                          â”‚
â”‚  Si Zone A down â†’ Zone B a toutes les donnÃ©es           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Configuration** :

1. **DÃ©clarer l'attribut d'awareness** dans `elasticsearch.yml` :

```yaml
# Node dans Zone A
node.attr.zone: zone_a
cluster.routing.allocation.awareness.attributes: zone

# Node dans Zone B
node.attr.zone: zone_b
cluster.routing.allocation.awareness.attributes: zone
```

2. **Forcer la distribution** (optionnel mais recommandÃ©) :

```yaml
cluster.routing.allocation.awareness.force.zone.values: zone_a,zone_b
```

Cela force Elasticsearch Ã  **ne jamais allouer** primaire et rÃ©plique sur la mÃªme zone.

---

# Shard Allocation Filtering

**ContrÃ´ler** oÃ¹ les shards sont allouÃ©s selon des attributs personnalisÃ©s.

**Use Cases** :
- Migrer indices vers nouveaux nÅ“uds
- RÃ©server certains nÅ“uds pour indices critiques
- Ã‰vacuer un nÅ“ud avant maintenance

**Attributs personnalisÃ©s** :

```yaml
# elasticsearch.yml
node.attr.type: hot
node.attr.environment: production
```

**Filtrer allocation par index** :

```bash
# Allouer uniquement sur nÅ“uds "hot"
PUT /logs-2024-01/_settings
{
  "index.routing.allocation.require.type": "hot"
}

# Exclure certains nÅ“uds
PUT /logs-2024-01/_settings
{
  "index.routing.allocation.exclude._name": "node-3,node-4"
}

# Inclure uniquement certains nÅ“uds
PUT /logs-2024-01/_settings
{
  "index.routing.allocation.include.environment": "production"
}
```

**Filtrer au niveau cluster** :

```bash
PUT /_cluster/settings
{
  "transient": {
    "cluster.routing.allocation.exclude._ip": "192.168.1.50"
  }
}
```

---

# Cross-Cluster Replication (CCR)

**CCR** rÃ©plique des indices d'un cluster (leader) vers un autre cluster (follower) pour disaster recovery ou gÃ©o-distribution.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Cluster Primary (Paris)       Cluster DR (Londres) â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Leader Index â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€>  â”‚Follower Indexâ”‚     â”‚
â”‚  â”‚  orders      â”‚   RÃ©plicationâ”‚  orders      â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                      â”‚
â”‚  Si Paris down â†’ Basculer vers Londres              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Configuration CCR** :

1. **Configurer le remote cluster** (sur follower) :

```bash
PUT /_cluster/settings
{
  "persistent": {
    "cluster.remote.leader_cluster.seeds": [
      "paris-node1:9300",
      "paris-node2:9300"
    ]
  }
}
```

2. **CrÃ©er un follower index** :

```bash
PUT /orders/_ccr/follow
{
  "remote_cluster": "leader_cluster",
  "leader_index": "orders"
}
```

3. **Surveiller la rÃ©plication** :

```bash
GET /orders/_ccr/stats
```

**Use Cases** :
- **Disaster Recovery** : Cluster de secours dans une autre rÃ©gion
- **GÃ©o-distribution** : DonnÃ©es rÃ©pliquÃ©es prÃ¨s des utilisateurs
- **Reporting** : Cluster de reporting sÃ©parÃ© du cluster de production

---
layout: section
---

# Partie 3: Planification de Disaster Recovery

RPO, RTO, et stratÃ©gies de sauvegarde

---

# RPO et RTO : DÃ©finitions

**RPO (Recovery Point Objective)** : Perte de donnÃ©es maximale acceptable

- RPO = 1 heure â†’ Snapshots toutes les heures
- RPO = 5 minutes â†’ RÃ©plication synchrone (CCR)

**RTO (Recovery Time Objective)** : Temps maximum pour restaurer le service

- RTO = 4 heures â†’ Restauration manuelle acceptable
- RTO = 15 minutes â†’ Cluster de standby requis

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Timeline d'incident                                â”‚
â”‚                                                     â”‚
â”‚  [Incident] â†â”€ RPO â”€â†’ [Last Backup]                â”‚
â”‚      â”‚                                              â”‚
â”‚      â†“                                              â”‚
â”‚  [Recovery Starts] â†â”€ RTO â”€â†’ [Service Restored]    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Exemples par criticitÃ©** :

| Type de donnÃ©es | RPO | RTO | StratÃ©gie |
|-----------------|-----|-----|-----------|
| Logs applicatifs | 24h | 8h | Snapshots quotidiens |
| DonnÃ©es transactionnelles | 1h | 2h | Snapshots horaires + rÃ©pliques |
| DonnÃ©es financiÃ¨res | 5 min | 15 min | CCR + rÃ©pliques multiples |
| DonnÃ©es critiques santÃ© | 0 (sync) | 5 min | CCR synchrone + standby cluster |

---

# StratÃ©gies de Sauvegarde

**3-2-1 Rule** : 3 copies, 2 supports diffÃ©rents, 1 copie off-site

**StratÃ©gie 1 : Snapshots rÃ©guliers** :
- **RPO** : DÃ©pend de la frÃ©quence (1h, 6h, 24h)
- **RTO** : Temps de restauration (15 min - 2h selon taille)

```bash
# SLM policy pour snapshots horaires
PUT /_slm/policy/hourly-snapshots
{
  "schedule": "0 * * * *",
  "name": "<hourly-{now/h}>",
  "repository": "s3_backup",
  "config": {
    "indices": "*"
  },
  "retention": {
    "expire_after": "7d",
    "min_count": 24
  }
}
```

**StratÃ©gie 2 : CCR pour DR** :
- **RPO** : Quasi-temps rÃ©el (< 1 minute)
- **RTO** : Basculement manuel (5-15 minutes)

**StratÃ©gie 3 : Hybrid (Snapshots + CCR)** :
- **CCR** pour recovery rapide
- **Snapshots** pour protection contre corruption logique et conformitÃ©

---

# Tester le Disaster Recovery

**RÃ¨gle d'or** : Un plan DR non testÃ© = pas de plan DR

**Tests rÃ©guliers** :

1. **Test de restauration de snapshot** (mensuel) :
   - Restaurer snapshot dans cluster de test
   - VÃ©rifier intÃ©gritÃ© des donnÃ©es
   - Mesurer le temps de restauration (RTO rÃ©el)

2. **Test de basculement CCR** (trimestriel) :
   - Promouvoir follower index en leader
   - Rediriger applications vers cluster DR
   - Mesurer le temps de basculement

3. **Simulation de panne complÃ¨te** (annuel) :
   - ArrÃªter le cluster primaire
   - Activer cluster DR
   - Valider que les applications fonctionnent

**Documenter les rÃ©sultats** :
- RPO/RTO atteints vs objectifs
- Points de blocage rencontrÃ©s
- Actions correctives

---

# Checklist de Disaster Recovery

**Avant incident** :
- âœ… Snapshots automatisÃ©s (SLM) configurÃ©s et testÃ©s
- âœ… CCR configurÃ© si RPO < 1h requis
- âœ… Runbook de disaster recovery documentÃ© et accessible
- âœ… Ã‰quipe formÃ©e aux procÃ©dures de DR
- âœ… Contacts d'escalation dÃ©finis
- âœ… AccÃ¨s aux credentials de secours disponibles

**Pendant incident** :
1. Ã‰valuer l'ampleur (quels indices/nÅ“uds affectÃ©s ?)
2. DÃ©cider : Restauration locale ou basculement DR ?
3. ExÃ©cuter le runbook appropriÃ©
4. Communiquer status aux stakeholders
5. Logger toutes les actions

**AprÃ¨s incident** :
1. Post-mortem : Cause root, timeline, impact
2. VÃ©rifier intÃ©gritÃ© des donnÃ©es restaurÃ©es
3. Mettre Ã  jour le runbook si nÃ©cessaire
4. Planifier actions prÃ©ventives

---
layout: section
---

# Partie 4: Checklists OpÃ©rationnelles

Pre-deployment, monitoring, et incident response

---

# Pre-Deployment Checklist

**Avant de dÃ©ployer en production** :

**Infrastructure** :
- âœ… Sizing appropriÃ© (CPU, RAM, disque selon charges attendues)
- âœ… Dedicated master nodes (3+ pour quorum)
- âœ… Rack awareness configurÃ© (multi-AZ)
- âœ… Network optimisÃ© (latence < 10ms entre nÅ“uds)
- âœ… Firewall configurÃ© (port 9200, 9300)

**Configuration Elasticsearch** :
- âœ… Heap size = 50% RAM, max 31 GB
- âœ… Swap dÃ©sactivÃ© (`bootstrap.memory_lock: true`)
- âœ… File descriptors â‰¥ 65535
- âœ… Virtual memory `vm.max_map_count` â‰¥ 262144
- âœ… Cluster name unique et meaningful

**SÃ©curitÃ©** :
- âœ… SÃ©curitÃ© activÃ©e (`xpack.security.enabled: true`)
- âœ… TLS/SSL configurÃ© (transport et HTTP)
- âœ… Utilisateurs et rÃ´les crÃ©Ã©s selon principe du moindre privilÃ¨ge
- âœ… Audit logging activÃ©
- âœ… Passwords complexes pour utilisateurs intÃ©grÃ©s

**Haute DisponibilitÃ©** :
- âœ… RÃ©pliques configurÃ©es (1-2 selon criticitÃ©)
- âœ… SLM policies pour snapshots automatiques
- âœ… Repository de snapshots testÃ©
- âœ… CCR configurÃ© si RPO < 1h

**Monitoring** :
- âœ… Stack Monitoring activÃ©
- âœ… Alertes configurÃ©es (cluster health, disk, heap)
- âœ… Dashboards Kibana crÃ©Ã©s pour mÃ©triques clÃ©s
- âœ… IntÃ©gration avec systÃ¨me de monitoring externe (Prometheus, Datadog)

---

# Monitoring Checklist

**MÃ©triques Ã  surveiller en continu** :

**SantÃ© du Cluster** :
- âœ… Cluster status (GREEN / YELLOW / RED)
- âœ… Nombre de nÅ“uds actifs
- âœ… Shards non assignÃ©s
- âœ… Tasks en attente (pending tasks)

**Performance** :
- âœ… Indexing rate (docs/sec)
- âœ… Search rate (queries/sec)
- âœ… Search latency (p95, p99)
- âœ… Indexing latency

**Ressources** :
- âœ… Heap usage (alerte si > 85%)
- âœ… GC frequency et duration (alerte si GC > 5s)
- âœ… Disk usage (alerte si > 85%)
- âœ… CPU usage
- âœ… Network I/O

**DisponibilitÃ©** :
- âœ… Uptime des nÅ“uds
- âœ… Rejected requests (thread pools)
- âœ… Circuit breakers trips

**Seuils d'alerte recommandÃ©s** :

| MÃ©trique | Warning | Critical |
|----------|---------|----------|
| Heap usage | > 75% | > 85% |
| Disk usage | > 75% | > 85% |
| GC duration | > 1s | > 5s |
| Cluster status | YELLOW | RED |
| Pending tasks | > 10 | > 50 |

---

# Incident Response Runbook

**Workflow gÃ©nÃ©ral** :

```
[Alerte] â†’ [Triage] â†’ [Diagnostic] â†’ [Mitigation] â†’ [Resolution] â†’ [Post-Mortem]
```

**Incident 1 : Cluster status RED** :

**SymptÃ´me** : `GET /_cluster/health` retourne `"status": "red"`

**Triage** :
```bash
# Identifier les indices RED
GET /_cat/indices?v&health=red

# Identifier shards non assignÃ©s
GET /_cat/shards?v&h=index,shard,prirep,state,unassigned.reason
```

**Diagnostic** :
```bash
# Pourquoi les shards ne sont pas assignÃ©s ?
GET /_cluster/allocation/explain
{
  "index": "problematic-index",
  "shard": 0,
  "primary": true
}
```

**Causes courantes** :
- NÅ“ud(s) down â†’ Attendre recovery ou forcer allocation
- Disk watermark exceeded â†’ LibÃ©rer espace ou ajouter nÅ“uds
- Shard corruption â†’ Restaurer depuis snapshot

**Mitigation** :
```bash
# Si disk full : Augmenter watermark temporairement
PUT /_cluster/settings
{
  "transient": {
    "cluster.routing.allocation.disk.watermark.low": "95%",
    "cluster.routing.allocation.disk.watermark.high": "97%"
  }
}

# Si corruption : Forcer allocation d'un replica comme primaire
POST /_cluster/reroute
{
  "commands": [{
    "allocate_replica": {
      "index": "my-index",
      "shard": 0,
      "node": "node-2"
    }
  }]
}
```

---

# Incident 2 : Performance DÃ©gradÃ©e

**SymptÃ´me** : Recherches lentes (p95 > 1s), indexation lente

**Diagnostic** :

```bash
# 1. VÃ©rifier les slow logs
GET /slow-index/_settings?include_defaults=false

# 2. Identifier les hot threads
GET /_nodes/hot_threads

# 3. VÃ©rifier les tasks en cours
GET /_cat/tasks?v&detailed

# 4. Analyser les thread pools
GET /_cat/thread_pool?v&h=name,active,rejected,queue
```

**Causes courantes** :
- **RequÃªtes lourdes** : Filtres inefficaces, wildcards
- **Heap pressure** : GC thrashing, circuit breakers
- **Disk I/O** : Merges intensifs, snapshots en cours
- **Shard allocation** : DÃ©sÃ©quilibre des shards

**Mitigation** :

```bash
# Augmenter les thread pools temporairement
PUT /_cluster/settings
{
  "transient": {
    "thread_pool.write.queue_size": 1000
  }
}

# DÃ©sactiver rÃ©plication temporairement (si indexation massive)
PUT /_cluster/settings
{
  "transient": {
    "cluster.routing.allocation.enable": "primaries"
  }
}

# AprÃ¨s indexation, rÃ©activer
PUT /_cluster/settings
{
  "transient": {
    "cluster.routing.allocation.enable": "all"
  }
}
```

---

# Incident 3 : Split-Brain Detection

**SymptÃ´me** : Deux clusters indÃ©pendants se forment (duplication de donnÃ©es, conflits)

**PrÃ©vention** :
```yaml
# elasticsearch.yml
discovery.zen.minimum_master_nodes: 2  # Pour 3 masters (quorum)
```

**Diagnostic** :
```bash
# VÃ©rifier les masters Ã©lus
GET /_cat/master?v

# Comparer cluster state sur diffÃ©rents nÅ“uds
GET /_cluster/state/master_node
```

**RÃ©solution** :
1. **ArrÃªter l'Ã©criture** sur les deux clusters
2. **Identifier le cluster authoritative** (le plus rÃ©cent/complet)
3. **ArrÃªter le cluster non-authoritative**
4. **Fusionner les donnÃ©es** si nÃ©cessaire via restauration
5. **Reconfigurer discovery.seed_hosts** pour Ã©viter rÃ©currence

---

# Post-Incident Actions

**Post-Mortem Template** :

```markdown
# Incident Post-Mortem: [Titre]

**Date**: 2024-01-15
**DurÃ©e**: 2h 30min
**Impact**: Recherches indisponibles pour 10% utilisateurs

## Timeline
- 10:00 : Alerte Cluster RED
- 10:05 : Ã‰quipe Ops notifiÃ©e
- 10:15 : Diagnostic identifie disk full
- 10:45 : Ajout de nÅ“uds data, rÃ©allocation shards
- 12:30 : Cluster GREEN, service restaurÃ©

## Root Cause
Croissance de donnÃ©es imprÃ©vue (3x normal) suite Ã  bug applicatif

## Impact
- 2000 requÃªtes Ã©chouÃ©es
- 0 perte de donnÃ©es (rÃ©pliques OK)

## Actions Correctives
1. ImplÃ©menter alerte sur croissance anormale de donnÃ©es
2. Automatiser ajout de nÅ“uds (scaling horizontal)
3. Fixer le bug applicatif
4. Augmenter disk watermark thresholds

## Lessons Learned
- Besoin de capacity planning plus proactif
- Runbook disk full Ã  mettre Ã  jour
```

---

# RÃ©sumÃ© : Bonnes Pratiques de Production

| Domaine | Best Practice | BÃ©nÃ©fice |
|---------|---------------|----------|
| **Architecture** | Dedicated master nodes (3+) | StabilitÃ© du cluster |
| **Architecture** | Hot-Warm-Cold tiers | Optimisation coÃ»ts |
| **Sizing** | Heap â‰¤ 31 GB, 50% RAM | Performance optimale |
| **HA** | RÃ©pliques + Rack Awareness | TolÃ©rance aux pannes |
| **HA** | CCR pour DR (RPO < 1h) | Recovery rapide |
| **Backup** | SLM automatisÃ© + tests mensuels | Protection donnÃ©es |
| **Monitoring** | Alertes sur heap, disk, status | DÃ©tection prÃ©coce |
| **SÃ©curitÃ©** | TLS + RBAC + Audit logging | ConformitÃ© et protection |
| **OpÃ©rations** | Runbooks documentÃ©s et testÃ©s | RÃ©solution rapide incidents |

**Principe fondamental** : **Concevoir pour la panne** (Design for Failure)

---

# Points ClÃ©s Ã  Retenir

**Architecture** :
- SÃ©parer les rÃ´les (dedicated masters, data tiers)
- Dimensionner selon charges rÃ©elles (load testing)
- Hot-Warm-Cold pour optimiser coÃ»ts

**Haute DisponibilitÃ©** :
- Quorum de masters (3+), rÃ©pliques (1-2)
- Rack awareness pour distribution gÃ©ographique
- CCR pour disaster recovery multi-rÃ©gion

**Disaster Recovery** :
- DÃ©finir RPO/RTO selon criticitÃ© mÃ©tier
- Snapshots automatisÃ©s (SLM) + tests rÃ©guliers
- Runbooks de DR documentÃ©s et pratiquÃ©s

**OpÃ©rations** :
- Checklists pre-deployment rigoureuses
- Monitoring proactif avec alertes
- Incident response runbooks pour scÃ©narios courants
- Post-mortems aprÃ¨s chaque incident

---

# Ressources et Documentation

**Documentation officielle Elasticsearch** :
- [Cluster design](https://www.elastic.co/guide/en/elasticsearch/reference/current/scalability.html)
- [High availability](https://www.elastic.co/guide/en/elasticsearch/reference/current/high-availability.html)
- [Shard allocation awareness](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-cluster.html#shard-allocation-awareness)

**Guides de production** :
- [Production deployment](https://www.elastic.co/guide/en/elasticsearch/reference/current/setup.html)
- [Disaster recovery](https://www.elastic.co/guide/en/elasticsearch/reference/current/snapshot-restore.html)
- [Monitoring best practices](https://www.elastic.co/guide/en/elasticsearch/reference/current/monitor-elasticsearch-cluster.html)

**Blogs et whitepapers** :
- [Elasticsearch Best Practices](https://www.elastic.co/blog/found-elasticsearch-in-production)
- [Sizing Elasticsearch](https://www.elastic.co/elasticon/conf/2016/sf/quantitative-cluster-sizing)
