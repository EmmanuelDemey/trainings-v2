# Cahier d'Exercices Pratiques - Elasticsearch Ops

Formation sur 3 jours - Exercices pratiques et ateliers

---

# Jour 1 - Fondamentaux et Architecture

## Lab 1.1: Cr√©ation et Interrogation d'Index

**Topic**: Concepts G√©n√©raux - Indexation et Recherche
**Pr√©requis**: Cluster Elasticsearch 8.x d√©marr√© et accessible

### Objectif

Cr√©er votre premier index Elasticsearch, y ins√©rer des documents, et ex√©cuter des recherches basiques pour comprendre le fonctionnement de l'index invers√©.

### Contexte

Vous travaillez pour une boutique en ligne qui souhaite indexer son catalogue produits dans Elasticsearch. Vous allez cr√©er un index `products`, y ajouter quelques produits, puis rechercher des articles sp√©cifiques.

### Exercice de Base

#### Setup

**Avant de commencer**:
1. V√©rifiez que votre cluster est accessible: `GET /`
2. V√©rifiez le statut du cluster: `GET /_cluster/health`
3. Le statut doit √™tre `green` ou `yellow` (acceptable en dev avec 1 n≈ìud)

#### √âtapes

**√âtape 1**: Cr√©er l'index `products`

Cr√©ez un index simple sans mapping explicite (mapping dynamique):

```bash
PUT /products
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0
  }
}
```

**R√©sultat attendu**:
```json
{
  "acknowledged": true,
  "shards_acknowledged": true,
  "index": "products"
}
```

**√âtape 2**: Indexer des documents produits

Ins√©rez 5 produits dans l'index:

```bash
POST /products/_doc/1
{
  "name": "Smartphone Galaxy S23",
  "category": "electronics",
  "price": 899.99,
  "stock": 150,
  "description": "Smartphone haut de gamme avec cam√©ra 200MP"
}

POST /products/_doc/2
{
  "name": "Laptop Dell XPS 15",
  "category": "electronics",
  "price": 1499.99,
  "stock": 45,
  "description": "Ordinateur portable professionnel avec √©cran OLED"
}

POST /products/_doc/3
{
  "name": "Chaise de Bureau Ergonomique",
  "category": "furniture",
  "price": 299.99,
  "stock": 80,
  "description": "Chaise ergonomique avec support lombaire ajustable"
}

POST /products/_doc/4
{
  "name": "Clavier M√©canique RGB",
  "category": "electronics",
  "price": 129.99,
  "stock": 200,
  "description": "Clavier gaming avec switches Cherry MX"
}

POST /products/_doc/5
{
  "name": "Bureau Assis-Debout",
  "category": "furniture",
  "price": 599.99,
  "stock": 25,
  "description": "Bureau √©lectrique avec hauteur r√©glable"
}
```

**√âtape 3**: Recherche simple (match query)

Recherchez tous les produits contenant le mot "bureau":

```bash
GET /products/_search
{
  "query": {
    "match": {
      "description": "bureau"
    }
  }
}
```

**R√©sultat attendu**: Devrait retourner les produits 3 et 5 (Chaise de Bureau, Bureau Assis-Debout).

**√âtape 4**: Recherche avec filtrage (bool query)

Recherchez les produits √©lectroniques dont le prix est inf√©rieur √† 1000‚Ç¨:

```bash
GET /products/_search
{
  "query": {
    "bool": {
      "must": [
        { "term": { "category.keyword": "electronics" }}
      ],
      "filter": [
        { "range": { "price": { "lt": 1000 }}}
      ]
    }
  }
}
```

**R√©sultat attendu**: Devrait retourner les produits 1 et 4 (Smartphone, Clavier).

#### Validation

**Commandes de v√©rification**:

1. Compter le nombre total de documents:
```bash
GET /products/_count
```
**R√©sultat attendu**: `"count": 5`

2. V√©rifier le mapping d√©tect√© automatiquement:
```bash
GET /products/_mapping
```
**Observations**: `name` et `description` devraient √™tre de type `text`, `price` de type `float`, `category` de type `text` avec sous-champ `keyword`.

3. Rechercher tous les documents:
```bash
GET /products/_search
{
  "query": { "match_all": {} }
}
```
**R√©sultat attendu**: 5 hits retourn√©s.

#### Crit√®res de Succ√®s

- ‚úÖ Index `products` cr√©√© avec 5 documents
- ‚úÖ Recherche "bureau" retourne 2 r√©sultats
- ‚úÖ Filtre prix <1000‚Ç¨ + category=electronics retourne 2 r√©sultats
- ‚úÖ `_count` retourne exactement 5 documents

#### D√©pannage

**Probl√®me**: "index_not_found_exception"
‚Üí V√©rifiez le nom de l'index (sensible √† la casse)

**Probl√®me**: Aucun r√©sultat pour la recherche "bureau"
‚Üí V√©rifiez que les documents sont bien index√©s avec `GET /products/_search`
‚Üí Attendez 1 seconde (refresh interval par d√©faut) et r√©essayez

**Probl√®me**: Filtre sur `category` ne fonctionne pas
‚Üí Utilisez `category.keyword` au lieu de `category` pour une correspondance exacte

---

## Lab 1.2: D√©finition de Mappings Explicites

**Topic**: Concepts G√©n√©raux - Mappings
**Pr√©requis**: Lab 1.1 compl√©t√©

### Objectif

Cr√©er un index avec un mapping explicite pour contr√¥ler pr√©cis√©ment comment les donn√©es sont index√©es et recherchables.

### Contexte

L'√©quipe marketing souhaite indexer des articles de blog avec des exigences sp√©cifiques: recherche full-text sur le contenu, filtrage exact sur les tags, et recherche g√©ographique sur la localisation de l'auteur.

### Exercice de Base

#### Setup

**Avant de commencer**:
1. Si l'index `blog_posts` existe d√©j√†, supprimez-le: `DELETE /blog_posts`
2. Pr√©parez le mapping en lisant la documentation: https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-types.html

#### √âtapes

**√âtape 1**: Cr√©er l'index avec mapping explicite

```bash
PUT /blog_posts
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0,
    "analysis": {
      "analyzer": {
        "french_analyzer": {
          "type": "french"
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "title": {
        "type": "text",
        "analyzer": "french_analyzer",
        "fields": {
          "keyword": { "type": "keyword" }
        }
      },
      "content": {
        "type": "text",
        "analyzer": "french_analyzer"
      },
      "author": {
        "type": "keyword"
      },
      "tags": {
        "type": "keyword"
      },
      "published_date": {
        "type": "date",
        "format": "yyyy-MM-dd"
      },
      "views": {
        "type": "integer"
      },
      "rating": {
        "type": "float"
      },
      "author_location": {
        "type": "geo_point"
      },
      "metadata": {
        "type": "object",
        "enabled": false
      }
    }
  }
}
```

**R√©sultat attendu**: `"acknowledged": true`

**√âtape 2**: Indexer des articles de blog

```bash
POST /blog_posts/_doc/1
{
  "title": "Introduction √† Elasticsearch",
  "content": "Elasticsearch est un moteur de recherche distribu√© bas√© sur Lucene",
  "author": "Jean Dupont",
  "tags": ["elasticsearch", "search", "tutorial"],
  "published_date": "2023-10-15",
  "views": 1250,
  "rating": 4.5,
  "author_location": {
    "lat": 48.8566,
    "lon": 2.3522
  }
}

POST /blog_posts/_doc/2
{
  "title": "Optimisation des Performances Elasticsearch",
  "content": "D√©couvrez les meilleures pratiques pour optimiser votre cluster",
  "author": "Marie Martin",
  "tags": ["elasticsearch", "performance", "optimization"],
  "published_date": "2023-11-01",
  "views": 890,
  "rating": 4.8,
  "author_location": {
    "lat": 45.764,
    "lon": 4.8357
  }
}

POST /blog_posts/_doc/3
{
  "title": "S√©curit√© dans Elasticsearch 8.x",
  "content": "La s√©curit√© est activ√©e par d√©faut dans Elasticsearch 8",
  "author": "Jean Dupont",
  "tags": ["elasticsearch", "security", "tutorial"],
  "published_date": "2023-11-10",
  "views": 2100,
  "rating": 4.9,
  "author_location": {
    "lat": 48.8566,
    "lon": 2.3522
  }
}
```

**√âtape 3**: Tester les diff√©rents types de champs

**Test 1 - Recherche full-text** (type `text`):
```bash
GET /blog_posts/_search
{
  "query": {
    "match": {
      "content": "optimiser performances"
    }
  }
}
```
**R√©sultat attendu**: Document 2 retourn√© (analyseur French appliqu√©).

**Test 2 - Filtrage exact** (type `keyword`):
```bash
GET /blog_posts/_search
{
  "query": {
    "term": {
      "author": "Jean Dupont"
    }
  }
}
```
**R√©sultat attendu**: Documents 1 et 3 retourn√©s.

**Test 3 - Recherche par date** (type `date`):
```bash
GET /blog_posts/_search
{
  "query": {
    "range": {
      "published_date": {
        "gte": "2023-11-01"
      }
    }
  }
}
```
**R√©sultat attendu**: Documents 2 et 3 retourn√©s.

**Test 4 - Recherche g√©ographique** (type `geo_point`):
```bash
GET /blog_posts/_search
{
  "query": {
    "geo_distance": {
      "distance": "50km",
      "author_location": {
        "lat": 48.8566,
        "lon": 2.3522
      }
    }
  }
}
```
**R√©sultat attendu**: Documents 1 et 3 retourn√©s (auteurs √† Paris).

**√âtape 4**: Tenter d'ajouter un nouveau champ dynamique

```bash
POST /blog_posts/_doc/4
{
  "title": "Nouveau champ non mapp√©",
  "content": "Test de mapping dynamique",
  "author": "Test User",
  "tags": ["test"],
  "published_date": "2023-11-11",
  "views": 0,
  "rating": 3.0,
  "author_location": { "lat": 48.0, "lon": 2.0 },
  "new_field": "Cette valeur sera index√©e dynamiquement"
}
```

V√©rifiez le mapping apr√®s insertion:
```bash
GET /blog_posts/_mapping
```
**Observation**: `new_field` a √©t√© ajout√© automatiquement au mapping (mapping dynamique activ√© par d√©faut).

#### Validation

**Commandes de v√©rification**:

1. V√©rifier le mapping complet:
```bash
GET /blog_posts/_mapping
```
**Crit√®res**: Tous les champs d√©finis doivent √™tre pr√©sents avec les bons types.

2. V√©rifier l'analyzeur French:
```bash
GET /blog_posts/_analyze
{
  "analyzer": "french_analyzer",
  "text": "Les performances sont optimis√©es"
}
```
**R√©sultat attendu**: Les tokens g√©n√©r√©s doivent √™tre sans accents et stemmed (ex: "perform", "optimis").

3. Compter les documents par tag:
```bash
GET /blog_posts/_search
{
  "size": 0,
  "aggs": {
    "tags_count": {
      "terms": { "field": "tags" }
    }
  }
}
```
**R√©sultat attendu**: Tag "elasticsearch" avec 3 documents.

#### Crit√®res de Succ√®s

- ‚úÖ Index cr√©√© avec mapping explicite (7 champs)
- ‚úÖ Recherche full-text fonctionne avec analyzer French
- ‚úÖ Filtrage exact sur `author` retourne les bons documents
- ‚úÖ Recherche g√©ographique dans un rayon de 50km fonctionne
- ‚úÖ Mapping visualis√© montre les types corrects pour chaque champ

#### D√©pannage

**Probl√®me**: "mapper_parsing_exception" lors de la cr√©ation
‚Üí V√©rifiez la syntaxe JSON du mapping
‚Üí Assurez-vous que tous les types de champs sont valides

**Probl√®me**: Recherche full-text ne retourne pas de r√©sultats
‚Üí V√©rifiez que l'analyzer "french_analyzer" est bien d√©fini dans les settings
‚Üí Testez avec `_analyze` API

**Probl√®me**: Recherche g√©ographique √©choue
‚Üí V√©rifiez le format des coordonn√©es: `{ "lat": X, "lon": Y }`
‚Üí Assurez-vous que le type `geo_point` est bien d√©fini

---

## Lab 1.3: Agr√©gations de Donn√©es

**Topic**: Concepts G√©n√©raux - Agr√©gations
**Pr√©requis**: Lab 1.2 compl√©t√© (index `blog_posts` existe)

### Objectif

Utiliser les agr√©gations Elasticsearch pour extraire des statistiques et analyser les donn√©es sans r√©cup√©rer tous les documents.

### Contexte

L'√©quipe analytics souhaite obtenir des statistiques sur les articles de blog: moyenne des vues, distribution par auteur, tendance temporelle des publications, et meilleurs articles par rating.

### Exercice de Base

#### Setup

**Avant de commencer**:
1. V√©rifiez que l'index `blog_posts` contient au moins 3-4 documents
2. Ajoutez quelques documents suppl√©mentaires si n√©cessaire pour enrichir les stats

#### √âtapes

**√âtape 1**: Agr√©gation Metrics - Statistiques sur les vues

Calculez les statistiques (min, max, avg, sum) sur le champ `views`:

```bash
GET /blog_posts/_search
{
  "size": 0,
  "aggs": {
    "views_stats": {
      "stats": {
        "field": "views"
      }
    },
    "avg_views": {
      "avg": {
        "field": "views"
      }
    },
    "max_views": {
      "max": {
        "field": "views"
      }
    }
  }
}
```

**R√©sultat attendu**:
```json
{
  "aggregations": {
    "views_stats": {
      "count": 3,
      "min": 890.0,
      "max": 2100.0,
      "avg": 1413.33,
      "sum": 4240.0
    },
    "avg_views": { "value": 1413.33 },
    "max_views": { "value": 2100.0 }
  }
}
```

**√âtape 2**: Agr√©gation Bucket - Distribution par auteur (Terms)

Groupez les articles par auteur et comptez combien chaque auteur a √©crit:

```bash
GET /blog_posts/_search
{
  "size": 0,
  "aggs": {
    "articles_par_auteur": {
      "terms": {
        "field": "author",
        "size": 10
      }
    }
  }
}
```

**R√©sultat attendu**:
```json
{
  "aggregations": {
    "articles_par_auteur": {
      "buckets": [
        { "key": "Jean Dupont", "doc_count": 2 },
        { "key": "Marie Martin", "doc_count": 1 }
      ]
    }
  }
}
```

**√âtape 3**: Agr√©gation Bucket - Histogramme temporel (Date Histogram)

Groupez les articles par mois de publication:

```bash
GET /blog_posts/_search
{
  "size": 0,
  "aggs": {
    "articles_par_mois": {
      "date_histogram": {
        "field": "published_date",
        "calendar_interval": "month",
        "format": "yyyy-MM"
      }
    }
  }
}
```

**R√©sultat attendu**: Buckets par mois (2023-10, 2023-11) avec doc_count.

**√âtape 4**: Agr√©gations Imbriqu√©es - Stats par auteur

Combinez une agr√©gation bucket (par auteur) avec des agr√©gations metrics (moyenne des vues et ratings):

```bash
GET /blog_posts/_search
{
  "size": 0,
  "aggs": {
    "stats_par_auteur": {
      "terms": {
        "field": "author",
        "size": 10
      },
      "aggs": {
        "avg_views": {
          "avg": { "field": "views" }
        },
        "avg_rating": {
          "avg": { "field": "rating" }
        },
        "total_views": {
          "sum": { "field": "views" }
        }
      }
    }
  }
}
```

**R√©sultat attendu**: Pour chaque auteur, moyenne des vues, moyenne du rating, et total des vues.

**√âtape 5**: Pipeline Aggregation - Moyenne des moyennes

Calculez la moyenne des vues moyennes par auteur (agr√©gation sur agr√©gation):

```bash
GET /blog_posts/_search
{
  "size": 0,
  "aggs": {
    "stats_par_auteur": {
      "terms": {
        "field": "author"
      },
      "aggs": {
        "avg_views": {
          "avg": { "field": "views" }
        }
      }
    },
    "avg_of_avg_views": {
      "avg_bucket": {
        "buckets_path": "stats_par_auteur>avg_views"
      }
    }
  }
}
```

**R√©sultat attendu**: Valeur unique repr√©sentant la moyenne des moyennes.

#### Validation

**Commandes de v√©rification**:

1. V√©rifier la distribution des tags:
```bash
GET /blog_posts/_search
{
  "size": 0,
  "aggs": {
    "tags_distribution": {
      "terms": {
        "field": "tags",
        "size": 20
      }
    }
  }
}
```
**R√©sultat attendu**: Tag "elasticsearch" doit √™tre le plus fr√©quent.

2. Top 3 articles par nombre de vues:
```bash
GET /blog_posts/_search
{
  "size": 0,
  "aggs": {
    "top_articles": {
      "top_hits": {
        "size": 3,
        "_source": ["title", "views"],
        "sort": [{ "views": "desc" }]
      }
    }
  }
}
```

3. Filtrer puis agr√©ger (articles avec rating >4.5):
```bash
GET /blog_posts/_search
{
  "size": 0,
  "query": {
    "range": {
      "rating": { "gte": 4.5 }
    }
  },
  "aggs": {
    "avg_views_good_rating": {
      "avg": { "field": "views" }
    }
  }
}
```

#### Crit√®res de Succ√®s

- ‚úÖ Stats aggregation retourne min, max, avg, sum des vues
- ‚úÖ Terms aggregation par auteur retourne les bons comptes
- ‚úÖ Date histogram groupe les articles par mois
- ‚úÖ Agr√©gations imbriqu√©es retournent stats par auteur
- ‚úÖ Pipeline aggregation calcule la moyenne des moyennes

#### D√©pannage

**Probl√®me**: "fielddata is disabled on text fields"
‚Üí Utilisez le sous-champ `.keyword` pour agr√©ger: `"field": "title.keyword"`

**Probl√®me**: R√©sultats d'agr√©gation vides
‚Üí V√©rifiez que des documents existent: `GET /blog_posts/_count`
‚Üí V√©rifiez le nom du champ: `GET /blog_posts/_mapping`

**Probl√®me**: Pipeline aggregation retourne null
‚Üí V√©rifiez que `buckets_path` pointe vers la bonne agr√©gation parent>child

---

## üåü Bonus 1.A: Optimisation du Scoring de Recherche

**Niveau**: Avanc√©
**Pr√©requis**: Lab 1.1 et 1.2 compl√©t√©s

### Objectif

Personnaliser le scoring de pertinence en utilisant des requ√™tes pond√©r√©es et `function_score` pour prioriser certains documents.

### Contexte

L'√©quipe marketing veut que les articles r√©cents avec un bon rating apparaissent plus haut dans les r√©sultats de recherche, m√™me s'ils matchent moins bien le terme recherch√©.

### Challenge

**Partie 1**: Requ√™te avec poids personnalis√©s (Boosting)

Cr√©ez une requ√™te qui recherche "elasticsearch" mais donne plus de poids aux articles dont le titre contient le terme:

```bash
GET /blog_posts/_search
{
  "query": {
    "bool": {
      "should": [
        {
          "match": {
            "content": {
              "query": "elasticsearch",
              "boost": 1.0
            }
          }
        },
        {
          "match": {
            "title": {
              "query": "elasticsearch",
              "boost": 3.0
            }
          }
        }
      ]
    }
  }
}
```

**Observation**: Comparez les scores avec et sans boost.

**Partie 2**: Function Score - Boost bas√© sur le rating

Utilisez `function_score` pour multiplier le score par le rating de l'article:

```bash
GET /blog_posts/_search
{
  "query": {
    "function_score": {
      "query": {
        "match": { "content": "elasticsearch" }
      },
      "functions": [
        {
          "field_value_factor": {
            "field": "rating",
            "factor": 1.2,
            "modifier": "sqrt",
            "missing": 1
          }
        }
      ],
      "boost_mode": "multiply",
      "score_mode": "sum"
    }
  }
}
```

**Partie 3**: Decay Function - Favoriser les articles r√©cents

Utilisez une fonction de d√©croissance temporelle pour favoriser les publications r√©centes:

```bash
GET /blog_posts/_search
{
  "query": {
    "function_score": {
      "query": {
        "match_all": {}
      },
      "functions": [
        {
          "gauss": {
            "published_date": {
              "origin": "2023-11-11",
              "scale": "30d",
              "decay": 0.5
            }
          }
        }
      ]
    }
  }
}
```

**Explication**: Les articles publi√©s il y a 30 jours auront un score r√©duit de 50%.

### Validation

Cr√©ez une requ√™te combin√©e qui:
1. Recherche le terme "elasticsearch"
2. Boost les titres (√ó3)
3. Multiplie le score par le rating
4. Favorise les articles r√©cents

```bash
GET /blog_posts/_search
{
  "query": {
    "function_score": {
      "query": {
        "bool": {
          "should": [
            { "match": { "content": { "query": "elasticsearch", "boost": 1.0 }}},
            { "match": { "title": { "query": "elasticsearch", "boost": 3.0 }}}
          ]
        }
      },
      "functions": [
        {
          "field_value_factor": {
            "field": "rating",
            "factor": 1.2
          }
        },
        {
          "gauss": {
            "published_date": {
              "origin": "2023-11-11",
              "scale": "30d",
              "decay": 0.5
            }
          }
        }
      ],
      "score_mode": "multiply",
      "boost_mode": "multiply"
    }
  }
}
```

**Crit√®re de succ√®s**: L'article le plus r√©cent avec le meilleur rating devrait appara√Ætre en premier.

---

## üåü Bonus 1.B: Mappings Nested et Parent-Child

**Niveau**: Avanc√©
**Pr√©requis**: Lab 1.2 compl√©t√©

### Objectif

Ma√Ætriser les mappings complexes pour g√©rer des relations entre objets (nested) et entre documents (parent-child).

### Contexte

Vous devez mod√©liser un syst√®me de blog avec commentaires. Deux approches possibles: nested objects (commentaires embarqu√©s dans l'article) ou parent-child (commentaires comme documents s√©par√©s).

### Challenge

**Partie 1**: Mapping Nested Objects

Cr√©ez un index avec des commentaires nested:

```bash
PUT /blog_with_comments
{
  "mappings": {
    "properties": {
      "title": { "type": "text" },
      "content": { "type": "text" },
      "author": { "type": "keyword" },
      "comments": {
        "type": "nested",
        "properties": {
          "user": { "type": "keyword" },
          "message": { "type": "text" },
          "date": { "type": "date" },
          "rating": { "type": "integer" }
        }
      }
    }
  }
}
```

Indexez un article avec commentaires:

```bash
POST /blog_with_comments/_doc/1
{
  "title": "Elasticsearch Nested Objects",
  "content": "Les objets nested permettent de pr√©server les relations",
  "author": "Jean Dupont",
  "comments": [
    {
      "user": "Alice",
      "message": "Excellent article!",
      "date": "2023-11-10",
      "rating": 5
    },
    {
      "user": "Bob",
      "message": "Tr√®s utile",
      "date": "2023-11-11",
      "rating": 4
    }
  ]
}
```

Recherchez des articles avec des commentaires sp√©cifiques (nested query):

```bash
GET /blog_with_comments/_search
{
  "query": {
    "nested": {
      "path": "comments",
      "query": {
        "bool": {
          "must": [
            { "match": { "comments.message": "excellent" }},
            { "range": { "comments.rating": { "gte": 4 }}}
          ]
        }
      }
    }
  }
}
```

**Partie 2**: Nested Aggregations

Agr√©gez sur les commentaires nested (moyenne des ratings):

```bash
GET /blog_with_comments/_search
{
  "size": 0,
  "aggs": {
    "comments_agg": {
      "nested": {
        "path": "comments"
      },
      "aggs": {
        "avg_comment_rating": {
          "avg": { "field": "comments.rating" }
        },
        "top_commenters": {
          "terms": { "field": "comments.user" }
        }
      }
    }
  }
}
```

**Partie 3**: Comparaison avec Array d'Objects Standards

Cr√©ez le m√™me index SANS nested (type object standard):

```bash
PUT /blog_flat_comments
{
  "mappings": {
    "properties": {
      "title": { "type": "text" },
      "comments": {
        "properties": {
          "user": { "type": "keyword" },
          "message": { "type": "text" },
          "rating": { "type": "integer" }
        }
      }
    }
  }
}
```

Indexez les m√™mes donn√©es et essayez la m√™me requ√™te (sans `nested`):

```bash
GET /blog_flat_comments/_search
{
  "query": {
    "bool": {
      "must": [
        { "match": { "comments.message": "excellent" }},
        { "range": { "comments.rating": { "gte": 4 }}}
      ]
    }
  }
}
```

**Observation**: Sans `nested`, Elasticsearch peut matcher "excellent" d'un commentaire et rating ‚â•4 d'un AUTRE commentaire (cross-matching). Avec `nested`, la relation est pr√©serv√©e.

### Validation

**Question 1**: Quand utiliser `nested` vs `object` standard?
- **nested**: Quand il est important de pr√©server les relations entre champs d'un objet
- **object**: Quand les champs sont ind√©pendants ou quand on agr√®ge sur tous les objets ensemble

**Question 2**: Quelle est la limitation principale de `nested`?
- Limite par d√©faut de 50 nested objects par document (configurable avec `index.mapping.nested_objects.limit`)
- Overhead de performance pour indexation et recherche

**Crit√®re de succ√®s**: 
- Comprendre la diff√©rence entre nested et object standard
- Savoir √©crire une nested query et une nested aggregation

---

## Lab 2.1: Formation d'un Cluster Multi-N≈ìuds

**Topic**: Installation et Configuration - Formation de Cluster
**Pr√©requis**: Un n≈ìud Elasticsearch 8.x d√©j√† d√©marr√©

### Objectif

D√©marrer un second n≈ìud Elasticsearch et le joindre au cluster existant en utilisant les enrollment tokens pour former un cluster multi-n≈ìuds s√©curis√©.

### Contexte

Votre cluster √† n≈ìud unique doit √©voluer pour supporter plus de charge et assurer la haute disponibilit√©. Vous allez ajouter un second n≈ìud en utilisant les m√©canismes de s√©curit√© automatique d'Elasticsearch 8.x.

### Exercice de Base

#### Setup

**Avant de commencer**:
1. V√©rifiez que le premier n≈ìud est en cours d'ex√©cution: `GET /`
2. Notez le `cluster_name` du n≈ìud actuel: `GET /_cluster/health`
3. Pr√©parez un second terminal pour le nouveau n≈ìud

#### √âtapes

**√âtape 1**: G√©n√©rer un enrollment token

Depuis le premier n≈ìud, g√©n√©rez un token d'enrollment pour permettre √† un nouveau n≈ìud de rejoindre le cluster:

```bash
cd /path/to/elasticsearch
bin/elasticsearch-create-enrollment-token -s node
```

**R√©sultat attendu**: Un token long (JWT) sera affich√©:
```
eyJ2ZXIiOiI4LjAuMCIsImFkciI6WyIxOTIuMTY4LjEuMTA6OTIwMCJdLCJmZ3IiOiJhYmMxMjMuLi4iLCJrZXkiOiJ4eXo3ODkuLi4ifQ==
```

**Note**: Ce token expire apr√®s 30 minutes. Si expir√©, r√©g√©n√©rez-en un nouveau.

**√âtape 2**: Pr√©parer le r√©pertoire du second n≈ìud

Cr√©ez un nouveau r√©pertoire pour le second n≈ìud (pour simulation locale):

```bash
# Option 1: Copier l'installation Elasticsearch
cp -r elasticsearch-8.x elasticsearch-node2

# Option 2: Utiliser la m√™me installation avec des r√©pertoires data s√©par√©s
# (configuration via elasticsearch.yml)
```

**√âtape 3**: D√©marrer le second n≈ìud avec l'enrollment token

D√©marrez le nouveau n≈ìud en passant le token:

```bash
cd elasticsearch-node2
bin/elasticsearch --enrollment-token <VOTRE_TOKEN>
```

**R√©sultat attendu**: Le n≈ìud d√©marre et affiche des logs indiquant:
```
[INFO ][o.e.n.Node] [node-2] started
[INFO ][o.e.c.s.ClusterApplierService] [node-2] detected_master {node-1}{...}
```

**√âtape 4**: V√©rifier la formation du cluster

V√©rifiez que les deux n≈ìuds sont visibles dans le cluster:

```bash
GET /_cat/nodes?v
```

**R√©sultat attendu**:
```
ip           heap.percent ram.percent cpu load_1m node.role master name
192.168.1.10 45           60          2   0.50    cdfhilmrstw *      node-1
192.168.1.11 30           55          1   0.40    cdfhilmrstw -      node-2
```

L'ast√©risque (*) indique le n≈ìud master √©lu.

**√âtape 5**: V√©rifier le statut du cluster

V√©rifiez que le cluster est pass√© en statut `green`:

```bash
GET /_cluster/health
```

**R√©sultat attendu**:
```json
{
  "cluster_name": "elasticsearch",
  "status": "green",
  "number_of_nodes": 2,
  "number_of_data_nodes": 2,
  "active_primary_shards": 5,
  "active_shards": 10,
  "unassigned_shards": 0
}
```

**Pourquoi `green` ?** Avec 2 n≈ìuds, les replicas des shards peuvent maintenant √™tre allou√©s sur le second n≈ìud (r√©plication fonctionnelle).

#### Validation

**Commandes de v√©rification**:

1. Lister tous les n≈ìuds avec leurs r√¥les:
```bash
GET /_cat/nodes?v&h=name,ip,node.role,master,heap.percent,ram.percent
```

2. V√©rifier l'allocation des shards entre les n≈ìuds:
```bash
GET /_cat/shards?v
```
**Observation**: Les shards primaires et replicas doivent √™tre r√©partis entre les 2 n≈ìuds.

3. V√©rifier les d√©tails du cluster:
```bash
GET /_cluster/stats?human&pretty
```
**R√©sultat attendu**: `"number_of_nodes": 2`

4. Tester la r√©silience (optionnel):
```bash
# Cr√©er un index avec 1 replica
PUT /test-resilience
{
  "settings": {
    "number_of_shards": 2,
    "number_of_replicas": 1
  }
}

# V√©rifier l'allocation
GET /_cat/shards/test-resilience?v
```
**Observation**: Chaque shard primaire a un replica sur l'autre n≈ìud.

#### Crit√®res de Succ√®s

- ‚úÖ Enrollment token g√©n√©r√© avec succ√®s
- ‚úÖ Second n≈ìud d√©marr√© et rejoint le cluster
- ‚úÖ `GET /_cat/nodes` affiche 2 n≈ìuds
- ‚úÖ Cluster status = `green`
- ‚úÖ Shards r√©pliqu√©s sur les deux n≈ìuds

#### D√©pannage

**Probl√®me**: "Enrollment token has expired"
‚Üí Le token expire apr√®s 30 minutes. R√©g√©n√©rez-en un nouveau avec `elasticsearch-create-enrollment-token -s node`

**Probl√®me**: Le second n≈ìud d√©marre mais ne rejoint pas le cluster
‚Üí V√©rifiez la connectivit√© r√©seau entre les n≈ìuds (port 9300 pour transport)
‚Üí V√©rifiez les logs du second n≈ìud: `tail -f logs/elasticsearch.log`
‚Üí Assurez-vous que `cluster.name` est identique sur les deux n≈ìuds

**Probl√®me**: Cluster reste en statut `yellow`
‚Üí Normal si vous n'avez qu'un seul n≈ìud et des replicas configur√©s
‚Üí Avec 2 n≈ìuds, v√©rifiez qu'aucun shard n'est unassigned: `GET /_cat/shards?h=index,shard,prirep,state,unassigned.reason`

**Probl√®me**: "security_exception" lors de requ√™tes API
‚Üí Elasticsearch 8.x active la s√©curit√© par d√©faut. Utilisez les credentials g√©n√©r√©s au premier d√©marrage
‚Üí Ou d√©sactivez temporairement: `xpack.security.enabled: false` (NON recommand√© en production)

---

## Lab 2.2: Configuration des R√¥les de N≈ìuds

**Topic**: Installation et Configuration - R√¥les de N≈ìuds
**Pr√©requis**: Lab 2.1 compl√©t√© (cluster √† 2 n≈ìuds)

### Objectif

Configurer un n≈ìud avec des r√¥les sp√©cifiques (master-only, data-only) en modifiant `elasticsearch.yml` pour optimiser l'architecture du cluster.

### Contexte

Votre cluster grandit et vous souhaitez s√©parer les responsabilit√©s: n≈ìuds master d√©di√©s pour la gestion du cluster, et n≈ìuds data d√©di√©s pour le stockage. Cette s√©paration am√©liore la stabilit√© et les performances.

### Exercice de Base

#### Setup

**Avant de commencer**:
1. Arr√™tez le second n≈ìud (celui d√©marr√© dans Lab 2.1)
2. Localisez le fichier `config/elasticsearch.yml` du second n≈ìud
3. Faites une sauvegarde: `cp elasticsearch.yml elasticsearch.yml.backup`

#### √âtapes

**√âtape 1**: Configurer un n≈ìud data-only

√âditez `elasticsearch.yml` du second n≈ìud pour en faire un n≈ìud data-only:

```yaml
# config/elasticsearch.yml (node-2)

cluster.name: elasticsearch
node.name: data-node-1

# D√©finir les r√¥les (data uniquement, pas master)
node.roles: [ data, ingest ]

# Configuration r√©seau (ajustez selon votre environnement)
network.host: 0.0.0.0
http.port: 9201
transport.port: 9301

# D√©couverte
discovery.seed_hosts: ["localhost:9300"]
```

**Explication des r√¥les**:
- `data`: Stockage et recherche de donn√©es
- `ingest`: Preprocessing de documents (pipelines)
- Absence de `master`: Ce n≈ìud ne participera PAS √† l'√©lection du master

**√âtape 2**: Red√©marrer le n≈ìud avec la nouvelle configuration

```bash
bin/elasticsearch
```

**R√©sultat attendu**: Le n≈ìud d√©marre et rejoint le cluster avec ses nouveaux r√¥les.

**√âtape 3**: V√©rifier les r√¥les des n≈ìuds

```bash
GET /_cat/nodes?v&h=name,node.role,master
```

**R√©sultat attendu**:
```
name         node.role   master
node-1       cdfhilmrstw *
data-node-1  di          -
```

**L√©gende des r√¥les**:
- `c` = cold_data
- `d` = data
- `f` = frozen_data
- `h` = hot_data
- `i` = ingest
- `l` = ml (machine learning)
- `m` = master
- `r` = remote_cluster_client
- `s` = content_data
- `t` = transform
- `w` = warm_data

**√âtape 4**: Cr√©er un n≈ìud master-only (simulation)

Si vous avez un troisi√®me environnement, configurez un n≈ìud master-only:

```yaml
# config/elasticsearch.yml (node-3)

cluster.name: elasticsearch
node.name: master-node-1

# Master uniquement
node.roles: [ master ]

network.host: 0.0.0.0
http.port: 9202
transport.port: 9302

discovery.seed_hosts: ["localhost:9300", "localhost:9301"]
cluster.initial_master_nodes: ["node-1", "master-node-1"]
```

**Ressources recommand√©es**:
- Master-only: 2-4 cores, 8 GB RAM, 50 GB disque
- Data-only: 8-16 cores, 64 GB RAM, 1+ TB disque SSD

**√âtape 5**: V√©rifier l'allocation des shards

V√©rifiez que les shards ne sont allou√©s QUE sur les n≈ìuds data:

```bash
GET /_cat/shards?v&h=index,shard,prirep,state,node
```

**R√©sultat attendu**: Tous les shards doivent √™tre sur `data-node-1` ou `node-1` (si node-1 a le r√¥le `data`).

#### Validation

**Commandes de v√©rification**:

1. D√©tails complets des r√¥les:
```bash
GET /_nodes?filter_path=nodes.*.name,nodes.*.roles
```

**R√©sultat attendu**:
```json
{
  "nodes": {
    "abc123": {
      "name": "node-1",
      "roles": ["master", "data", "ingest", ...]
    },
    "def456": {
      "name": "data-node-1",
      "roles": ["data", "ingest"]
    }
  }
}
```

2. V√©rifier que le n≈ìud master-only ne stocke PAS de donn√©es:
```bash
GET /_cat/allocation?v&h=node,shards,disk.used
```

**Observation**: Un n≈ìud master-only doit avoir `shards: 0`.

3. Tester l'√©lection du master:
```bash
GET /_cat/master?v
```
**R√©sultat attendu**: Seuls les n≈ìuds avec r√¥le `master` peuvent √™tre √©lus.

#### Crit√®res de Succ√®s

- ‚úÖ N≈ìud data-only configur√© avec `node.roles: [data, ingest]`
- ‚úÖ `_cat/nodes` affiche les r√¥les corrects pour chaque n≈ìud
- ‚úÖ Shards ne sont PAS allou√©s sur les n≈ìuds master-only
- ‚úÖ Cluster fonctionne normalement apr√®s changement de r√¥les

#### D√©pannage

**Probl√®me**: N≈ìud refuse de d√©marrer apr√®s changement de r√¥les
‚Üí V√©rifiez la syntaxe YAML (indentation, pas de tabs)
‚Üí Consultez les logs: `tail -f logs/elasticsearch.log`
‚Üí Erreur commune: `cluster.initial_master_nodes` doit √™tre retir√© apr√®s la premi√®re initialisation

**Probl√®me**: Shards restent sur le n≈ìud master-only
‚Üí Les shards existants ne migrent pas automatiquement. Forcez la r√©allocation:
```bash
POST /_cluster/reroute
```

**Probl√®me**: "master_not_discovered_exception"
‚Üí Au moins un n≈ìud avec r√¥le `master` doit √™tre actif
‚Üí V√©rifiez `discovery.seed_hosts` pour que les n≈ìuds se trouvent

---

## Lab 2.3: Inspection du Cluster avec les _cat APIs

**Topic**: Installation et Configuration - APIs de V√©rification
**Pr√©requis**: Lab 2.1 compl√©t√© (cluster √† 2+ n≈ìuds)

### Objectif

Ma√Ætriser les _cat APIs pour inspecter rapidement l'√©tat du cluster, des indices, des shards, et de l'allocation de ressources.

### Contexte

En tant qu'administrateur, vous devez diagnostiquer r√©guli√®rement l'√©tat du cluster. Les _cat APIs fournissent une vue concise et lisible pour identifier rapidement les probl√®mes.

### Exercice de Base

#### Setup

**Avant de commencer**:
1. Assurez-vous que votre cluster a au moins 2 n≈ìuds actifs
2. Cr√©ez quelques index de test avec des donn√©es:

```bash
# Cr√©er un index avec 2 shards, 1 replica
PUT /logs-2023.11
{
  "settings": {
    "number_of_shards": 2,
    "number_of_replicas": 1
  }
}

# Indexer quelques documents
POST /logs-2023.11/_bulk
{"index":{}}
{"message":"Log entry 1","timestamp":"2023-11-10T10:00:00"}
{"index":{}}
{"message":"Log entry 2","timestamp":"2023-11-10T10:01:00"}
{"index":{}}
{"message":"Log entry 3","timestamp":"2023-11-10T10:02:00"}
```

#### √âtapes

**√âtape 1**: Lister tous les indices avec _cat/indices

```bash
GET /_cat/indices?v
```

**R√©sultat attendu**:
```
health status index        pri rep docs.count store.size
green  open   logs-2023.11   2   1          3       15kb
green  open   products       1   0          5       10kb
```

**Colonnes cl√©s**:
- `health`: green/yellow/red
- `pri`: Nombre de shards primaires
- `rep`: Nombre de replicas
- `docs.count`: Nombre de documents
- `store.size`: Taille totale (primaires + replicas)

**√âtape 2**: Filtrer et trier les indices

Afficher uniquement les indices avec plus de 10 GB, tri√©s par taille:

```bash
GET /_cat/indices?v&s=store.size:desc&h=index,health,docs.count,store.size
```

**Personnalisation**:
- `s=colonne:desc`: Tri par colonne (desc ou asc)
- `h=col1,col2`: S√©lection des colonnes √† afficher
- `v`: Affiche les headers (verbose)

**√âtape 3**: Inspecter l'allocation des shards avec _cat/shards

```bash
GET /_cat/shards?v
```

**R√©sultat attendu**:
```
index        shard prirep state   node
logs-2023.11 0     p      STARTED node-1
logs-2023.11 0     r      STARTED data-node-1
logs-2023.11 1     p      STARTED data-node-1
logs-2023.11 1     r      STARTED node-1
```

**Colonnes cl√©s**:
- `prirep`: `p` (primary) ou `r` (replica)
- `state`: STARTED, RELOCATING, INITIALIZING, UNASSIGNED
- `node`: N≈ìud h√©bergeant le shard

**√âtape 4**: Identifier les shards probl√©matiques

Filtrer uniquement les shards UNASSIGNED:

```bash
GET /_cat/shards?v&h=index,shard,prirep,state,unassigned.reason | grep UNASSIGNED
```

**Raisons d'unassignment courantes**:
- `INDEX_CREATED`: Nouveau shard, allocation en cours
- `NODE_LEFT`: N≈ìud d√©connect√©, r√©allocation n√©cessaire
- `REPLICA_ADDED`: Replica ajout√©, recherche de n≈ìud disponible
- `ALLOCATION_FAILED`: √âchec d'allocation (disque plein, r√®gles d'allocation)

**√âtape 5**: V√©rifier l'utilisation disque avec _cat/allocation

```bash
GET /_cat/allocation?v
```

**R√©sultat attendu**:
```
shards disk.indices disk.used disk.avail disk.total node
     4         15kb      2gb       48gb       50gb node-1
     4         15kb      1.5gb     48.5gb     50gb data-node-1
```

**M√©triques cl√©s**:
- `shards`: Nombre de shards sur ce n≈ìud
- `disk.used`: Espace disque utilis√©
- `disk.avail`: Espace disque disponible
- `disk.total`: Capacit√© disque totale

**Alerte**: Si `disk.used` >85%, le watermark LOW est atteint (plus de nouveaux shards).

**√âtape 6**: Surveiller les pending tasks avec _cat/pending_tasks

```bash
GET /_cat/pending_tasks?v
```

**R√©sultat attendu** (si aucune t√¢che en attente):
```
insertOrder timeInQueue priority source
```

**Si des t√¢ches sont en attente**:
```
insertOrder timeInQueue priority source
       1234        10s     URGENT  shard-started
```

**Interpr√©tation**: Des pending tasks avec `timeInQueue` >10s indiquent un master surcharg√©.

#### Validation

**Commandes de v√©rification**:

1. Comparer _cat/indices et _cat/shards pour un index:
```bash
GET /_cat/indices/logs-2023.11?v
GET /_cat/shards/logs-2023.11?v
```
**V√©rification**: `pri √ó (1 + rep)` = nombre total de shards dans _cat/shards.

2. Exporter les r√©sultats en JSON (pour scripts):
```bash
GET /_cat/nodes?format=json
GET /_cat/indices?format=json&pretty
```

3. Utiliser help pour d√©couvrir toutes les colonnes disponibles:
```bash
GET /_cat/indices?help
GET /_cat/nodes?help
```

**Exemple de sortie**:
```
health                | h                              | current health status
status                | s                              | open/close status
index                 | i,idx                          | index name
...
```

#### Crit√®res de Succ√®s

- ‚úÖ _cat/indices liste tous les indices avec sant√© et taille
- ‚úÖ _cat/shards montre l'allocation des shards entre n≈ìuds
- ‚úÖ _cat/allocation affiche l'utilisation disque par n≈ìud
- ‚úÖ Capable de filtrer et trier les r√©sultats avec `?h=` et `?s=`
- ‚úÖ Capable d'identifier les shards UNASSIGNED et leur raison

#### D√©pannage

**Probl√®me**: "No handler found for uri [/_cat/...]"
‚Üí V√©rifiez l'orthographe de l'API (sensible √† la casse)
‚Üí Exemple correct: `/_cat/indices` (pas `/_cat/index`)

**Probl√®me**: Colonnes d√©salign√©es dans la sortie
‚Üí Utilisez `?v` pour afficher les headers
‚Üí Utilisez `?format=json` pour une sortie structur√©e

**Probl√®me**: Trop de colonnes, sortie illisible
‚Üí Utilisez `?h=col1,col2,col3` pour s√©lectionner uniquement les colonnes n√©cessaires
‚Üí Exemple: `GET /_cat/nodes?v&h=name,heap.percent,ram.percent,cpu,load_1m`

---

## üåü Bonus 2.A: Shard Allocation Awareness

**Niveau**: Avanc√©
**Pr√©requis**: Lab 2.1 et 2.2 compl√©t√©s

### Objectif

Configurer la "shard allocation awareness" pour r√©partir intelligemment les shards en fonction d'attributs personnalis√©s (zone de disponibilit√©, rack serveur) et forcer la relocation de shards.

### Contexte

Votre cluster Elasticsearch est d√©ploy√© sur plusieurs zones de disponibilit√© (AZ1, AZ2, AZ3). Vous souhaitez garantir que les replicas ne sont JAMAIS sur la m√™me zone que leur primaire (r√©silience aux pannes de zone).

### Challenge

**Partie 1**: D√©finir des attributs personnalis√©s

√âditez `elasticsearch.yml` de chaque n≈ìud pour d√©finir un attribut `zone`:

```yaml
# N≈ìud 1 (AZ1)
node.name: node-az1
node.attr.zone: az1

# N≈ìud 2 (AZ2)
node.name: node-az2
node.attr.zone: az2

# N≈ìud 3 (AZ3 - optionnel)
node.name: node-az3
node.attr.zone: az3
```

Red√©marrez les n≈ìuds pour appliquer la configuration.

**V√©rification**:
```bash
GET /_cat/nodeattrs?v&h=node,attr,value
```

**R√©sultat attendu**:
```
node      attr  value
node-az1  zone  az1
node-az2  zone  az2
node-az3  zone  az3
```

**Partie 2**: Activer la shard allocation awareness

Configurez le cluster pour √™tre "aware" de l'attribut `zone`:

```bash
PUT /_cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.awareness.attributes": "zone"
  }
}
```

**Effet**: Elasticsearch √©vitera de placer un replica sur le m√™me `zone` que son primaire.

**Partie 3**: Forcer l'allocation avec forced awareness

Pour garantir qu'au moins un shard est dans chaque zone:

```bash
PUT /_cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.awareness.attributes": "zone",
    "cluster.routing.allocation.awareness.force.zone.values": "az1,az2,az3"
  }
}
```

**Effet**: Si une zone devient indisponible, Elasticsearch NE r√©allouera PAS les replicas manquants sur les autres zones (attend le retour de la zone).

**Partie 4**: Cr√©er un index et v√©rifier la distribution

```bash
PUT /zone-aware-index
{
  "settings": {
    "number_of_shards": 3,
    "number_of_replicas": 1
  }
}

# V√©rifier l'allocation
GET /_cat/shards/zone-aware-index?v&h=index,shard,prirep,state,node
```

**Observation**: Pour chaque shard primaire, son replica est sur un n≈ìud avec un `zone` diff√©rent.

**Partie 5**: Forcer la relocation d'un shard

Identifiez un shard √† d√©placer:

```bash
GET /_cat/shards/zone-aware-index?v&h=index,shard,prirep,node
```

Forcez la relocation d'un shard primaire du n≈ìud A vers le n≈ìud B:

```bash
POST /_cluster/reroute
{
  "commands": [
    {
      "move": {
        "index": "zone-aware-index",
        "shard": 0,
        "from_node": "node-az1",
        "to_node": "node-az2"
      }
    }
  ]
}
```

**R√©sultat attendu**: Le shard 0 commence √† se d√©placer (√©tat RELOCATING), puis atteint STARTED sur node-az2.

**Suivi de la relocation**:
```bash
GET /_cat/recovery/zone-aware-index?v&h=index,shard,stage,source_node,target_node
```

**Partie 6**: Exclure un n≈ìud de l'allocation

Simulez la mise en maintenance d'un n≈ìud en excluant tous les shards:

```bash
PUT /_cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.exclude._name": "node-az1"
  }
}
```

**Effet**: Tous les shards quittent `node-az1` et sont r√©allou√©s sur les autres n≈ìuds.

**Retour √† la normale**:
```bash
PUT /_cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.exclude._name": null
  }
}
```

### Validation

**Questions √† r√©pondre**:

1. **Quelle est la diff√©rence entre `awareness` et `forced awareness` ?**
   - `awareness`: Pr√©f√©rence, Elasticsearch essaie de respecter les zones mais r√©allouera ailleurs si n√©cessaire
   - `forced awareness`: Strict, Elasticsearch refuse de r√©allouer si la zone cible n'est pas disponible

2. **Quand utiliser `cluster.routing.allocation.exclude` ?**
   - Mise en maintenance d'un n≈ìud (vidage des shards avant arr√™t)
   - Retrait progressif d'un n≈ìud du cluster
   - Isolation d'un n≈ìud probl√©matique

3. **Comment annuler une relocation manuelle ?**
   - Utilisez `cancel` dans `_cluster/reroute`:
   ```bash
   POST /_cluster/reroute
   {
     "commands": [
       {
         "cancel": {
           "index": "zone-aware-index",
           "shard": 0,
           "node": "node-az2"
         }
       }
     ]
   }
   ```

**Crit√®re de succ√®s**: 
- Comprendre les strat√©gies d'allocation awareness
- Savoir forcer la relocation de shards manuellement
- Ma√Ætriser l'exclusion de n≈ìuds pour maintenance

---


---

# Jour 2 - Performance, Optimisation et Production

## Lab 3.1: Dimensionnement de Cluster - Calcul du Nombre de Shards

**Topic**: Performance et Dimensionnement - Planification de Capacit√©
**Pr√©requis**: Compr√©hension des concepts de shards et replicas

### Objectif

Apprendre √† calculer le nombre optimal de shards pour un cas d'usage donn√© en prenant en compte le volume de donn√©es, la croissance, et les contraintes de performance.

### Contexte

Vous √™tes charg√© de dimensionner un cluster Elasticsearch pour un syst√®me de logs applicatifs. L'√©quipe vous fournit les exigences suivantes et vous devez d√©terminer la configuration optimale.

### Exercice de Base

#### Sc√©nario

**Cas d'usage**: Logs d'application e-commerce

**Exigences**:
- Volume initial: 500 GB de logs
- Croissance: 50 GB/jour (nouveaux logs)
- R√©tention: 30 jours
- Replicas: 1 (haute disponibilit√©)
- Taux d'indexation: 10,000 documents/seconde (pics)
- Taux de recherche: 100 requ√™tes/seconde
- Latence cible: p95 < 200ms pour les recherches

**Infrastructure disponible**:
- N≈ìuds data: 5 n≈ìuds
- CPU par n≈ìud: 16 cores
- RAM par n≈ìud: 64 GB (31 GB heap, 33 GB OS cache)
- Disque par n≈ìud: 2 TB SSD

#### Setup (Si Lab 2.x non fait)

Ce lab est un exercice de calcul et de conception (papier/crayon ou tableur). Il ne n√©cessite pas de cluster actif, mais comprendre l'architecture d'un cluster (Lab 2.1/2.2) aide grandement.

#### √âtapes

**√âtape 1**: Calculer le volume total apr√®s 30 jours

```
Volume initial:     500 GB
Croissance (30j):   50 GB/jour √ó 30 = 1,500 GB
Volume total:       500 + 1,500 = 2,000 GB
```

Avec 1 replica (√ó2):
```
Volume avec replicas: 2,000 GB √ó 2 = 4,000 GB
```

**√âtape 2**: D√©terminer la taille cible d'un shard

**R√®gles de sizing**:
- ‚úÖ Taille optimale: 10-50 GB par shard
- ‚ö†Ô∏è Maximum recommand√©: 50 GB (au-del√†, performance d√©grad√©e)
- ‚ö†Ô∏è Minimum recommand√©: 1 GB (trop de petits shards = overhead)

**Choix**: 30 GB par shard (milieu de la plage optimale)

**√âtape 3**: Calculer le nombre de shards primaires n√©cessaires

```
Nombre de shards primaires = Volume total (sans replicas) / Taille cible par shard
                            = 2,000 GB / 30 GB
                            = 66.67
                            ‚âà 67 shards primaires
```

**√âtape 4**: V√©rifier la contrainte de shards par n≈ìud

**R√®gle**: Maximum 20 shards par GB de heap JVM

```
Heap par n≈ìud:       31 GB
Max shards/n≈ìud:     31 GB √ó 20 = 620 shards
Shards totaux:       67 primaires + 67 replicas = 134 shards
Shards par n≈ìud:     134 / 5 n≈ìuds = 26.8 ‚âà 27 shards/n≈ìud
```

**Validation**: 27 shards/n≈ìud << 620 max ‚Üí ‚úÖ OK

**√âtape 5**: Strat√©gie d'indexation - Index par jour (Time-Based Indices)

Au lieu d'un seul gros index, utilisez des index quotidiens:

```
Pattern: logs-YYYY.MM.DD
Exemple: logs-2023.11.10

Volume par jour:     50 GB
Taille par shard:    30 GB
Shards par index:    50 / 30 = 1.67 ‚âà 2 shards primaires par index
```

**Configuration recommand√©e**:
```bash
# Template pour tous les index logs-*
PUT /_index_template/logs-template
{
  "index_patterns": ["logs-*"],
  "template": {
    "settings": {
      "number_of_shards": 2,
      "number_of_replicas": 1,
      "refresh_interval": "5s"
    }
  }
}
```

**Avantages**:
- ‚úÖ Suppression facile des vieux logs (DELETE index entier)
- ‚úÖ R√©duction de la taille de l'index (recherches plus rapides)
- ‚úÖ Gestion ILM simplifi√©e (Index Lifecycle Management)

**√âtape 6**: Valider avec les contraintes de performance

**Indexation**:
```
Taux cible:          10,000 docs/sec
Nombre de n≈ìuds:     5
Load par n≈ìud:       2,000 docs/sec
```
Avec 16 cores/n≈ìud, chaque core g√®re ~125 docs/sec ‚Üí ‚úÖ Faisable

**Recherche**:
```
Taux cible:          100 requ√™tes/sec
Shards actifs/jour:  2 primaires + 2 replicas = 4 shards
```
Latence d√©pend de la complexit√© des requ√™tes, mais avec SSD et cache OS: ‚úÖ p95 < 200ms atteignable

#### Validation

**Formule de v√©rification finale**:

```
Volume total (30j avec replicas) = 4,000 GB
Capacit√© cluster (5 √ó 2TB)       = 10,000 GB (10 TB)
Utilisation disque               = 4,000 / 10,000 = 40%
```

**Marge de s√©curit√©**: 60% disponible ‚Üí ‚úÖ Excellent (recommand√© >20%)

**V√©rification des watermarks**:
```
Disk usage:          40%
Watermark LOW:       85% (pas encore atteint)
Watermark HIGH:      90% (safe)
Watermark FLOOD:     95% (safe)
```

**Tableau r√©capitulatif**:

| M√©trique | Valeur | Validation |
|----------|--------|------------|
| Volume total (avec replicas) | 4,000 GB | ‚úÖ |
| Shards primaires par jour | 2 | ‚úÖ |
| Shards totaux (30 jours) | 120 (60p + 60r) | ‚úÖ |
| Shards par n≈ìud | 24 | ‚úÖ (< 620 max) |
| Utilisation disque | 40% | ‚úÖ (< 85%) |
| Indexation par n≈ìud | 2,000 docs/sec | ‚úÖ |
| Latence recherche (estim√©e) | < 200ms (p95) | ‚úÖ |

#### Crit√®res de Succ√®s

- ‚úÖ Volume total calcul√© correctement (4 TB avec replicas)
- ‚úÖ Taille de shard dans la plage optimale (30 GB)
- ‚úÖ Nombre de shards par n≈ìud < 620 (r√®gle 20/GB heap)
- ‚úÖ Utilisation disque < 85% (watermark safe)
- ‚úÖ Strat√©gie time-based indices adopt√©e (logs-YYYY.MM.DD)

#### D√©pannage

**Probl√®me**: Trop de shards (>1000 dans le cluster)
‚Üí Augmentez la taille cible par shard (40-50 GB au lieu de 30 GB)
‚Üí R√©duisez la r√©tention (20 jours au lieu de 30)
‚Üí Utilisez ILM pour forcer-merger les vieux index (r√©duire les segments)

**Probl√®me**: Disque plein trop rapidement
‚Üí Activez la compression: `"index.codec": "best_compression"`
‚Üí R√©duisez le nombre de replicas sur les index anciens (0 replica apr√®s 7 jours)
‚Üí Archivez dans S3 avec searchable snapshots (Elasticsearch 7.10+)

**Probl√®me**: Latence de recherche >200ms
‚Üí R√©duisez le nombre de shards (moins de shards √† interroger)
‚Üí Utilisez des filtres cach√©s (bool query avec filter context)
‚Üí Ajoutez du routing pour limiter les shards scann√©s

---

## Lab 3.2: Configuration du Heap JVM

**Topic**: Performance et Dimensionnement - Configuration Syst√®me
**Pr√©requis**: Acc√®s au serveur Elasticsearch, droits root/sudo

### Objectif

Configurer correctement le heap JVM d'Elasticsearch en respectant les r√®gles de sizing (50% RAM, max 32GB, Xms=Xmx) et v√©rifier l'application de la configuration.

### Contexte

Votre serveur Elasticsearch dispose de 64 GB de RAM. Vous devez configurer le heap JVM de mani√®re optimale pour √©quilibrer la m√©moire entre le heap (JVM) et le cache OS (filesystem cache).

### Exercice de Base

#### Setup

**Avant de commencer**:
1. V√©rifiez la RAM totale du serveur:
```bash
free -h
# ou
cat /proc/meminfo | grep MemTotal
```

2. Localisez le fichier `jvm.options`:
```bash
# Installation par package (Debian/Ubuntu)
/etc/elasticsearch/jvm.options

# Installation par archive
config/jvm.options
```

3. Arr√™tez Elasticsearch:
```bash
sudo systemctl stop elasticsearch
# ou
bin/elasticsearch stop
```

#### √âtapes

**√âtape 1**: Calculer le heap optimal

**R√®gles de sizing**:
1. ‚úÖ **50% de la RAM**: Le heap doit √™tre au maximum 50% de la RAM physique
2. ‚úÖ **Maximum 32 GB**: Ne jamais d√©passer 32 GB (limite compressed oops)
3. ‚úÖ **Xms = Xmx**: Les deux valeurs doivent √™tre identiques (√©vite resizing)

**Pour un serveur avec 64 GB de RAM**:
```
RAM totale:          64 GB
50% de la RAM:       32 GB
Maximum recommand√©:  32 GB

Heap configur√©:      31 GB (laisse 1 GB de marge pour la JVM elle-m√™me)
OS cache:            33 GB (le reste)
```

**√âtape 2**: Modifier jvm.options

√âditez le fichier `jvm.options`:

```bash
sudo vi /etc/elasticsearch/jvm.options
```

Modifiez les lignes `-Xms` et `-Xmx`:

```
################################################################
## IMPORTANT: JVM heap size
################################################################

# Xms represents the initial size of total heap space
# Xmx represents the maximum size of total heap space

-Xms31g
-Xmx31g
```

**Important**: 
- Utilisez `g` pour gigabytes (pas `GB`)
- Les deux valeurs DOIVENT √™tre identiques
- Commentez les anciennes valeurs au lieu de les supprimer (backup)

**√âtape 3**: V√©rifier les autres param√®tres JVM critiques

Dans le m√™me fichier, v√©rifiez que ces param√®tres sont pr√©sents:

```
# Utiliser G1GC (garbage collector recommand√©)
-XX:+UseG1GC

# Heap dump en cas d'OutOfMemoryError
-XX:+HeapDumpOnOutOfMemoryError
-XX:HeapDumpPath=/var/lib/elasticsearch

# GC logging
-Xlog:gc*,gc+age=trace,safepoint:file=/var/log/elasticsearch/gc.log:utctime,pid,tags:filecount=32,filesize=64m
```

**√âtape 4**: Red√©marrer Elasticsearch

```bash
sudo systemctl start elasticsearch
# ou
bin/elasticsearch -d
```

**V√©rifiez les logs de d√©marrage**:
```bash
sudo tail -f /var/log/elasticsearch/elasticsearch.log
```

Cherchez cette ligne:
```
[INFO ][o.e.e.NodeEnvironment] heap size [31gb], compressed ordinary object pointers [true]
```

**√âtape 5**: V√©rifier la configuration heap via l'API

```bash
GET /_nodes/stats/jvm?filter_path=nodes.*.jvm.mem.heap_max_in_bytes
```

**R√©sultat attendu**:
```json
{
  "nodes": {
    "abc123": {
      "jvm": {
        "mem": {
          "heap_max_in_bytes": 33285996544
        }
      }
    }
  }
}
```

**Conversion**:
```
33,285,996,544 bytes / (1024^3) = 31 GB ‚úÖ
```

**√âtape 6**: Monitorer l'utilisation du heap

```bash
GET /_nodes/stats/jvm?filter_path=nodes.*.jvm.mem.heap_used_percent
```

**R√©sultat**:
```json
{
  "nodes": {
    "abc123": {
      "jvm": {
        "mem": {
          "heap_used_percent": 45
        }
      }
    }
  }
}
```

**Interpr√©tation**:
- ‚úÖ <75%: Sain, pas de probl√®me
- ‚ö†Ô∏è 75-85%: Surveiller, GC plus fr√©quents
- ‚ùå >85%: Critique, risque d'OutOfMemoryError

#### Validation

**Commandes de v√©rification**:

1. V√©rifier heap min et max:
```bash
GET /_nodes/stats/jvm?filter_path=nodes.*.jvm.mem.heap_max_in_bytes,nodes.*.jvm.mem.heap_committed_in_bytes
```
**Crit√®re**: `heap_committed_in_bytes` doit √™tre √©gal √† `heap_max_in_bytes` (Xms=Xmx)

2. V√©rifier compressed oops (< 32 GB):
```bash
GET /_nodes?filter_path=nodes.*.jvm.using_compressed_ordinary_object_pointers
```
**R√©sultat attendu**: `"using_compressed_ordinary_object_pointers": true`

3. V√©rifier GC stats:
```bash
GET /_nodes/stats/jvm?filter_path=nodes.*.jvm.gc
```
**Observation**: 
- `collection_time_in_millis` doit cro√Ætre lentement
- `collection_count` / uptime doit √™tre faible (<10 GC/minute)

4. Tester sous charge (optionnel):
```bash
# Indexer 10,000 documents
for i in {1..10000}; do
  curl -X POST "localhost:9200/test/_doc" -H 'Content-Type: application/json' -d'
  {
    "field": "value",
    "number": '$i'
  }
  '
done

# V√©rifier heap apr√®s indexation
GET /_nodes/stats/jvm?filter_path=nodes.*.jvm.mem.heap_used_percent
```

#### Crit√®res de Succ√®s

- ‚úÖ Heap configur√© √† 31 GB (50% de 64 GB RAM, <32 GB)
- ‚úÖ Xms = Xmx (v√©rifi√© avec heap_committed = heap_max)
- ‚úÖ Compressed oops activ√© (true)
- ‚úÖ Elasticsearch d√©marre sans erreur
- ‚úÖ Heap usage < 75% en fonctionnement normal

#### D√©pannage

**Probl√®me**: Elasticsearch ne d√©marre pas apr√®s modification
‚Üí V√©rifiez les logs: `sudo journalctl -u elasticsearch -f`
‚Üí Erreur courante: Syntaxe invalide dans jvm.options (pas d'espace, pas de quotes)
‚Üí Rollback: Restaurez les anciennes valeurs et red√©marrez

**Probl√®me**: "Could not reserve enough space for object heap"
‚Üí Le heap demand√© d√©passe la RAM disponible
‚Üí R√©duisez Xms/Xmx (essayez 16g au lieu de 31g)
‚Üí V√©rifiez la RAM r√©ellement disponible: `free -h`

**Probl√®me**: Compressed oops = false
‚Üí Heap configur√© > 32 GB
‚Üí R√©duisez √† 31 GB maximum

**Probl√®me**: Heap usage constamment >85%
‚Üí Cluster sous-dimensionn√©, ajoutez des n≈ìuds
‚Üí Ou augmentez le heap (si RAM disponible et <32 GB)
‚Üí Ou optimisez les requ√™tes et l'indexation

---

## Lab 3.3: Analyse des Thread Pools et Rejections

**Topic**: Performance et Dimensionnement - Optimisation des Ressources
**Pr√©requis**: Cluster Elasticsearch avec quelques donn√©es

### Objectif

Analyser les statistiques des thread pools pour identifier les rejections (requ√™tes rejet√©es) et comprendre leurs implications sur la performance du cluster.

### Contexte

Votre cluster subit des pics de charge et certaines requ√™tes √©chouent avec des erreurs "EsRejectedExecutionException". Vous devez diagnostiquer quel thread pool est satur√© et proposer des solutions.

### Exercice de Base

#### Setup

**Avant de commencer**:
1. Assurez-vous que le cluster traite quelques requ√™tes (indexation ou recherche)
2. Si n√©cessaire, g√©n√©rez de la charge:

```bash
# Script simple de charge (indexation)
for i in {1..1000}; do
  curl -X POST "localhost:9200/load-test/_doc" -H 'Content-Type: application/json' -d'
  {
    "timestamp": "'$(date -Iseconds)'",
    "value": '$RANDOM'
  }
  ' &
done
```

#### √âtapes

**√âtape 1**: Lister tous les thread pools

```bash
GET /_cat/thread_pool?v
```

**R√©sultat attendu**:
```
node_name name            active queue rejected
node-1    analyze              0     0        0
node-1    fetch_shard_started  0     0        0
node-1    fetch_shard_store    0     0        0
node-1    flush                0     0        0
node-1    force_merge          0     0        0
node-1    generic              0     0        0
node-1    get                  0     0        0
node-1    management           1     0        0
node-1    refresh              0     0        0
node-1    search               2    10        5
node-1    search_throttled     0     0        0
node-1    snapshot             0     0        0
node-1    warmer               0     0        0
node-1    write                3     5        0
```

**Colonnes cl√©s**:
- `active`: Nombre de threads actuellement en cours d'ex√©cution
- `queue`: Nombre de t√¢ches en attente dans la queue
- `rejected`: Nombre de t√¢ches rejet√©es (cumul depuis le d√©marrage)

**√âtape 2**: Filtrer les thread pools importants

Affichez uniquement les thread pools critiques:

```bash
GET /_cat/thread_pool/write,search,get?v&h=node_name,name,active,queue,rejected,completed
```

**R√©sultat**:
```
node_name name   active queue rejected completed
node-1    write       3     5        0   1234567
node-1    search      2    10        5    987654
node-1    get         0     0        0     12345
```

**√âtape 3**: Analyser les rejections en d√©tail

```bash
GET /_nodes/stats/thread_pool?filter_path=nodes.*.thread_pool.write,nodes.*.thread_pool.search
```

**R√©sultat (extrait)**:
```json
{
  "nodes": {
    "abc123": {
      "thread_pool": {
        "write": {
          "threads": 16,
          "queue": 5,
          "active": 3,
          "rejected": 0,
          "largest": 16,
          "completed": 1234567
        },
        "search": {
          "threads": 25,
          "queue": 10,
          "active": 2,
          "rejected": 5,
          "largest": 25,
          "completed": 987654
        }
      }
    }
  }
}
```

**Analyse**:
- **write**: 0 rejections ‚Üí ‚úÖ Indexation OK
- **search**: 5 rejections ‚Üí ‚ö†Ô∏è Recherche satur√©e (queue pleine)

**√âtape 4**: Calculer le taux de rejection

```
Taux de rejection = rejected / (completed + rejected) √ó 100%

Pour le thread pool search:
= 5 / (987654 + 5) √ó 100%
= 0.0005%
```

**Interpr√©tation**:
- <0.1%: Acceptable (pics occasionnels)
- 0.1-1%: Attention (surcharge r√©guli√®re)
- >1%: Critique (cluster sous-dimensionn√©)

**√âtape 5**: Identifier la cause des rejections

**Questions √† poser**:

1. **Le thread pool est-il √† sa capacit√© max ?**
```bash
GET /_cat/thread_pool/search?v&h=node_name,active,threads
```
Si `active` ‚âà `threads` ‚Üí Pool satur√©

2. **La queue est-elle pleine ?**
```bash
GET /_nodes/stats/thread_pool?filter_path=nodes.*.thread_pool.search.queue,nodes.*.thread_pool.search.queue_size
```
Si `queue` ‚âà `queue_size` (1000 par d√©faut) ‚Üí Queue satur√©e

3. **Quelle est la charge du cluster ?**
```bash
GET /_nodes/stats/os?filter_path=nodes.*.os.cpu.percent
```
Si CPU >80% ‚Üí Surcharge globale

**√âtape 6**: Simuler une saturation (pour comprendre)

G√©n√©rez une charge importante:

```bash
# 100 requ√™tes de recherche en parall√®le
for i in {1..100}; do
  curl -X GET "localhost:9200/_search?pretty" -H 'Content-Type: application/json' -d'
  {
    "query": {
      "match_all": {}
    },
    "size": 1000
  }
  ' &
done

# Imm√©diatement apr√®s, v√©rifiez les rejections
GET /_cat/thread_pool/search?v&h=node_name,active,queue,rejected
```

**Observation**: Vous devriez voir `rejected` augmenter si le cluster est satur√©.

#### Validation

**Commandes de v√©rification**:

1. Comparer rejections avant/apr√®s charge:
```bash
# Snapshot initial
GET /_nodes/stats/thread_pool?filter_path=nodes.*.thread_pool.*.rejected > before.json

# G√©n√©rer charge...

# Snapshot final
GET /_nodes/stats/thread_pool?filter_path=nodes.*.thread_pool.*.rejected > after.json

# Comparer avec diff ou manuellement
```

2. Identifier les n≈ìuds avec le plus de rejections:
```bash
GET /_cat/thread_pool/write,search?v&h=node_name,name,rejected&s=rejected:desc
```

3. Calculer le ratio rejected/completed pour tous les pools:
```bash
GET /_nodes/stats/thread_pool
# Analyser manuellement ou avec un script
```

#### Crit√®res de Succ√®s

- ‚úÖ Capable de lister les thread pools avec `_cat/thread_pool`
- ‚úÖ Identifier les thread pools avec rejections (search, write)
- ‚úÖ Calculer le taux de rejection (rejected / completed)
- ‚úÖ Comprendre la cause (pool satur√©, queue pleine, CPU √©lev√©)
- ‚úÖ Proposer des solutions adapt√©es

#### Solutions aux Rejections

**Si thread pool WRITE satur√©**:
- ‚úÖ Augmenter le refresh_interval (r√©duire charge d'indexation)
- ‚úÖ Utiliser Bulk API avec batches appropri√©s (5-15 MB)
- ‚úÖ Ajouter des n≈ìuds data (scale horizontal)
- ‚ùå NE PAS augmenter thread pool size (masque le probl√®me)

**Si thread pool SEARCH satur√©**:
- ‚úÖ Optimiser les requ√™tes (utiliser filter context)
- ‚úÖ R√©duire le nombre de shards (moins de overhead)
- ‚úÖ Ajouter des n≈ìuds data ou coordinating-only
- ‚úÖ Impl√©menter un rate limiting c√¥t√© application
- ‚ùå NE PAS augmenter queue_size (augmente seulement la latence)

**Si rejections occasionnelles (<0.1%)**:
- ‚úÖ Acceptable, pics normaux
- ‚úÖ Impl√©menter retry logic c√¥t√© client (avec backoff exponentiel)

#### D√©pannage

**Probl√®me**: Aucune rejection visible mais requ√™tes √©chouent
‚Üí V√©rifiez les logs Elasticsearch: `tail -f /var/log/elasticsearch/elasticsearch.log`
‚Üí Recherchez "EsRejectedExecutionException"
‚Üí Les rejections sont cumulatives depuis le d√©marrage, un red√©marrage les remet √† z√©ro

**Probl√®me**: Rejections m√™me avec CPU/RAM disponibles
‚Üí Bottleneck peut √™tre ailleurs (disque I/O, r√©seau)
‚Üí V√©rifiez disk I/O: `iostat -x 1` (Linux)
‚Üí V√©rifiez latence r√©seau entre n≈ìuds

**Probl√®me**: Rejections sur un seul n≈ìud du cluster
‚Üí R√©partition d√©s√©quilibr√©e des shards
‚Üí Utilisez `_cluster/reroute` pour √©quilibrer
‚Üí V√©rifiez que tous les n≈ìuds ont la m√™me capacit√©

---

## üåü Bonus 3.A: Architecture Hot-Warm-Cold avec ILM

**Niveau**: Avanc√©
**Pr√©requis**: Labs 2.2 et 3.1 compl√©t√©s

### Objectif

Concevoir une architecture hot-warm-cold pour optimiser co√ªt/performance, et configurer des policies Index Lifecycle Management (ILM) pour automatiser les transitions.

### Contexte

Votre cluster stocke des logs avec des patterns d'acc√®s variables: les logs r√©cents (<7 jours) sont consult√©s fr√©quemment (hot), les logs moyens (7-30 jours) occasionnellement (warm), et les vieux logs (>30 jours) rarement (cold). Vous voulez optimiser les co√ªts en utilisant du mat√©riel diff√©rent par tier.

### Challenge

**Partie 1**: Configurer les node attributes pour les tiers

D√©finissez un attribut `data_tier` sur chaque n≈ìud:

```yaml
# N≈ìuds HOT (haute performance)
node.name: hot-node-1
node.roles: [ data_hot ]
# Pas besoin de node.attr.data_tier avec data_hot role

# N≈ìuds WARM (performance moyenne)
node.name: warm-node-1
node.roles: [ data_warm ]

# N≈ìuds COLD (basse performance, stockage √©conomique)
node.name: cold-node-1
node.roles: [ data_cold ]
```

Red√©marrez les n≈ìuds et v√©rifiez:

```bash
GET /_cat/nodes?v&h=name,node.role
```

**R√©sultat attendu**:
```
name         node.role
hot-node-1   h
warm-node-1  w
cold-node-1  c
```

**Partie 2**: Cr√©er une policy ILM

D√©finissez une policy qui transition hot‚Üíwarm‚Üícold‚Üídelete:

```bash
PUT /_ilm/policy/logs-policy
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover": {
            "max_size": "50GB",
            "max_age": "1d",
            "max_docs": 10000000
          },
          "set_priority": {
            "priority": 100
          }
        }
      },
      "warm": {
        "min_age": "7d",
        "actions": {
          "shrink": {
            "number_of_shards": 1
          },
          "forcemerge": {
            "max_num_segments": 1
          },
          "set_priority": {
            "priority": 50
          },
          "allocate": {
            "require": {
              "data": "warm"
            }
          }
        }
      },
      "cold": {
        "min_age": "30d",
        "actions": {
          "searchable_snapshot": {
            "snapshot_repository": "my-repository"
          },
          "set_priority": {
            "priority": 0
          }
        }
      },
      "delete": {
        "min_age": "90d",
        "actions": {
          "delete": {}
        }
      }
    }
  }
}
```

**Explication des phases**:
- **hot** (0-7j): Rollover automatique quand 50GB ou 1 jour atteint
- **warm** (7-30j): Shrink √† 1 shard, force merge, d√©place vers n≈ìuds warm
- **cold** (30-90j): Convert to searchable snapshot (S3/GCS)
- **delete** (>90j): Suppression automatique

**Partie 3**: Cr√©er un index template avec ILM

```bash
PUT /_index_template/logs-template
{
  "index_patterns": ["logs-*"],
  "data_stream": {},
  "template": {
    "settings": {
      "number_of_shards": 2,
      "number_of_replicas": 1,
      "index.lifecycle.name": "logs-policy",
      "index.lifecycle.rollover_alias": "logs"
    }
  }
}
```

**Partie 4**: Cr√©er le premier index et l'alias

```bash
# Cr√©er l'index initial
PUT /logs-000001
{
  "settings": {
    "number_of_shards": 2,
    "number_of_replicas": 1,
    "index.lifecycle.name": "logs-policy",
    "index.lifecycle.rollover_alias": "logs"
  },
  "aliases": {
    "logs": {
      "is_write_index": true
    }
  }
}
```

**Partie 5**: Tester le rollover

Indexez des donn√©es via l'alias:

```bash
POST /logs/_doc
{
  "timestamp": "2023-11-10T10:00:00",
  "message": "Test log entry"
}
```

Forcez un rollover manuel (pour test):

```bash
POST /logs/_rollover
{
  "conditions": {
    "max_age": "1d",
    "max_docs": 1000,
    "max_size": "5GB"
  }
}
```

V√©rifiez les index cr√©√©s:

```bash
GET /_cat/indices/logs-*?v&h=index,health,status,docs.count,store.size
```

**R√©sultat attendu**:
```
index        health status docs.count store.size
logs-000001  green  open          999       1mb
logs-000002  green  open            1      5kb
```

**Partie 6**: Simuler les transitions de phase

Modifiez temporairement les d√©lais pour voir les transitions:

```bash
PUT /_ilm/policy/logs-policy
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover": {
            "max_docs": 100
          }
        }
      },
      "warm": {
        "min_age": "1m",  # 1 minute au lieu de 7 jours
        "actions": {
          "forcemerge": {
            "max_num_segments": 1
          }
        }
      }
    }
  }
}
```

Attendez 1-2 minutes et v√©rifiez:

```bash
GET /logs-*/_ilm/explain
```

**R√©sultat**:
```json
{
  "indices": {
    "logs-000001": {
      "index": "logs-000001",
      "managed": true,
      "policy": "logs-policy",
      "phase": "warm",
      "action": "forcemerge",
      "step": "forcemerge"
    }
  }
}
```

### Validation

**Tableau de comparaison Hot-Warm-Cold**:

| Tier | Hardware | Cas d'usage | Co√ªt | Performance |
|------|----------|-------------|------|-------------|
| **Hot** | SSD NVMe, 64GB RAM, 16 cores | Logs <7j, indexation + recherche intensive | ‚Ç¨‚Ç¨‚Ç¨ | Tr√®s haute |
| **Warm** | SSD SATA, 32GB RAM, 8 cores | Logs 7-30j, recherche occasionnelle | ‚Ç¨‚Ç¨ | Moyenne |
| **Cold** | HDD ou S3, 16GB RAM, 4 cores | Logs >30j, archivage, recherche rare | ‚Ç¨ | Basse |

**Questions √† r√©pondre**:

1. **Quand utiliser shrink dans la phase warm ?**
   - ‚úÖ Quand les donn√©es ne changent plus (read-only)
   - ‚úÖ Pour r√©duire le nombre de shards et am√©liorer les recherches
   - ‚ùå PAS sur des index actifs (write)

2. **Qu'est-ce qu'un searchable snapshot ?**
   - Index stock√© dans un object store (S3, GCS, Azure Blob)
   - Donn√©es charg√©es √† la demande (on-demand)
   - Co√ªt de stockage tr√®s r√©duit (~90% moins cher que EBS)

3. **Comment forcer une transition imm√©diate ?**
```bash
POST /logs-000001/_ilm/move_to_step
{
  "current_step": {
    "phase": "hot",
    "action": "complete",
    "name": "complete"
  },
  "next_step": {
    "phase": "warm",
    "action": "allocate",
    "name": "allocate"
  }
}
```

**Crit√®re de succ√®s**: 
- Comprendre l'architecture hot-warm-cold
- Savoir cr√©er une ILM policy multi-phases
- Ma√Ætriser les actions: rollover, shrink, forcemerge, searchable_snapshot

---

## üåü Bonus 3.B: Troubleshooting Slow Indexing Performance

**Niveau**: Avanc√©
**Pr√©requis**: Lab 3.2 compl√©t√©, compr√©hension du Bulk API

### Objectif

Diagnostiquer et r√©soudre les probl√®mes de performance d'indexation en utilisant les techniques d'optimisation: Bulk API, refresh_interval tuning, et disable replicas temporairement.

### Contexte

Votre pipeline d'indexation est lent (~1,000 docs/sec au lieu des 10,000 attendus). Vous devez identifier les goulots d'√©tranglement et appliquer les optimisations appropri√©es.

### Challenge

**Partie 1**: Benchmark de l'indexation initiale

Cr√©ez un index de test:

```bash
PUT /indexing-perf-test
{
  "settings": {
    "number_of_shards": 2,
    "number_of_replicas": 1,
    "refresh_interval": "1s"
  }
}
```

Indexez 10,000 documents UN PAR UN (m√©thode lente):

```bash
time for i in {1..10000}; do
  curl -X POST "localhost:9200/indexing-perf-test/_doc" \
    -H 'Content-Type: application/json' \
    -d'{"field":"value","number":'$i'}' \
    -s -o /dev/null
done
```

**Temps mesur√©** (exemple): `real 5m30s` ‚Üí **30 docs/sec** ‚ùå

**Partie 2**: Optimisation 1 - Utiliser Bulk API

Supprimez l'index et recr√©ez-le:

```bash
DELETE /indexing-perf-test
PUT /indexing-perf-test
{
  "settings": {
    "number_of_shards": 2,
    "number_of_replicas": 1,
    "refresh_interval": "1s"
  }
}
```

Indexez avec Bulk API (batches de 1000 docs):

```bash
# G√©n√©rer le fichier bulk
for i in {1..10000}; do
  echo '{"index":{}}'
  echo '{"field":"value","number":'$i'}'
done > bulk-data.json

# Indexer avec Bulk
time curl -X POST "localhost:9200/indexing-perf-test/_bulk" \
  -H 'Content-Type: application/x-ndjson' \
  --data-binary @bulk-data.json \
  -s -o /dev/null
```

**Temps mesur√©** (exemple): `real 0m15s` ‚Üí **666 docs/sec** ‚úÖ (√ó22 plus rapide)

**Partie 3**: Optimisation 2 - Augmenter refresh_interval

```bash
PUT /indexing-perf-test/_settings
{
  "index.refresh_interval": "30s"
}
```

R√©indexez avec le m√™me Bulk:

```bash
time curl -X POST "localhost:9200/indexing-perf-test/_bulk" \
  -H 'Content-Type: application/x-ndjson' \
  --data-binary @bulk-data.json \
  -s -o /dev/null
```

**Temps mesur√©** (exemple): `real 0m8s` ‚Üí **1,250 docs/sec** ‚úÖ (√ó2 am√©lioration)

**Explication**: Refresh toutes les 30s au lieu de 1s = moins de segments cr√©√©s = moins d'overhead.

**Partie 4**: Optimisation 3 - D√©sactiver les replicas pendant l'indexation

```bash
PUT /indexing-perf-test/_settings
{
  "number_of_replicas": 0
}
```

R√©indexez:

```bash
time curl -X POST "localhost:9200/indexing-perf-test/_bulk" \
  -H 'Content-Type: application/x-ndjson' \
  --data-binary @bulk-data.json \
  -s -o /dev/null
```

**Temps mesur√©** (exemple): `real 0m4s` ‚Üí **2,500 docs/sec** ‚úÖ (√ó2 am√©lioration)

**Important**: R√©tablir les replicas apr√®s indexation:

```bash
PUT /indexing-perf-test/_settings
{
  "number_of_replicas": 1
}
```

**Partie 5**: Optimisation 4 - Ajuster la taille des batches Bulk

Testez diff√©rentes tailles:

```bash
# Batch de 100 docs
split -l 200 bulk-data.json bulk-100-

# Batch de 5000 docs
split -l 10000 bulk-data.json bulk-5000-

# Mesurer la performance pour chaque
```

**R√®gle g√©n√©rale**: 5-15 MB par batch ou 1000-5000 documents.

**Partie 6**: Identifier d'autres goulots d'√©tranglement

V√©rifiez les m√©triques d'indexation:

```bash
GET /_nodes/stats/indices?filter_path=nodes.*.indices.indexing
```

**Analyse**:
- `indexing.index_time_in_millis`: Temps total pass√© √† indexer
- `indexing.index_failed`: Documents √©chou√©s (doit √™tre 0)
- `indexing.throttle_time_in_millis`: Temps pass√© en throttling (merge)

Si `throttle_time` est √©lev√©:

```bash
# Augmenter le threshold pour les merges
PUT /_cluster/settings
{
  "persistent": {
    "indices.store.throttle.max_bytes_per_sec": "200mb"
  }
}
```

### Validation

**Tableau r√©capitulatif des optimisations**:

| Technique | Am√©lioration | Quand l'utiliser |
|-----------|--------------|------------------|
| Bulk API | √ó20-50 | Toujours pour indexation massive |
| refresh_interval: 30s | √ó2-3 | Indexation initiale, r√©indexation |
| replicas: 0 temporaire | √ó2 | Indexation initiale uniquement |
| Batch size optimal | √ó1.5-2 | Ajuster selon network/heap |
| Disable _source | √ó1.2-1.5 | Si _source non n√©cessaire (rare) |

**Score final de performance**:

```
Baseline (1 par 1):           30 docs/sec
Avec toutes optimisations: 2,500 docs/sec
Am√©lioration totale:          √ó83
```

**Crit√®re de succ√®s**: 
- Comprendre l'impact de chaque optimisation
- Savoir quand appliquer chaque technique
- Mesurer et comparer les performances avant/apr√®s

---

## Lab 6.1: Cr√©ation et Restauration de Snapshots

**Objectif**: Ma√Ætriser la configuration de repositories de snapshots, la cr√©ation de sauvegardes, et la restauration d'indices pour assurer la protection des donn√©es.

**Contexte**: Les snapshots sont essentiels pour prot√©ger vos donn√©es contre les suppressions accidentelles, les corruptions, et les pannes mat√©rielles. Dans ce lab, vous allez configurer un repository filesystem, cr√©er plusieurs snapshots, et pratiquer diff√©rents sc√©narios de restauration.

### √âtape 1: Configurer le Chemin du Repository

Avant de cr√©er un repository, vous devez d√©clarer le chemin autoris√© dans `elasticsearch.yml`.

**Sur un cluster Docker/local** :

1. Localiser le fichier de configuration :

```bash
# Docker
docker exec -it elasticsearch-node-1 cat /usr/share/elasticsearch/config/elasticsearch.yml

# Installation locale
cat /etc/elasticsearch/elasticsearch.yml
```

2. Ajouter la configuration `path.repo` :

```yaml
# Configuration pour snapshots
path.repo: ["/usr/share/elasticsearch/backups"]
```

3. **Pour Docker**, cr√©er le r√©pertoire et monter le volume :

```bash
# Cr√©er le r√©pertoire sur l'h√¥te
mkdir -p ~/elasticsearch-backups

# Red√©marrer le conteneur avec le volume
docker run -d \
  --name elasticsearch-node-1 \
  -p 9200:9200 \
  -p 9300:9300 \
  -e "discovery.type=single-node" \
  -e "path.repo=/usr/share/elasticsearch/backups" \
  -v ~/elasticsearch-backups:/usr/share/elasticsearch/backups \
  docker.elastic.co/elasticsearch/elasticsearch:8.12.0
```

4. **Pour installation locale**, cr√©er le r√©pertoire et d√©finir les permissions :

```bash
sudo mkdir -p /mnt/elasticsearch/backups
sudo chown elasticsearch:elasticsearch /mnt/elasticsearch/backups
sudo chmod 775 /mnt/elasticsearch/backups
```

5. Red√©marrer Elasticsearch pour appliquer la configuration :

```bash
# Docker
docker restart elasticsearch-node-1

# systemd
sudo systemctl restart elasticsearch
```

### √âtape 2: Cr√©er un Repository de Snapshots

Une fois le chemin configur√©, cr√©ez le repository via l'API :

```bash
PUT /_snapshot/my_backup
{
  "type": "fs",
  "settings": {
    "location": "/usr/share/elasticsearch/backups",
    "compress": true,
    "chunk_size": "128mb",
    "max_restore_bytes_per_sec": "40mb",
    "max_snapshot_bytes_per_sec": "40mb"
  }
}
```

**R√©sultat attendu** :
```json
{
  "acknowledged": true
}
```

**V√©rifier le repository** :

```bash
GET /_snapshot/my_backup
```

**R√©sultat attendu** :
```json
{
  "my_backup": {
    "type": "fs",
    "settings": {
      "location": "/usr/share/elasticsearch/backups",
      "compress": "true",
      "chunk_size": "128mb",
      "max_restore_bytes_per_sec": "40mb",
      "max_snapshot_bytes_per_sec": "40mb"
    }
  }
}
```

**Tester la connectivit√© du repository** :

```bash
POST /_snapshot/my_backup/_verify
```

**R√©sultat attendu** :
```json
{
  "nodes": {
    "abc123xyz": {
      "name": "elasticsearch-node-1"
    }
  }
}
```

### √âtape 3: Cr√©er des Donn√©es de Test

Cr√©ons plusieurs indices avec des donn√©es pour tester les snapshots :

```bash
# Index 1: Produits
PUT /products
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0
  }
}

POST /products/_bulk
{"index":{"_id":"1"}}
{"name":"Laptop","price":999,"category":"electronics"}
{"index":{"_id":"2"}}
{"name":"Mouse","price":25,"category":"electronics"}
{"index":{"_id":"3"}}
{"name":"Desk Chair","price":199,"category":"furniture"}
{"index":{"_id":"4"}}
{"name":"Monitor","price":299,"category":"electronics"}
{"index":{"_id":"5"}}
{"name":"Keyboard","price":79,"category":"electronics"}

# Index 2: Commandes
PUT /orders
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0
  }
}

POST /orders/_bulk
{"index":{"_id":"1"}}
{"order_id":"ORD-001","customer":"Alice","total":999,"date":"2024-01-15"}
{"index":{"_id":"2"}}
{"order_id":"ORD-002","customer":"Bob","total":324,"date":"2024-01-16"}
{"index":{"_id":"3"}}
{"order_id":"ORD-003","customer":"Charlie","total":199,"date":"2024-01-17"}

# Index 3: Utilisateurs
PUT /users
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0
  }
}

POST /users/_bulk
{"index":{"_id":"1"}}
{"username":"alice","email":"alice@example.com","role":"admin"}
{"index":{"_id":"2"}}
{"username":"bob","email":"bob@example.com","role":"user"}
{"index":{"_id":"3"}}
{"username":"charlie","email":"charlie@example.com","role":"user"}
```

**V√©rifier les indices cr√©√©s** :

```bash
GET /_cat/indices?v&h=index,docs.count,store.size
```

**R√©sultat attendu** :
```
index    docs.count store.size
products 5          4.2kb
orders   3          3.1kb
users    3          2.8kb
```

### √âtape 4: Cr√©er un Snapshot Complet

Cr√©ons un premier snapshot incluant tous les indices :

```bash
PUT /_snapshot/my_backup/snapshot_full_2024_01_15
{
  "indices": "*",
  "ignore_unavailable": true,
  "include_global_state": true,
  "metadata": {
    "taken_by": "ops-team",
    "taken_because": "lab-exercise-full-backup",
    "environment": "development"
  }
}
```

**R√©sultat attendu** :
```json
{
  "accepted": true
}
```

**Surveiller la progression du snapshot** :

```bash
GET /_snapshot/my_backup/snapshot_full_2024_01_15/_status
```

**R√©sultat pendant la cr√©ation** :
```json
{
  "snapshots": [{
    "snapshot": "snapshot_full_2024_01_15",
    "repository": "my_backup",
    "state": "IN_PROGRESS",
    "shards_stats": {
      "initializing": 0,
      "started": 2,
      "finalizing": 0,
      "done": 1,
      "failed": 0,
      "total": 3
    }
  }]
}
```

**Attendre que l'√©tat devienne SUCCESS** :

```bash
GET /_snapshot/my_backup/snapshot_full_2024_01_15
```

**R√©sultat attendu** :
```json
{
  "snapshots": [{
    "snapshot": "snapshot_full_2024_01_15",
    "uuid": "abc-123-xyz",
    "state": "SUCCESS",
    "indices": ["products", "orders", "users"],
    "include_global_state": true,
    "shards": {
      "total": 3,
      "failed": 0,
      "successful": 3
    },
    "start_time": "2024-01-15T10:00:00.000Z",
    "end_time": "2024-01-15T10:00:05.432Z",
    "duration_in_millis": 5432
  }]
}
```

### √âtape 5: Cr√©er un Snapshot Partiel

Cr√©ons un snapshot incluant uniquement les indices "products" et "orders" :

```bash
PUT /_snapshot/my_backup/snapshot_products_orders
{
  "indices": "products,orders",
  "ignore_unavailable": false,
  "include_global_state": false,
  "partial": false,
  "metadata": {
    "taken_by": "ops-team",
    "taken_because": "lab-exercise-partial-backup"
  }
}
```

**V√©rifier que le snapshot est termin√©** :

```bash
GET /_snapshot/my_backup/snapshot_products_orders
```

**R√©sultat attendu** :
```json
{
  "snapshots": [{
    "snapshot": "snapshot_products_orders",
    "state": "SUCCESS",
    "indices": ["products", "orders"],
    "include_global_state": false,
    "shards": {
      "total": 2,
      "successful": 2
    }
  }]
}
```

### √âtape 6: Lister Tous les Snapshots

```bash
GET /_snapshot/my_backup/_all
```

**R√©sultat attendu** :
```json
{
  "snapshots": [
    {
      "snapshot": "snapshot_full_2024_01_15",
      "state": "SUCCESS",
      "indices": ["products", "orders", "users"]
    },
    {
      "snapshot": "snapshot_products_orders",
      "state": "SUCCESS",
      "indices": ["products", "orders"]
    }
  ]
}
```

### √âtape 7: Sc√©nario de Restauration 1 - Suppression Accidentelle

Simulons une suppression accidentelle et restaurons depuis le snapshot :

1. **Supprimer accidentellement l'index "orders"** :

```bash
DELETE /orders
```

2. **V√©rifier que l'index n'existe plus** :

```bash
GET /_cat/indices?v&h=index
```

3. **Restaurer uniquement l'index "orders" depuis le snapshot** :

```bash
POST /_snapshot/my_backup/snapshot_full_2024_01_15/_restore
{
  "indices": "orders",
  "ignore_unavailable": true,
  "include_global_state": false
}
```

**R√©sultat attendu** :
```json
{
  "accepted": true
}
```

4. **Surveiller la restauration** :

```bash
GET /_cat/recovery?v&h=index,stage,type,files_percent&s=index
```

**R√©sultat pendant la restauration** :
```
index  stage     type     files_percent
orders translog  snapshot 100.0%
```

5. **V√©rifier que les donn√©es sont restaur√©es** :

```bash
GET /orders/_search
{
  "query": {
    "match_all": {}
  }
}
```

**R√©sultat attendu** :
```json
{
  "hits": {
    "total": { "value": 3 },
    "hits": [
      {
        "_source": {
          "order_id": "ORD-001",
          "customer": "Alice",
          "total": 999
        }
      },
      ...
    ]
  }
}
```

### √âtape 8: Sc√©nario de Restauration 2 - Restauration avec Renommage

Restaurons un index sous un nouveau nom pour comparer des versions ou tester :

```bash
POST /_snapshot/my_backup/snapshot_full_2024_01_15/_restore
{
  "indices": "products",
  "rename_pattern": "(.+)",
  "rename_replacement": "restored_$1",
  "include_aliases": false,
  "index_settings": {
    "index.number_of_replicas": 0
  }
}
```

**V√©rifier que le nouvel index existe** :

```bash
GET /_cat/indices?v&h=index,docs.count
```

**R√©sultat attendu** :
```
index             docs.count
products          5
orders            3
users             3
restored_products 5
```

**Comparer les donn√©es** :

```bash
# Original
GET /products/_count

# Restaur√©
GET /restored_products/_count
```

Les deux devraient retourner `{"count": 5}`.

### √âtape 9: Sc√©nario de Restauration 3 - Restauration Compl√®te

Simulons une corruption compl√®te du cluster et restaurons tout :

1. **Supprimer tous les indices (ATTENTION : uniquement en environnement de test !)** :

```bash
DELETE /products,orders,users,restored_products
```

2. **V√©rifier qu'il n'y a plus d'indices** :

```bash
GET /_cat/indices
```

3. **Restaurer tous les indices depuis le snapshot** :

```bash
POST /_snapshot/my_backup/snapshot_full_2024_01_15/_restore
{
  "indices": "*",
  "include_global_state": true,
  "ignore_unavailable": true
}
```

4. **V√©rifier la restauration compl√®te** :

```bash
GET /_cat/indices?v&h=index,docs.count,store.size
```

**R√©sultat attendu** :
```
index    docs.count store.size
products 5          4.2kb
orders   3          3.1kb
users    3          2.8kb
```

### Validation Finale

V√©rifiez que vous avez r√©ussi le lab :

```bash
# 1. Lister les repositories
GET /_snapshot/_all

# 2. Lister tous les snapshots
GET /_snapshot/my_backup/_all

# 3. V√©rifier que tous les indices sont pr√©sents
GET /_cat/indices?v

# 4. Compter les documents
GET /products/_count
GET /orders/_count
GET /users/_count
```

**R√©sultats attendus** :
- Repository `my_backup` existe et est accessible
- Au moins 2 snapshots pr√©sents et en √©tat `SUCCESS`
- 3 indices pr√©sents : `products`, `orders`, `users`
- Counts : products=5, orders=3, users=3

### Points Cl√©s √† Retenir

‚úÖ Le chemin du repository doit √™tre d√©clar√© dans `path.repo` dans `elasticsearch.yml`  
‚úÖ Les snapshots sont **incr√©mentaux** : seuls les nouveaux segments sont copi√©s  
‚úÖ Utilisez `include_global_state: true` pour sauvegarder templates et policies  
‚úÖ La restauration n√©cessite que les indices n'existent pas (ou soient ferm√©s)  
‚úÖ `rename_pattern` et `rename_replacement` permettent de restaurer avec un nouveau nom  
‚úÖ Les snapshots sont **repository-scoped** : supprimer le repository supprime tous ses snapshots  
‚úÖ Utilisez `_verify` pour tester la connectivit√© du repository  
‚úÖ Les m√©tadonn√©es personnalis√©es (`metadata`) aident √† documenter les snapshots

---

## Lab 6.2: Proc√©dure de Rolling Restart

**Objectif**: Ma√Ætriser la proc√©dure de red√©marrage de n≈ìuds Elasticsearch sans interruption de service, en respectant les bonnes pratiques pour √©viter les d√©placements inutiles de shards.

**Contexte**: Le rolling restart est une op√©ration de maintenance courante (changements de configuration, updates syst√®me, maintenance mat√©rielle). Une mauvaise ex√©cution peut entra√Æner des d√©placements massifs de shards, impactant les performances et la disponibilit√©. Dans ce lab, vous allez pratiquer la proc√©dure standard de rolling restart.

### Pr√©requis : Cluster Multi-N≈ìuds

Ce lab n√©cessite un cluster avec **au moins 2 n≈ìuds** pour d√©montrer le rolling restart sans perte de service.

**Option A : Cluster Docker avec 3 n≈ìuds**

Cr√©ez un fichier `docker-compose.yml` :

```yaml
version: '3.8'
services:
  es01:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.0
    container_name: es01
    environment:
      - node.name=es01
      - cluster.name=es-cluster-lab
      - discovery.seed_hosts=es02,es03
      - cluster.initial_master_nodes=es01,es02,es03
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - xpack.security.enabled=false
    ulimits:
      memlock:
        soft: -1
        hard: -1
    ports:
      - "9200:9200"
    networks:
      - elastic

  es02:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.0
    container_name: es02
    environment:
      - node.name=es02
      - cluster.name=es-cluster-lab
      - discovery.seed_hosts=es01,es03
      - cluster.initial_master_nodes=es01,es02,es03
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - xpack.security.enabled=false
    ulimits:
      memlock:
        soft: -1
        hard: -1
    networks:
      - elastic

  es03:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.0
    container_name: es03
    environment:
      - node.name=es03
      - cluster.name=es-cluster-lab
      - discovery.seed_hosts=es01,es02
      - cluster.initial_master_nodes=es01,es02,es03
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - xpack.security.enabled=false
    ulimits:
      memlock:
        soft: -1
        hard: -1
    networks:
      - elastic

networks:
  elastic:
    driver: bridge
```

D√©marrer le cluster :

```bash
docker-compose up -d
```

**Option B : Cluster existant** avec au moins 2 n≈ìuds

### √âtape 1: V√©rifier l'√âtat Initial du Cluster

```bash
# V√©rifier que tous les n≈ìuds sont pr√©sents
GET /_cat/nodes?v&h=name,ip,heap.percent,ram.percent,cpu,load_1m,node.role,master

# V√©rifier la sant√© du cluster
GET /_cluster/health
```

**R√©sultat attendu** :
```json
{
  "cluster_name": "es-cluster-lab",
  "status": "green",
  "timed_out": false,
  "number_of_nodes": 3,
  "number_of_data_nodes": 3,
  "active_primary_shards": 0,
  "active_shards": 0,
  "relocating_shards": 0,
  "initializing_shards": 0,
  "unassigned_shards": 0
}
```

### √âtape 2: Cr√©er des Indices de Test avec R√©pliques

Pour d√©montrer le rolling restart, cr√©ons des indices avec r√©pliques :

```bash
PUT /test-restart-1
{
  "settings": {
    "number_of_shards": 2,
    "number_of_replicas": 1
  }
}

PUT /test-restart-2
{
  "settings": {
    "number_of_shards": 3,
    "number_of_replicas": 2
  }
}

# Indexer des donn√©es
POST /test-restart-1/_bulk
{"index":{"_id":"1"}}
{"message":"Document 1","timestamp":"2024-01-15T10:00:00Z"}
{"index":{"_id":"2"}}
{"message":"Document 2","timestamp":"2024-01-15T10:01:00Z"}
{"index":{"_id":"3"}}
{"message":"Document 3","timestamp":"2024-01-15T10:02:00Z"}

POST /test-restart-2/_bulk
{"index":{"_id":"1"}}
{"data":"Test data 1","value":100}
{"index":{"_id":"2"}}
{"data":"Test data 2","value":200}
{"index":{"_id":"3"}}
{"data":"Test data 3","value":300}
```

**V√©rifier la r√©partition des shards** :

```bash
GET /_cat/shards/test-restart-*?v&h=index,shard,prirep,state,node&s=index,shard
```

**R√©sultat attendu** (exemple) :
```
index           shard prirep state   node
test-restart-1  0     p      STARTED es01
test-restart-1  0     r      STARTED es02
test-restart-1  1     p      STARTED es02
test-restart-1  1     r      STARTED es03
test-restart-2  0     p      STARTED es01
test-restart-2  0     r      STARTED es02
test-restart-2  0     r      STARTED es03
...
```

Vous devriez voir des shards primaires (p) et r√©pliques (r) r√©partis sur diff√©rents n≈ìuds.

### √âtape 3: D√©sactiver l'Allocation des Shards

**C'est l'√©tape CRITIQUE pour √©viter les d√©placements de shards inutiles** :

```bash
PUT /_cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.enable": "primaries"
  }
}
```

**R√©sultat attendu** :
```json
{
  "acknowledged": true,
  "persistent": {
    "cluster": {
      "routing": {
        "allocation": {
          "enable": "primaries"
        }
      }
    }
  },
  "transient": {}
}
```

**V√©rifier le setting** :

```bash
GET /_cluster/settings?include_defaults=false
```

### √âtape 4: Effectuer un Synced Flush (Optionnel mais Recommand√©)

Ceci acc√©l√®re la r√©cup√©ration des shards apr√®s le red√©marrage :

```bash
POST /_flush/synced
```

**Note** : Cette API est d√©pr√©ci√©e en Elasticsearch 8.x mais peut encore √™tre utilis√©e. En 8.x+, un flush standard est suffisant :

```bash
POST /_flush
```

### √âtape 5: Arr√™ter le Premier N≈ìud (N≈ìud Data)

**Docker** :

```bash
docker stop es03
```

**systemd** :

```bash
sudo systemctl stop elasticsearch
```

**V√©rifier que le n≈ìud est bien arr√™t√©** :

```bash
GET /_cat/nodes?v
```

Vous ne devriez plus voir `es03` dans la liste.

**V√©rifier l'√©tat du cluster** :

```bash
GET /_cluster/health
```

**R√©sultat attendu** :
```json
{
  "cluster_name": "es-cluster-lab",
  "status": "yellow",
  "number_of_nodes": 2,
  "unassigned_shards": 5
}
```

Le statut est **YELLOW** car certaines r√©pliques sont temporairement non assign√©es (le n≈ìud es03 est arr√™t√©), mais les primaires sont toujours actifs.

**V√©rifier les shards** :

```bash
GET /_cat/shards/test-restart-*?v&h=index,shard,prirep,state,unassigned.reason,node&s=state
```

Vous devriez voir des shards `UNASSIGNED` avec `unassigned.reason=NODE_LEFT`.

### √âtape 6: Red√©marrer le N≈ìud

**Docker** :

```bash
docker start es03
```

**systemd** :

```bash
sudo systemctl start elasticsearch
```

**Surveiller le retour du n≈ìud dans le cluster** :

```bash
# R√©p√©ter cette commande toutes les 5-10 secondes
GET /_cat/nodes?v&h=name,uptime,heap.percent,node.role
```

Quand `es03` r√©appara√Æt avec un `uptime` faible (quelques secondes), le n≈ìud a rejoint le cluster.

**V√©rifier la sant√© du cluster** :

```bash
GET /_cluster/health?wait_for_status=yellow&timeout=2m
```

Ceci bloque jusqu'√† ce que le cluster atteigne au moins le statut YELLOW (toutes les primaires actives).

### √âtape 7: V√©rifier la R√©cup√©ration des Shards

```bash
GET /_cat/recovery?v&h=index,shard,stage,type,source_node,target_node,files_percent&s=index
```

**R√©sultat pendant la r√©cup√©ration** :
```
index           shard stage     type   source_node target_node files_percent
test-restart-1  0     translog  peer   es01        es03        100.0%
test-restart-2  0     done      peer   es01        es03        100.0%
```

Les shards sont en cours de r√©cup√©ration (`stage: translog` ou `done`).

### √âtape 8: Attendre que le Cluster Revienne √† GREEN

```bash
GET /_cluster/health?wait_for_status=green&timeout=5m
```

**R√©sultat attendu** :
```json
{
  "status": "green",
  "number_of_nodes": 3,
  "active_shards": 15,
  "relocating_shards": 0,
  "unassigned_shards": 0
}
```

### √âtape 9: R√©p√©ter pour les Autres N≈ìuds

R√©p√©tez les √©tapes 5-8 pour chaque n≈ìud restant :

1. Arr√™ter `es02`
2. Attendre que le cluster soit YELLOW
3. Red√©marrer `es02`
4. Attendre que le cluster soit GREEN
5. Arr√™ter `es01`
6. Attendre que le cluster soit YELLOW
7. Red√©marrer `es01`
8. Attendre que le cluster soit GREEN

**Conseil** : Pour les n≈ìuds master, arr√™tez-les en dernier pour minimiser les √©lections master pendant le restart.

### √âtape 10: R√©activer l'Allocation Compl√®te des Shards

Une fois tous les n≈ìuds red√©marr√©s :

```bash
PUT /_cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.enable": "all"
  }
}
```

**R√©sultat attendu** :
```json
{
  "acknowledged": true,
  "persistent": {
    "cluster": {
      "routing": {
        "allocation": {
          "enable": "all"
        }
      }
    }
  }
}
```

### √âtape 11: V√©rification Finale

```bash
# 1. V√©rifier que tous les n≈ìuds sont pr√©sents
GET /_cat/nodes?v&h=name,uptime,heap.percent,node.role,master

# 2. V√©rifier la sant√© du cluster
GET /_cluster/health

# 3. V√©rifier que les donn√©es sont toujours pr√©sentes
GET /test-restart-1/_count
GET /test-restart-2/_count

# 4. V√©rifier la r√©partition des shards
GET /_cat/shards/test-restart-*?v&h=index,shard,prirep,state,node
```

**R√©sultats attendus** :
- 3 n≈ìuds pr√©sents
- Cluster status `green`
- Count test-restart-1 : 3 documents
- Count test-restart-2 : 3 documents
- Tous les shards en √©tat `STARTED`

### Validation

V√©rifiez que vous avez r√©ussi le lab :

```bash
# V√©rifier les settings de cluster
GET /_cluster/settings?include_defaults=false
```

Le setting `cluster.routing.allocation.enable` doit √™tre `"all"`.

```bash
# V√©rifier qu'aucun shard n'est en cours de relocation
GET /_cat/shards?v&h=index,shard,state,relocating_node&s=state
```

Aucun shard ne devrait avoir de valeur dans `relocating_node`.

### Points Cl√©s √† Retenir

‚úÖ **Toujours d√©sactiver l'allocation** avant un rolling restart avec `"enable": "primaries"`  
‚úÖ **Red√©marrer un n≈ìud √† la fois** et attendre que le cluster revienne √† GREEN  
‚úÖ **Utiliser SIGTERM** pour un shutdown gracieux (jamais SIGKILL)  
‚úÖ **Surveiller la r√©cup√©ration** avec `GET /_cat/recovery`  
‚úÖ **R√©activer l'allocation** apr√®s avoir red√©marr√© tous les n≈ìuds  
‚úÖ **Ordre recommand√©** : data nodes ‚Üí coordinating nodes ‚Üí master nodes  
‚úÖ **wait_for_status avec timeout** permet d'attendre que le cluster soit stable  
‚úÖ Le cluster passe temporairement en **YELLOW** pendant le restart (normal)

---

## üåü Bonus Challenge 6.A: Configuration de Snapshot Lifecycle Management (SLM)

**Niveau**: Avanc√©  
**Objectif**: Automatiser la cr√©ation et le nettoyage de snapshots avec des politiques SLM, incluant la r√©tention automatique et la planification flexible.

**Contexte**: Cr√©er manuellement des snapshots quotidiens est fastidieux et sujet aux oublis. Snapshot Lifecycle Management (SLM) permet d'automatiser ce processus avec des politiques d√©claratives incluant la planification, la r√©tention, et les alertes en cas d'√©chec.

### Sc√©nario

Vous g√©rez un cluster Elasticsearch avec plusieurs types d'indices :
- **Indices transactionnels** (`orders-*`, `payments-*`) : Critiques, n√©cessitent des sauvegardes fr√©quentes
- **Indices analytiques** (`analytics-*`) : Moins critiques, sauvegardes hebdomadaires suffisantes
- **Indices de logs** (`logs-*`) : Volumineux, sauvegardes quotidiennes avec courte r√©tention

Vous allez cr√©er **3 politiques SLM** avec diff√©rentes strat√©gies de r√©tention et planification.

### √âtape 1: Pr√©requis - Repository Configur√©

V√©rifiez que vous avez un repository configur√© (depuis Lab 6.1) :

```bash
GET /_snapshot/my_backup
```

Si le repository n'existe pas, cr√©ez-le :

```bash
PUT /_snapshot/my_backup
{
  "type": "fs",
  "settings": {
    "location": "/usr/share/elasticsearch/backups",
    "compress": true
  }
}
```

### √âtape 2: Cr√©er des Indices de Test

Cr√©ons des indices repr√©sentant diff√©rents cas d'usage :

```bash
# Indices transactionnels (critiques)
PUT /orders-2024-01
PUT /orders-2024-02
PUT /payments-2024-01

# Indices analytiques
PUT /analytics-2024-q1
PUT /analytics-2024-q2

# Indices de logs (volumineux)
PUT /logs-2024-01-15
PUT /logs-2024-01-16
PUT /logs-2024-01-17

# Indexer quelques donn√©es
POST /orders-2024-01/_bulk
{"index":{"_id":"1"}}
{"order_id":"ORD-001","amount":100}
{"index":{"_id":"2"}}
{"order_id":"ORD-002","amount":200}

POST /analytics-2024-q1/_doc
{"metric":"revenue","value":50000,"period":"Q1"}

POST /logs-2024-01-15/_bulk
{"index":{}}
{"timestamp":"2024-01-15T10:00:00Z","level":"INFO","message":"Application started"}
{"index":{}}
{"timestamp":"2024-01-15T10:05:00Z","level":"WARN","message":"High memory usage"}
```

### √âtape 3: Politique SLM pour Indices Transactionnels (Critiques)

**Exigences** :
- Snapshots **quotidiens** √† 2h du matin
- Inclure uniquement `orders-*` et `payments-*`
- R√©tention : **90 jours**
- Garder au minimum **30 snapshots**
- Limite maximale : **120 snapshots**

```bash
PUT /_slm/policy/daily-critical-backup
{
  "schedule": "0 0 2 * * ?",
  "name": "<critical-{now/d}>",
  "repository": "my_backup",
  "config": {
    "indices": ["orders-*", "payments-*"],
    "ignore_unavailable": false,
    "include_global_state": false,
    "partial": false,
    "metadata": {
      "policy": "daily-critical-backup",
      "criticality": "high",
      "team": "finance"
    }
  },
  "retention": {
    "expire_after": "90d",
    "min_count": 30,
    "max_count": 120
  }
}
```

**Explication des param√®tres** :
- `schedule: "0 0 2 * * ?"` : Expression cron pour 2h00 tous les jours
  - Format : `<second> <minute> <hour> <day_of_month> <month> <day_of_week>`
  - `?` signifie "n'importe quel" pour day_of_month ou day_of_week
- `name: "<critical-{now/d}>"` : Template g√©n√©rant `critical-2024-01-15`
- `expire_after: "90d"` : Supprimer les snapshots de plus de 90 jours
- `min_count: 30` : Toujours garder au moins 30 snapshots, m√™me si expir√©s
- `max_count: 120` : Ne jamais d√©passer 120 snapshots (suppression du plus ancien)

### √âtape 4: Politique SLM pour Indices Analytiques (Hebdomadaire)

**Exigences** :
- Snapshots **hebdomadaires** le dimanche √† 3h du matin
- Inclure uniquement `analytics-*`
- R√©tention : **180 jours** (6 mois)
- Garder au minimum **10 snapshots**
- Limite maximale : **52 snapshots** (1 an de semaines)

```bash
PUT /_slm/policy/weekly-analytics-backup
{
  "schedule": "0 0 3 ? * SUN",
  "name": "<analytics-{now/w}>",
  "repository": "my_backup",
  "config": {
    "indices": ["analytics-*"],
    "ignore_unavailable": true,
    "include_global_state": false,
    "partial": false,
    "metadata": {
      "policy": "weekly-analytics-backup",
      "criticality": "medium",
      "team": "data-science"
    }
  },
  "retention": {
    "expire_after": "180d",
    "min_count": 10,
    "max_count": 52
  }
}
```

**Explication** :
- `schedule: "0 0 3 ? * SUN"` : Tous les dimanches √† 3h00
- `name: "<analytics-{now/w}>"` : Template g√©n√©rant `analytics-2024-w03` (semaine 3)

### √âtape 5: Politique SLM pour Indices de Logs (Quotidien, Courte R√©tention)

**Exigences** :
- Snapshots **quotidiens** √† 1h du matin
- Inclure uniquement `logs-*`
- R√©tention : **14 jours** seulement (logs ont courte valeur)
- Garder au minimum **7 snapshots** (1 semaine)
- Limite maximale : **30 snapshots**

```bash
PUT /_slm/policy/daily-logs-backup
{
  "schedule": "0 0 1 * * ?",
  "name": "<logs-{now/d}>",
  "repository": "my_backup",
  "config": {
    "indices": ["logs-*"],
    "ignore_unavailable": true,
    "include_global_state": false,
    "partial": true,
    "metadata": {
      "policy": "daily-logs-backup",
      "criticality": "low",
      "team": "ops"
    }
  },
  "retention": {
    "expire_after": "14d",
    "min_count": 7,
    "max_count": 30
  }
}
```

**Explication** :
- `partial: true` : Permet au snapshot de r√©ussir m√™me si certains shards primaires √©chouent
- `ignore_unavailable: true` : Ignore les indices logs-* qui n'existent pas encore

### √âtape 6: Lister Toutes les Politiques SLM

```bash
GET /_slm/policy
```

**R√©sultat attendu** :
```json
{
  "daily-critical-backup": {
    "version": 1,
    "modified_date": "2024-01-15T10:00:00.000Z",
    "policy": {
      "schedule": "0 0 2 * * ?",
      "name": "<critical-{now/d}>",
      ...
    },
    "next_execution_millis": 1705287600000,
    "stats": {
      "policy": "daily-critical-backup",
      "snapshots_taken": 0,
      "snapshots_failed": 0,
      "snapshots_deleted": 0,
      "snapshot_deletion_failures": 0
    }
  },
  "weekly-analytics-backup": { ... },
  "daily-logs-backup": { ... }
}
```

### √âtape 7: Ex√©cuter Manuellement les Politiques (Pour Tests)

Plut√¥t que d'attendre la planification, ex√©cutons les politiques manuellement :

```bash
# Ex√©cuter la politique critical
POST /_slm/policy/daily-critical-backup/_execute

# Ex√©cuter la politique analytics
POST /_slm/policy/weekly-analytics-backup/_execute

# Ex√©cuter la politique logs
POST /_slm/policy/daily-logs-backup/_execute
```

**R√©sultat attendu pour chaque** :
```json
{
  "snapshot_name": "critical-2024-01-15"
}
```

### √âtape 8: V√©rifier les Snapshots Cr√©√©s

```bash
GET /_snapshot/my_backup/_all
```

**R√©sultat attendu** :
```json
{
  "snapshots": [
    {
      "snapshot": "critical-2024-01-15",
      "state": "SUCCESS",
      "indices": ["orders-2024-01", "orders-2024-02", "payments-2024-01"]
    },
    {
      "snapshot": "analytics-2024-w03",
      "state": "SUCCESS",
      "indices": ["analytics-2024-q1", "analytics-2024-q2"]
    },
    {
      "snapshot": "logs-2024-01-15",
      "state": "SUCCESS",
      "indices": ["logs-2024-01-15", "logs-2024-01-16", "logs-2024-01-17"]
    }
  ]
}
```

### √âtape 9: Consulter les Statistiques des Politiques

```bash
GET /_slm/policy/daily-critical-backup
```

**R√©sultat attendu** :
```json
{
  "daily-critical-backup": {
    "policy": { ... },
    "version": 1,
    "modified_date_millis": 1705305600000,
    "last_success": {
      "snapshot_name": "critical-2024-01-15",
      "time_string": "2024-01-15T10:05:00.000Z",
      "time": 1705305900000
    },
    "last_failure": null,
    "next_execution": "2024-01-16T02:00:00.000Z",
    "next_execution_millis": 1705374000000,
    "stats": {
      "policy": "daily-critical-backup",
      "snapshots_taken": 1,
      "snapshots_failed": 0,
      "snapshots_deleted": 0,
      "snapshot_deletion_failures": 0
    }
  }
}
```

### √âtape 10: Tester la R√©tention Automatique

Pour tester la r√©tention, simulons des snapshots anciens :

1. **Cr√©er manuellement des snapshots avec dates anciennes** (simulant des anciens backups) :

```bash
# Snapshot de 100 jours (devrait √™tre supprim√© par daily-critical-backup)
PUT /_snapshot/my_backup/critical-2023-10-07
{
  "indices": "orders-2024-01",
  "metadata": {
    "simulated_old_snapshot": true
  }
}

# Snapshot de 20 jours (devrait √™tre conserv√©)
PUT /_snapshot/my_backup/critical-2023-12-26
{
  "indices": "orders-2024-01"
}
```

2. **Forcer l'ex√©cution de la r√©tention** :

```bash
POST /_slm/_execute_retention
```

**R√©sultat attendu** :
```json
{
  "acknowledged": true
}
```

3. **V√©rifier que le snapshot ancien a √©t√© supprim√©** :

```bash
GET /_snapshot/my_backup/_all
```

Le snapshot `critical-2023-10-07` (100 jours) devrait avoir √©t√© supprim√© automatiquement.

### √âtape 11: Surveillance et Alertes SLM

**V√©rifier l'historique global de SLM** :

```bash
GET /_slm/stats
```

**R√©sultat attendu** :
```json
{
  "retention_runs": 1,
  "retention_failed": 0,
  "retention_timed_out": 0,
  "retention_deletion_time": "15ms",
  "retention_deletion_time_millis": 15,
  "total_snapshots_taken": 3,
  "total_snapshots_failed": 0,
  "total_snapshots_deleted": 1,
  "total_snapshot_deletion_failures": 0,
  "policy_stats": [
    {
      "policy": "daily-critical-backup",
      "snapshots_taken": 1,
      "snapshots_failed": 0
    },
    ...
  ]
}
```

**Consulter le statut d'une politique sp√©cifique** :

```bash
GET /_slm/policy/daily-critical-backup/_status
```

### √âtape 12: Modifier une Politique SLM

Imaginons que nous voulons changer la fr√©quence de `daily-logs-backup` √† toutes les 6 heures :

```bash
PUT /_slm/policy/daily-logs-backup
{
  "schedule": "0 0 */6 * * ?",
  "name": "<logs-{now/d}>",
  "repository": "my_backup",
  "config": {
    "indices": ["logs-*"],
    "ignore_unavailable": true,
    "include_global_state": false
  },
  "retention": {
    "expire_after": "14d",
    "min_count": 7,
    "max_count": 30
  }
}
```

**V√©rifier la modification** :

```bash
GET /_slm/policy/daily-logs-backup
```

Le `version` devrait avoir incr√©ment√©, et le `schedule` √™tre `"0 0 */6 * * ?"`.

### √âtape 13: D√©sactiver/Activer une Politique SLM

**D√©sactiver temporairement une politique** (ex: maintenance) :

```bash
POST /_slm/stop
```

Ceci arr√™te **toutes** les politiques SLM.

**V√©rifier le statut** :

```bash
GET /_slm/status
```

**R√©sultat attendu** :
```json
{
  "operation_mode": "STOPPED"
}
```

**R√©activer SLM** :

```bash
POST /_slm/start
```

**V√©rifier** :

```bash
GET /_slm/status
```

**R√©sultat attendu** :
```json
{
  "operation_mode": "RUNNING"
}
```

### √âtape 14: Supprimer une Politique SLM

```bash
DELETE /_slm/policy/weekly-analytics-backup
```

**R√©sultat attendu** :
```json
{
  "acknowledged": true
}
```

**Note** : Supprimer une politique SLM **ne supprime pas** les snapshots d√©j√† cr√©√©s. Ils restent dans le repository.

### Validation Finale

V√©rifiez que vous avez r√©ussi le bonus challenge :

```bash
# 1. Lister toutes les politiques SLM actives
GET /_slm/policy

# 2. V√©rifier les statistiques globales
GET /_slm/stats

# 3. V√©rifier les snapshots cr√©√©s
GET /_snapshot/my_backup/_all

# 4. V√©rifier le statut SLM
GET /_slm/status
```

**R√©sultats attendus** :
- Au moins 2 politiques SLM configur√©es et actives
- `operation_mode: "RUNNING"`
- Au moins 2 snapshots cr√©√©s par les politiques
- Statistiques montrant `total_snapshots_taken > 0` et `total_snapshots_failed = 0`

### D√©fis Suppl√©mentaires

**D√©fi 1** : Cr√©er une politique SLM mensuelle pour les archives

```bash
PUT /_slm/policy/monthly-archive
{
  "schedule": "0 0 4 1 * ?",
  "name": "<archive-{now/M}>",
  "repository": "my_backup",
  "config": {
    "indices": "*",
    "include_global_state": true
  },
  "retention": {
    "expire_after": "365d",
    "min_count": 12,
    "max_count": 24
  }
}
```

**D√©fi 2** : Cr√©er une alerte Kibana pour surveiller les √©checs SLM

1. Aller dans **Stack Management** ‚Üí **Rules**
2. Cr√©er une r√®gle **Elasticsearch query**
3. Query pour d√©tecter les √©checs :

```json
{
  "query": {
    "bool": {
      "must": [
        { "term": { "event.action": "snapshot-failed" } },
        { "range": { "@timestamp": { "gte": "now-1h" } } }
      ]
    }
  }
}
```

### Points Cl√©s √† Retenir

‚úÖ **SLM automatise** la cr√©ation et le nettoyage de snapshots selon des politiques d√©claratives  
‚úÖ Les **expressions cron** d√©finissent la planification (quotidien, hebdomadaire, mensuel)  
‚úÖ La **r√©tention** combine `expire_after`, `min_count`, et `max_count` pour contr√¥ler le nettoyage  
‚úÖ Les **templates de noms** (`{now/d}`, `{now/w}`) g√©n√®rent des noms uniques avec dates  
‚úÖ `POST /_slm/_execute_retention` force l'ex√©cution de la r√©tention  
‚úÖ `POST /_slm/policy/<name>/_execute` ex√©cute manuellement une politique (utile pour tests)  
‚úÖ `POST /_slm/stop` et `POST /_slm/start` contr√¥lent globalement toutes les politiques  
‚úÖ Supprimer une politique **ne supprime pas** les snapshots d√©j√† cr√©√©s  
‚úÖ Les **m√©tadonn√©es personnalis√©es** aident √† documenter et organiser les snapshots  
‚úÖ Utilisez diff√©rentes politiques SLM pour diff√©rents types de donn√©es (criticit√©, fr√©quence)

**F√©licitations !** Vous ma√Ætrisez maintenant les op√©rations de maintenance avanc√©es d'Elasticsearch ! üéâ


---


## Lab 8.1: Configuration de Dedicated Master Nodes

**Objectif**: Mettre en place une architecture de cluster avec s√©paration des r√¥les master et data pour am√©liorer la stabilit√© et les performances en production.

**Contexte**: Dans un cluster de production, il est recommand√© de s√©parer les n≈ìuds master (gestion du cluster) des n≈ìuds data (stockage et recherche) pour √©viter que les t√¢ches de gestion impactent les performances des requ√™tes. Dans ce lab, vous allez cr√©er un cluster avec 3 dedicated master nodes et 3 data nodes.

### Pr√©requis : Docker et Docker Compose

Ce lab utilise Docker Compose pour simuler un cluster multi-n≈ìuds.

### √âtape 1: Cr√©er la Configuration Docker Compose

Cr√©ez un fichier `docker-compose-prod.yml` :

```yaml
version: '3.8'

services:
  # === DEDICATED MASTER NODES ===
  master-1:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.0
    container_name: es-master-1
    environment:
      - node.name=master-1
      - cluster.name=es-prod-cluster
      - node.roles=master
      - discovery.seed_hosts=master-2,master-3
      - cluster.initial_master_nodes=master-1,master-2,master-3
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - xpack.security.enabled=false
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - master1-data:/usr/share/elasticsearch/data
    networks:
      - elastic-prod

  master-2:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.0
    container_name: es-master-2
    environment:
      - node.name=master-2
      - cluster.name=es-prod-cluster
      - node.roles=master
      - discovery.seed_hosts=master-1,master-3
      - cluster.initial_master_nodes=master-1,master-2,master-3
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - xpack.security.enabled=false
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - master2-data:/usr/share/elasticsearch/data
    networks:
      - elastic-prod

  master-3:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.0
    container_name: es-master-3
    environment:
      - node.name=master-3
      - cluster.name=es-prod-cluster
      - node.roles=master
      - discovery.seed_hosts=master-1,master-2
      - cluster.initial_master_nodes=master-1,master-2,master-3
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - xpack.security.enabled=false
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - master3-data:/usr/share/elasticsearch/data
    networks:
      - elastic-prod

  # === DATA NODES ===
  data-1:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.0
    container_name: es-data-1
    environment:
      - node.name=data-1
      - cluster.name=es-prod-cluster
      - node.roles=data,ingest
      - discovery.seed_hosts=master-1,master-2,master-3
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
      - xpack.security.enabled=false
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - data1-data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
    networks:
      - elastic-prod
    depends_on:
      - master-1
      - master-2
      - master-3

  data-2:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.0
    container_name: es-data-2
    environment:
      - node.name=data-2
      - cluster.name=es-prod-cluster
      - node.roles=data,ingest
      - discovery.seed_hosts=master-1,master-2,master-3
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
      - xpack.security.enabled=false
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - data2-data:/usr/share/elasticsearch/data
    networks:
      - elastic-prod
    depends_on:
      - master-1
      - master-2
      - master-3

  data-3:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.0
    container_name: es-data-3
    environment:
      - node.name=data-3
      - cluster.name=es-prod-cluster
      - node.roles=data,ingest
      - discovery.seed_hosts=master-1,master-2,master-3
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
      - xpack.security.enabled=false
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - data3-data:/usr/share/elasticsearch/data
    networks:
      - elastic-prod
    depends_on:
      - master-1
      - master-2
      - master-3

volumes:
  master1-data:
  master2-data:
  master3-data:
  data1-data:
  data2-data:
  data3-data:

networks:
  elastic-prod:
    driver: bridge
```

### √âtape 2: D√©marrer le Cluster

```bash
docker-compose -f docker-compose-prod.yml up -d
```

**Attendre 30-60 secondes** que tous les n≈ìuds d√©marrent et forment le cluster.

### √âtape 3: V√©rifier la Formation du Cluster

```bash
# V√©rifier que tous les n≈ìuds sont pr√©sents
curl "http://localhost:9200/_cat/nodes?v&h=name,node.role,master,heap.percent,ram.percent"
```

**R√©sultat attendu** :
```
name     node.role master heap.percent ram.percent
master-1 m         *      15           25
master-2 m         -      12           20
master-3 m         -      10           18
data-1   di        -      25           35
data-2   di        -      30           40
data-3   di        -      28           38
```

**Analyse** :
- ‚úÖ 3 n≈ìuds avec r√¥le `m` (master-only)
- ‚úÖ 3 n≈ìuds avec r√¥le `di` (data + ingest)
- ‚úÖ Un seul master √©lu (marqu√© `*`)

### √âtape 4: V√©rifier le Quorum de Masters

```bash
curl "http://localhost:9200/_cluster/health?pretty"
```

**R√©sultat attendu** :
```json
{
  "cluster_name": "es-prod-cluster",
  "status": "green",
  "number_of_nodes": 6,
  "number_of_data_nodes": 3,
  "active_primary_shards": 0,
  "active_shards": 0
}
```

**Points cl√©s** :
- `number_of_nodes: 6` (3 masters + 3 data)
- `number_of_data_nodes: 3` (seulement les data nodes)

### √âtape 5: Tester la R√©silience des Masters

Simulons la panne d'un master node :

```bash
# Arr√™ter master-3
docker stop es-master-3
```

**V√©rifier que le cluster reste op√©rationnel** :

```bash
curl "http://localhost:9200/_cluster/health?pretty"
```

**R√©sultat attendu** :
```json
{
  "status": "green",
  "number_of_nodes": 5,
  "number_of_data_nodes": 3
}
```

**Analyse** : Le cluster reste GREEN car :
- Quorum = 2 (sur 3 masters)
- 2 masters actifs (master-1, master-2) > quorum
- Tous les data nodes op√©rationnels

**Red√©marrer master-3** :

```bash
docker start es-master-3
```

### √âtape 6: Cr√©er des Indices de Test

Cr√©ons des indices avec r√©pliques :

```bash
curl -X PUT "http://localhost:9200/test-prod-1" -H 'Content-Type: application/json' -d'
{
  "settings": {
    "number_of_shards": 3,
    "number_of_replicas": 2
  }
}'

curl -X PUT "http://localhost:9200/test-prod-2" -H 'Content-Type: application/json' -d'
{
  "settings": {
    "number_of_shards": 2,
    "number_of_replicas": 1
  }
}'
```

**Indexer des donn√©es** :

```bash
for i in {1..100}; do
  curl -X POST "http://localhost:9200/test-prod-1/_doc" -H 'Content-Type: application/json' -d"
  {
    \"id\": $i,
    \"message\": \"Test document $i\",
    \"timestamp\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\"
  }"
done
```

### √âtape 7: V√©rifier la R√©partition des Shards

```bash
curl "http://localhost:9200/_cat/shards/test-prod-*?v&h=index,shard,prirep,state,node&s=index,shard"
```

**R√©sultat attendu** :
```
index        shard prirep state   node
test-prod-1  0     p      STARTED data-1
test-prod-1  0     r      STARTED data-2
test-prod-1  0     r      STARTED data-3
test-prod-1  1     p      STARTED data-2
test-prod-1  1     r      STARTED data-1
test-prod-1  1     r      STARTED data-3
...
```

**Analyse** :
- ‚úÖ Les shards sont **uniquement** sur les data nodes (data-1, data-2, data-3)
- ‚úÖ **Aucun shard** sur les master nodes
- ‚úÖ R√©pliques distribu√©es sur diff√©rents data nodes

### √âtape 8: Tester la Panne d'un Data Node

```bash
# Arr√™ter data-2
docker stop es-data-2
```

**V√©rifier la sant√© du cluster** :

```bash
curl "http://localhost:9200/_cluster/health?pretty"
```

**R√©sultat attendu** :
```json
{
  "status": "yellow",
  "number_of_nodes": 5,
  "number_of_data_nodes": 2,
  "relocating_shards": 0,
  "initializing_shards": 0,
  "unassigned_shards": 3
}
```

**Analyse** :
- Cluster passe en YELLOW (certaines r√©pliques non assign√©es)
- Mais **toujours fonctionnel** (tous les primaires actifs)
- Masters non affect√©s (quorum intact)

**Red√©marrer data-2** :

```bash
docker start es-data-2
```

Le cluster retournera automatiquement √† GREEN apr√®s r√©cup√©ration des shards.

### √âtape 9: Mesurer les Ressources des Masters vs Data Nodes

```bash
curl "http://localhost:9200/_cat/nodes?v&h=name,node.role,heap.current,heap.percent,heap.max,ram.current,ram.percent"
```

**Observation** :
- Masters : Heap faible (< 20%), peu de RAM
- Data nodes : Heap plus √©lev√© (30-50%), plus de RAM

**Pourquoi ?** Les masters g√®rent uniquement les m√©tadonn√©es (√©tat du cluster), les data nodes g√®rent les donn√©es et les requ√™tes.

### Validation Finale

```bash
# 1. V√©rifier l'architecture
curl "http://localhost:9200/_cat/nodes?v"

# 2. V√©rifier le master √©lu
curl "http://localhost:9200/_cat/master?v"

# 3. V√©rifier les settings de d√©couverte
curl "http://localhost:9200/_nodes/master-1/settings?pretty&filter_path=nodes.*.settings.cluster,nodes.*.settings.discovery"

# 4. Compter les shards
curl "http://localhost:9200/_cat/shards?v&h=index,shard,prirep,node" | grep -c data
```

**R√©sultats attendus** :
- 6 n≈ìuds (3 masters, 3 data)
- Un seul master √©lu
- `cluster.initial_master_nodes` configur√© avec 3 masters
- Tous les shards sur data nodes uniquement

### Points Cl√©s √† Retenir

‚úÖ **Dedicated master nodes** s√©parent gestion (masters) et traitement (data)  
‚úÖ **Quorum de 3 masters** tol√®re 1 panne (quorum = 2)  
‚úÖ **node.roles** d√©finit les responsabilit√©s de chaque n≈ìud  
‚úÖ Masters : `node.roles: [master]`, Data : `node.roles: [data, ingest]`  
‚úÖ **cluster.initial_master_nodes** liste les masters pour l'initialisation  
‚úÖ **discovery.seed_hosts** permet aux n≈ìuds de se d√©couvrir  
‚úÖ Panne d'un master **n'affecte pas** les op√©rations data  
‚úÖ Data nodes peuvent √™tre ajout√©s/retir√©s sans toucher aux masters  
‚úÖ Architecture recommand√©e pour **production** (stabilit√© et scalabilit√©)

---

## Lab 8.2: Impl√©mentation de Shard Allocation Awareness

**Objectif**: Configurer la rack awareness (shard allocation awareness) pour distribuer les shards primaires et r√©pliques sur diff√©rentes zones de disponibilit√©, garantissant ainsi la haute disponibilit√© en cas de panne d'une zone enti√®re.

**Contexte**: Par d√©faut, Elasticsearch alloue les shards sans consid√©ration g√©ographique. Si tous vos n≈ìuds sont dans le m√™me datacenter/rack et qu'il tombe en panne, vous perdez toutes vos donn√©es. La rack awareness force Elasticsearch √† distribuer les r√©pliques sur diff√©rentes zones.

### Sc√©nario

Vous g√©rez un cluster avec n≈ìuds r√©partis sur **2 zones de disponibilit√©** (Zone A et Zone B). Vous voulez garantir que chaque shard primaire a au moins une r√©plique dans une zone diff√©rente.

### √âtape 1: Arr√™ter le Cluster Pr√©c√©dent

```bash
docker-compose -f docker-compose-prod.yml down -v
```

### √âtape 2: Cr√©er la Configuration avec Awareness

Cr√©ez `docker-compose-awareness.yml` :

```yaml
version: '3.8'

services:
  # === ZONE A ===
  master-a:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.0
    container_name: es-master-a
    environment:
      - node.name=master-a
      - cluster.name=es-aware-cluster
      - node.roles=master
      - node.attr.zone=zone_a
      - cluster.routing.allocation.awareness.attributes=zone
      - cluster.routing.allocation.awareness.force.zone.values=zone_a,zone_b
      - discovery.seed_hosts=master-b,data-a1,data-b1
      - cluster.initial_master_nodes=master-a,master-b
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - xpack.security.enabled=false
    networks:
      - elastic-aware

  data-a1:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.0
    container_name: es-data-a1
    environment:
      - node.name=data-a1
      - cluster.name=es-aware-cluster
      - node.roles=data,ingest
      - node.attr.zone=zone_a
      - cluster.routing.allocation.awareness.attributes=zone
      - cluster.routing.allocation.awareness.force.zone.values=zone_a,zone_b
      - discovery.seed_hosts=master-a,master-b
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
      - xpack.security.enabled=false
    ports:
      - "9200:9200"
    networks:
      - elastic-aware

  data-a2:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.0
    container_name: es-data-a2
    environment:
      - node.name=data-a2
      - cluster.name=es-aware-cluster
      - node.roles=data,ingest
      - node.attr.zone=zone_a
      - cluster.routing.allocation.awareness.attributes=zone
      - cluster.routing.allocation.awareness.force.zone.values=zone_a,zone_b
      - discovery.seed_hosts=master-a,master-b
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
      - xpack.security.enabled=false
    networks:
      - elastic-aware

  # === ZONE B ===
  master-b:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.0
    container_name: es-master-b
    environment:
      - node.name=master-b
      - cluster.name=es-aware-cluster
      - node.roles=master
      - node.attr.zone=zone_b
      - cluster.routing.allocation.awareness.attributes=zone
      - cluster.routing.allocation.awareness.force.zone.values=zone_a,zone_b
      - discovery.seed_hosts=master-a,data-a1,data-b1
      - cluster.initial_master_nodes=master-a,master-b
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - xpack.security.enabled=false
    networks:
      - elastic-aware

  data-b1:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.0
    container_name: es-data-b1
    environment:
      - node.name=data-b1
      - cluster.name=es-aware-cluster
      - node.roles=data,ingest
      - node.attr.zone=zone_b
      - cluster.routing.allocation.awareness.attributes=zone
      - cluster.routing.allocation.awareness.force.zone.values=zone_a,zone_b
      - discovery.seed_hosts=master-a,master-b
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
      - xpack.security.enabled=false
    networks:
      - elastic-aware

  data-b2:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.12.0
    container_name: es-data-b2
    environment:
      - node.name=data-b2
      - cluster.name=es-aware-cluster
      - node.roles=data,ingest
      - node.attr.zone=zone_b
      - cluster.routing.allocation.awareness.attributes=zone
      - cluster.routing.allocation.awareness.force.zone.values=zone_a,zone_b
      - discovery.seed_hosts=master-a,master-b
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
      - xpack.security.enabled=false
    networks:
      - elastic-aware

networks:
  elastic-aware:
    driver: bridge
```

**Param√®tres cl√©s** :
- `node.attr.zone` : D√©clare la zone du n≈ìud (zone_a ou zone_b)
- `cluster.routing.allocation.awareness.attributes` : Elasticsearch doit √™tre aware de l'attribut "zone"
- `cluster.routing.allocation.awareness.force.zone.values` : Force la distribution sur les 2 zones

### √âtape 3: D√©marrer le Cluster

```bash
docker-compose -f docker-compose-awareness.yml up -d
```

### √âtape 4: V√©rifier les Attributs de Zone

```bash
curl "http://localhost:9200/_cat/nodeattrs?v&h=node,attr,value&s=node"
```

**R√©sultat attendu** :
```
node     attr value
data-a1  zone zone_a
data-a2  zone zone_a
data-b1  zone zone_b
data-b2  zone zone_b
master-a zone zone_a
master-b zone zone_b
```

### √âtape 5: Cr√©er un Index avec R√©pliques

```bash
curl -X PUT "http://localhost:9200/ha-test" -H 'Content-Type: application/json' -d'
{
  "settings": {
    "number_of_shards": 4,
    "number_of_replicas": 1
  }
}'

# Indexer des donn√©es
for i in {1..50}; do
  curl -X POST "http://localhost:9200/ha-test/_doc" -H 'Content-Type: application/json' -d"
  {
    \"id\": $i,
    \"zone_test\": true,
    \"timestamp\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\"
  }"
done
```

### √âtape 6: V√©rifier la Distribution des Shards

```bash
curl "http://localhost:9200/_cat/shards/ha-test?v&h=index,shard,prirep,state,node&s=shard"
```

**R√©sultat attendu (exemple)** :
```
index   shard prirep state   node
ha-test 0     p      STARTED data-a1
ha-test 0     r      STARTED data-b1
ha-test 1     p      STARTED data-b2
ha-test 1     r      STARTED data-a2
ha-test 2     p      STARTED data-a2
ha-test 2     r      STARTED data-b2
ha-test 3     p      STARTED data-b1
ha-test 3     r      STARTED data-a1
```

**Analyse** :
- ‚úÖ Chaque shard primaire (p) a sa r√©plique (r) sur une **zone diff√©rente**
- ‚úÖ Shard 0 : primaire sur zone_a, r√©plique sur zone_b
- ‚úÖ Shard 1 : primaire sur zone_b, r√©plique sur zone_a
- ‚úÖ **JAMAIS** primaire et r√©plique sur la m√™me zone

### √âtape 7: Visualiser la Distribution par Zone

```bash
curl "http://localhost:9200/_cat/shards/ha-test?v&h=index,shard,prirep,node" | awk '
BEGIN {print "Zone A (primaires) | Zone A (r√©pliques) | Zone B (primaires) | Zone B (r√©pliques)"}
/data-a/ && /p/ {za_p++}
/data-a/ && /r/ {za_r++}
/data-b/ && /p/ {zb_p++}
/data-b/ && /r/ {zb_r++}
END {print za_p " | " za_r " | " zb_p " | " zb_r}
'
```

**R√©sultat attendu** : Distribution √©quilibr√©e, par exemple `2 | 2 | 2 | 2`

### √âtape 8: Tester la R√©silience (Panne d'une Zone Compl√®te)

Simulons la panne de **toute la Zone A** :

```bash
docker stop es-master-a es-data-a1 es-data-a2
```

**V√©rifier la sant√© du cluster** :

```bash
curl "http://localhost:9200/_cluster/health?pretty"
```

**R√©sultat attendu** :
```json
{
  "status": "yellow",
  "number_of_nodes": 3,
  "number_of_data_nodes": 2,
  "active_primary_shards": 4,
  "active_shards": 4,
  "relocating_shards": 0,
  "initializing_shards": 0,
  "unassigned_shards": 4,
  "unassigned_reason": "NODE_LEFT"
}
```

**Analyse** :
- ‚úÖ Cluster status YELLOW (pas RED !)
- ‚úÖ **Tous les primaires actifs** (4/4)
- ‚úÖ Donn√©es toujours accessibles (Zone B a tous les primaires ou r√©pliques)
- ‚ùå 4 r√©pliques non assign√©es (√©taient sur Zone A)

**Tester l'acc√®s aux donn√©es** :

```bash
curl "http://localhost:9200/ha-test/_count"
```

**R√©sultat attendu** : `{"count": 50}` - **Toutes les donn√©es accessibles** !

### √âtape 9: Restaurer la Zone A

```bash
docker start es-master-a es-data-a1 es-data-a2
```

Le cluster retournera automatiquement √† GREEN apr√®s r√©allocation des r√©pliques.

### √âtape 10: Tester Sans Awareness (Comparaison)

Pour comparer, cr√©ons un index **sans** forcer la distribution :

```bash
# Temporairement d√©sactiver force awareness
curl -X PUT "http://localhost:9200/_cluster/settings" -H 'Content-Type: application/json' -d'
{
  "transient": {
    "cluster.routing.allocation.awareness.force.zone.values": null
  }
}'

# Cr√©er index sans garantie de distribution
curl -X PUT "http://localhost:9200/no-awareness-test" -H 'Content-Type: application/json' -d'
{
  "settings": {
    "number_of_shards": 2,
    "number_of_replicas": 1
  }
}'
```

**V√©rifier la distribution** :

```bash
curl "http://localhost:9200/_cat/shards/no-awareness-test?v&h=shard,prirep,node"
```

**Possibilit√©** : Primaire et r√©plique sur **la m√™me zone** (risque de perte de donn√©es si la zone tombe).

### Validation Finale

```bash
# 1. V√©rifier les attributs de zone
curl "http://localhost:9200/_cat/nodeattrs?v&h=node,attr,value" | grep zone

# 2. V√©rifier les settings awareness
curl "http://localhost:9200/_cluster/settings?include_defaults=true&filter_path=*.cluster.routing.allocation.awareness.*"

# 3. Statistiques de shards par zone
curl "http://localhost:9200/_cat/shards/ha-test?v&h=node,prirep" | awk '
/data-a/ {zone_a++}
/data-b/ {zone_b++}
END {print "Zone A shards: " zone_a "\nZone B shards: " zone_b}'

# 4. Tester la perte d'une zone compl√®te
docker stop es-data-a1 es-data-a2
curl "http://localhost:9200/ha-test/_count"
# Devrait retourner le count complet m√™me avec Zone A down
```

### Points Cl√©s √† Retenir

‚úÖ **Rack awareness** distribue primaires et r√©pliques sur diff√©rentes zones  
‚úÖ `node.attr.zone` d√©clare la zone de chaque n≈ìud  
‚úÖ `cluster.routing.allocation.awareness.attributes` active l'awareness  
‚úÖ `force.zone.values` **garantit** qu'aucune r√©plique ne sera sur la m√™me zone que le primaire  
‚úÖ **Tol√©rance aux pannes** : Perte d'une zone enti√®re = YELLOW mais donn√©es accessibles  
‚úÖ **Sans awareness** : Risque que primaire et r√©plique soient sur m√™me zone (perte de donn√©es)  
‚úÖ Use cases : Multi-datacenter, multi-AZ cloud (AWS Availability Zones, Azure Zones)  
‚úÖ Attributs personnalis√©s possibles : `rack`, `datacenter`, `region`  
‚úÖ Combiner avec r√©pliques multiples (2+) pour tol√©rer perte de plusieurs zones  
‚úÖ Essentiel pour **haute disponibilit√©** en production

---

## üåü Bonus Challenge 8.A: Architecture Compl√®te de Production

**Niveau**: Avanc√©  
**Objectif**: Concevoir et impl√©menter une architecture de cluster Elasticsearch compl√®te int√©grant tous les best practices de production : dedicated masters, hot-warm-cold tiers, rack awareness, monitoring, et disaster recovery.

**Contexte**: Vous √™tes architecte infrastructure pour une plateforme de logs qui ing√®re 500 GB/jour avec r√©tention de 90 jours. Vous devez concevoir un cluster production-ready avec haute disponibilit√©, performances optimales, et co√ªts ma√Ætris√©s.

### Exigences du Projet

**Exigences fonctionnelles** :
- Ingestion : 500 GB/jour (~6 MB/s)
- R√©tention : 90 jours
- RPO : 1 heure
- RTO : 30 minutes
- Disponibilit√© : 99.9% (< 8h downtime/an)

**Exigences techniques** :
- Multi-zone (2 zones minimum)
- Hot-Warm-Cold architecture
- Dedicated master nodes
- Snapshots automatis√©s
- Monitoring et alerting

### √âtape 1: Dimensionnement

**Calcul de stockage** :
- 500 GB/jour √ó 90 jours = 45 TB total
- Hot tier (7 jours) : 3.5 TB
- Warm tier (30 jours) : 15 TB
- Cold tier (53 jours) : 26.5 TB

**Calcul de shards** :
- Taille cible par shard : 30 GB
- Hot : 3500 GB / 30 GB = ~117 shards
- Avec rotation quotidienne : 7 indices √ó 17 shards = 119 primaires

**Dimensionnement n≈ìuds** :

| Tier | N≈ìuds | RAM | CPU | Disque | Total Disque |
|------|-------|-----|-----|--------|--------------|
| Master | 3 | 8 GB | 4 cores | 100 GB | 300 GB |
| Hot | 6 | 32 GB | 16 cores | 1 TB SSD | 6 TB |
| Warm | 4 | 16 GB | 8 cores | 5 TB HDD | 20 TB |
| Cold | 3 | 8 GB | 4 cores | 12 TB HDD | 36 TB |

**Total** : 16 n≈ìuds

### √âtape 2: Architecture Diagram (Textuel)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ZONE A                           ZONE B                        ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ  ‚îÇ  Master-A1  ‚îÇ                  ‚îÇ  Master-B1  ‚îÇ              ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îÇ         Master-A2 (zone_a)               (zone_b)              ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  HOT TIER (SSD, 7 jours)                                 ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Hot-A1 ‚îÇ  ‚îÇ Hot-A2 ‚îÇ  ‚îÇ Hot-B1 ‚îÇ  ‚îÇ Hot-B2 ‚îÇ  ...    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  WARM TIER (HDD, 30 jours)                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Warm-A1 ‚îÇ  ‚îÇ Warm-A2 ‚îÇ  ‚îÇ Warm-B1 ‚îÇ  ‚îÇ Warm-B2 ‚îÇ    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  COLD TIER (Searchable Snapshots, 53 jours)             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Cold-A1 ‚îÇ  ‚îÇ Cold-B1 ‚îÇ  ‚îÇ Cold-B2 ‚îÇ                 ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  MONITORING                                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Kibana (Stack Monitoring) + Prometheus + Grafana        ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### √âtape 3: Configuration ILM Policy

Cr√©ez une politique ILM compl√®te pour g√©rer le cycle de vie :

```json
PUT _ilm/policy/logs-lifecycle
{
  "policy": {
    "phases": {
      "hot": {
        "min_age": "0ms",
        "actions": {
          "rollover": {
            "max_primary_shard_size": "30GB",
            "max_age": "1d"
          },
          "set_priority": {
            "priority": 100
          }
        }
      },
      "warm": {
        "min_age": "7d",
        "actions": {
          "set_priority": {
            "priority": 50
          },
          "migrate": {
            "enabled": true
          },
          "shrink": {
            "number_of_shards": 1
          },
          "forcemerge": {
            "max_num_segments": 1
          },
          "readonly": {}
        }
      },
      "cold": {
        "min_age": "37d",
        "actions": {
          "set_priority": {
            "priority": 0
          },
          "migrate": {
            "enabled": true
          },
          "searchable_snapshot": {
            "snapshot_repository": "s3_backup",
            "force_merge_index": true
          }
        }
      },
      "delete": {
        "min_age": "90d",
        "actions": {
          "delete": {
            "delete_searchable_snapshot": true
          }
        }
      }
    }
  }
}
```

### √âtape 4: Configuration Index Template

```json
PUT _index_template/logs-template
{
  "index_patterns": ["logs-*"],
  "template": {
    "settings": {
      "number_of_shards": 3,
      "number_of_replicas": 1,
      "index.lifecycle.name": "logs-lifecycle",
      "index.lifecycle.rollover_alias": "logs-write"
    },
    "mappings": {
      "properties": {
        "@timestamp": {
          "type": "date"
        },
        "message": {
          "type": "text"
        },
        "level": {
          "type": "keyword"
        },
        "service": {
          "type": "keyword"
        },
        "host": {
          "type": "keyword"
        }
      }
    }
  },
  "priority": 500
}
```

### √âtape 5: Configuration SLM (Snapshot Lifecycle Management)

```json
PUT _slm/policy/daily-snapshots
{
  "schedule": "0 30 2 * * ?",
  "name": "<logs-{now/d}>",
  "repository": "s3_backup",
  "config": {
    "indices": ["logs-*"],
    "ignore_unavailable": true,
    "include_global_state": false
  },
  "retention": {
    "expire_after": "7d",
    "min_count": 7,
    "max_count": 30
  }
}
```

### √âtape 6: Monitoring et Alerting Setup

**Alertes critiques √† configurer** :

1. **Cluster Health RED** :
```json
PUT _watcher/watch/cluster-health-red
{
  "trigger": {
    "schedule": {
      "interval": "1m"
    }
  },
  "input": {
    "http": {
      "request": {
        "url": "http://localhost:9200/_cluster/health"
      }
    }
  },
  "condition": {
    "compare": {
      "ctx.payload.status": {
        "eq": "red"
      }
    }
  },
  "actions": {
    "notify_ops": {
      "webhook": {
        "url": "https://alerts.example.com/elasticsearch",
        "method": "post",
        "body": "Cluster is RED! Immediate action required."
      }
    }
  }
}
```

2. **Heap > 85%** (d√©j√† cr√©√© dans Lab 5.2)

3. **Disk > 85%** :
```json
PUT _watcher/watch/disk-usage-high
{
  "trigger": {
    "schedule": {
      "interval": "5m"
    }
  },
  "input": {
    "search": {
      "request": {
        "indices": [".monitoring-es-*"],
        "body": {
          "query": {
            "bool": {
              "must": [
                {
                  "range": {
                    "@timestamp": {
                      "gte": "now-10m"
                    }
                  }
                },
                {
                  "range": {
                    "node_stats.fs.total.available_in_bytes": {
                      "lt": "{{ ctx.metadata.threshold_bytes }}"
                    }
                  }
                }
              ]
            }
          }
        }
      }
    }
  },
  "metadata": {
    "threshold_bytes": 107374182400
  }
}
```

### √âtape 7: Disaster Recovery Plan

**Documentation du DR Plan** :

```markdown
# Disaster Recovery Runbook

## RPO: 1 heure
## RTO: 30 minutes

### Scenario 1: Perte d'une Zone Compl√®te

**Detection**:
- Cluster status: YELLOW
- 50% des n≈ìuds down
- Shards unassigned avec raison NODE_LEFT

**Actions**:
1. V√©rifier que les primaires sont tous actifs (Zone B)
2. Ne PAS forcer allocation imm√©diatement (attendre 10 minutes)
3. Si Zone A ne revient pas : Augmenter r√©pliques temporairement
4. Communiquer aux stakeholders : Service d√©grad√© mais op√©rationnel

**Recovery**:
1. Une fois Zone A revenue : Laisser auto-recovery
2. Monitorer la r√©allocation des shards
3. V√©rifier cluster GREEN apr√®s recovery

### Scenario 2: Corruption de Donn√©es

**Detection**:
- Requ√™tes retournent donn√©es incorrectes
- Shards en √©tat UNASSIGNED avec raison ALLOCATION_FAILED

**Actions**:
1. Identifier l'index corrompu
2. Restaurer depuis dernier snapshot (max 1h de perte)
3. Restaurer avec rename : `restored_<index-name>`
4. Valider les donn√©es
5. Basculer alias vers index restaur√©
6. Supprimer index corrompu

**Commandes**:
```bash
# 1. Lister snapshots
GET /_snapshot/s3_backup/_all

# 2. Restaurer
POST /_snapshot/s3_backup/logs-2024-01-15/_restore
{
  "indices": "logs-2024-01-15",
  "rename_pattern": "(.+)",
  "rename_replacement": "restored_$1"
}

# 3. Basculer alias
POST /_aliases
{
  "actions": [
    {"remove": {"index": "logs-2024-01-15", "alias": "logs-current"}},
    {"add": {"index": "restored_logs-2024-01-15", "alias": "logs-current"}}
  ]
}
```
```

### √âtape 8: Validation de l'Architecture

**Checklist de validation** :

- ‚úÖ **Sizing** : Calcul√© selon charges r√©elles (500 GB/jour)
- ‚úÖ **HA** : Multi-zone, r√©pliques, quorum masters
- ‚úÖ **Performance** : Hot tier SSD, shards < 50 GB
- ‚úÖ **Co√ªts** : Warm/Cold HDD pour archives
- ‚úÖ **Lifecycle** : ILM automatis√© (hot‚Üíwarm‚Üícold‚Üídelete)
- ‚úÖ **Backups** : SLM quotidien, r√©tention 7 jours
- ‚úÖ **Monitoring** : Alertes critiques (health, heap, disk)
- ‚úÖ **DR** : Runbook document√©, RPO/RTO test√©s
- ‚úÖ **Security** : RBAC, TLS, audit logging
- ‚úÖ **Documentation** : Architecture diagrams, runbooks

### Points Cl√©s √† Retenir

‚úÖ **Dimensionnement** bas√© sur charges r√©elles et croissance  
‚úÖ **Hot-Warm-Cold** optimise co√ªts (SSD uniquement pour donn√©es actives)  
‚úÖ **ILM automatise** le cycle de vie (rollover, migration, suppression)  
‚úÖ **Multi-zone** avec rack awareness garantit HA  
‚úÖ **SLM** automatise les snapshots pour DR  
‚úÖ **Monitoring proactif** d√©tecte probl√®mes avant impact utilisateur  
‚úÖ **Runbooks document√©s** acc√©l√®rent r√©solution incidents  
‚úÖ **Tests r√©guliers** (DR, load testing) valident l'architecture  
‚úÖ **Architecture √©volutive** : Ajouter n≈ìuds horizontalement selon besoins  
‚úÖ **Trade-offs** : Performance vs Co√ªts vs Complexit√©

**F√©licitations !** Vous avez con√ßu une architecture Elasticsearch production-ready ! üéâ

---

## üåü Bonus Challenge 8.B: Cr√©ation d'un Runbook de R√©ponse aux Incidents

**Niveau**: Avanc√©  
**Objectif**: Cr√©er un runbook d√©taill√© et actionnable pour les incidents de production les plus courants, avec diagnostics, mitigations, et proc√©dures de r√©solution √©tape par √©tape.

**Contexte**: Les incidents de production doivent √™tre r√©solus rapidement. Un runbook bien document√© permet √† l'√©quipe ops (m√™me juniors) de diagnostiquer et r√©soudre les probl√®mes sans escalation syst√©matique.

### Format du Runbook

Chaque incident doit inclure :
1. **Sympt√¥mes** : Comment d√©tecter l'incident
2. **Diagnostic** : Commandes pour identifier la cause root
3. **S√©v√©rit√©** : P0 (critique), P1 (majeur), P2 (mineur)
4. **Mitigation** : Actions imm√©diates pour r√©duire l'impact
5. **R√©solution** : Solution d√©finitive
6. **Post-Mortem** : Template de documentation

### Incident 1: Cluster Status RED - Shards Non Assign√©s

**S√©v√©rit√©** : P0 (CRITIQUE)

**Sympt√¥mes** :
- Dashboard monitoring affiche status RED
- Alertes : "Cluster health RED"
- Utilisateurs rapportent donn√©es manquantes

**Diagnostic √âtape par √âtape** :

```bash
# √âtape 1 : V√©rifier la sant√© globale
GET /_cluster/health?pretty

# R√©sultat attendu :
# {
#   "status": "red",
#   "unassigned_shards": 5,
#   ...
# }

# √âtape 2 : Identifier les indices RED
GET /_cat/indices?v&health=red&s=index

# √âtape 3 : Identifier les shards non assign√©s
GET /_cat/shards?v&h=index,shard,prirep,state,unassigned.reason,node | grep UNASSIGNED

# √âtape 4 : Comprendre POURQUOI non assign√©s
GET /_cluster/allocation/explain
{
  "index": "problematic-index",
  "shard": 0,
  "primary": true
}
```

**Causes Courantes et Solutions** :

**Cause 1 : N≈ìud(s) down** :
```bash
# Diagnostic
GET /_cat/nodes?v

# Solution : Attendre recovery auto (5 min)
# Ou red√©marrer n≈ìud manuellement
```

**Cause 2 : Disk watermark exceeded** :
```bash
# Diagnostic
GET /_cat/allocation?v&h=node,disk.avail,disk.used,disk.percent

# Solution imm√©diate : Augmenter watermark temporairement
PUT /_cluster/settings
{
  "transient": {
    "cluster.routing.allocation.disk.watermark.low": "95%",
    "cluster.routing.allocation.disk.watermark.high": "97%",
    "cluster.routing.allocation.disk.watermark.flood_stage": "99%"
  }
}

# Solution d√©finitive : Ajouter disque ou supprimer donn√©es anciennes
DELETE /old-index-2023-*
```

**Cause 3 : Shard corruption** :
```bash
# Diagnostic
GET /_cluster/allocation/explain

# Si d√©cision = "NO" et raison = "ALLOCATION_FAILED"

# Solution : Restaurer depuis snapshot
POST /_snapshot/my_backup/latest/_restore
{
  "indices": "corrupted-index"
}
```

**Mitigation (Urgence)** :

Si l'index RED est non-critique :
```bash
# Supprimer temporairement pour restaurer cluster GREEN
DELETE /non-critical-index
```

Si l'index RED est critique :
```bash
# Forcer allocation d'un replica comme primaire (DANGER)
POST /_cluster/reroute
{
  "commands": [{
    "allocate_replica": {
      "index": "critical-index",
      "shard": 0,
      "node": "node-with-replica"
    }
  }]
}
```

**Post-Incident** :
- Documenter cause root
- Si disk full : Planifier augmentation capacit√©
- Si corruption : V√©rifier int√©grit√© hardware

---

### Incident 2: Performance D√©grad√©e - Searches Lentes

**S√©v√©rit√©** : P1 (MAJEUR)

**Sympt√¥mes** :
- P95 search latency > 5s (normal < 1s)
- Utilisateurs rapportent lenteur application
- Dashboards Kibana chargent lentement

**Diagnostic** :

```bash
# √âtape 1 : Identifier les slow queries
GET /slow-index/_settings?include_defaults=false&filter_path=*.index.search.slowlog

# Si pas activ√©, activer temporairement :
PUT /*/_settings
{
  "index.search.slowlog.threshold.query.warn": "1s",
  "index.search.slowlog.threshold.query.info": "500ms"
}

# √âtape 2 : Analyser les hot threads
GET /_nodes/hot_threads?threads=10

# √âtape 3 : V√©rifier heap usage
GET /_cat/nodes?v&h=name,heap.percent,heap.current,heap.max,ram.percent

# √âtape 4 : Identifier les tasks lentes
GET /_cat/tasks?v&detailed&s=running_time:desc

# √âtape 5 : Analyser thread pools
GET /_cat/thread_pool?v&h=name,active,rejected,queue,completed&s=rejected:desc
```

**Causes Courantes et Solutions** :

**Cause 1 : Requ√™tes inefficaces (wildcards, regex)** :
```bash
# Identifier dans slow logs :
# "query": { "wildcard": { "field": "*pattern*" } }

# Solution : Optimiser requ√™te c√¥t√© application
# Remplacer wildcard par prefix ou term query
```

**Cause 2 : Heap pressure (GC thrashing)** :
```bash
# Diagnostic
GET /_nodes/stats/jvm?filter_path=nodes.*.jvm.gc

# Si Old GC > 5s fr√©quent ‚Üí Augmenter heap ou r√©duire charge

# Mitigation imm√©diate : Clear fielddata cache
POST /_cache/clear?fielddata=true
```

**Cause 3 : Shard count trop √©lev√©** :
```bash
# Diagnostic
GET /_cat/shards?v | wc -l

# Si > 10,000 shards total ‚Üí Probl√®me

# Solution : Shrink indices anciens
POST /old-index/_shrink/old-index-shrunk
{
  "settings": {
    "index.number_of_shards": 1
  }
}
```

**Mitigation** :
```bash
# R√©duire search thread pool temporairement (limit concurrent searches)
PUT /_cluster/settings
{
  "transient": {
    "thread_pool.search.size": 10
  }
}

# Ou augmenter queue size
PUT /_cluster/settings
{
  "transient": {
    "thread_pool.search.queue_size": 2000
  }
}
```

---

### Incident 3: Indexation Bloqu√©e - Rejections

**S√©v√©rit√©** : P1 (MAJEUR)

**Sympt√¥mes** :
- Erreurs 429 (Too Many Requests)
- M√©triques : `bulk.rejected` > 0
- Donn√©es ne sont plus index√©es

**Diagnostic** :

```bash
# √âtape 1 : V√©rifier les rejections
GET /_cat/thread_pool?v&h=name,active,rejected,queue&s=rejected:desc

# √âtape 2 : V√©rifier la queue write
GET /_nodes/stats/thread_pool?filter_path=nodes.*.thread_pool.write

# √âtape 3 : Identifier la cause de lenteur
GET /_cat/pending_tasks?v

# √âtape 4 : V√©rifier si merges en cours
GET /_cat/nodes?v&h=name,merges.current,merges.current_docs

# √âtape 5 : Disk I/O saturation ?
GET /_nodes/stats/fs?filter_path=nodes.*.fs.io_stats
```

**Solutions** :

**Cause : Write queue satur√©e** :
```bash
# Augmenter temporairement
PUT /_cluster/settings
{
  "transient": {
    "thread_pool.write.queue_size": 2000
  }
}

# R√©duire la charge (c√¥t√© application)
# Ou ajouter des data nodes
```

**Cause : Merges trop agressifs** :
```bash
# Throttle merges
PUT /*/_settings
{
  "index.merge.scheduler.max_thread_count": 1
}
```

---

### Incident 4: Split-Brain D√©tect√©

**S√©v√©rit√©** : P0 (CRITIQUE)

**Sympt√¥mes** :
- Deux clusters ind√©pendants se forment
- Certains n≈ìuds ne voient pas le master
- Donn√©es dupliqu√©es/conflictuelles

**Diagnostic** :

```bash
# √âtape 1 : V√©rifier master √©lu
GET /_cat/master?v

# Ex√©cuter sur TOUS les n≈ìuds
# Si diff√©rents masters ‚Üí Split-brain confirm√©

# √âtape 2 : V√©rifier cluster state
GET /_cluster/state/master_node,nodes

# √âtape 3 : Identifier la partition r√©seau
# Consulter logs r√©seau, firewalls
```

**R√©solution** :

**URGENT : Arr√™ter l'√©criture imm√©diatement** :
```bash
# Sur TOUS les clusters
PUT /_cluster/settings
{
  "persistent": {
    "cluster.blocks.read_only": true
  }
}
```

**Choisir le cluster authoritative** :
1. Cluster avec le plus de n≈ìuds
2. Cluster avec donn√©es les plus r√©centes

**Fusionner** :
```bash
# 1. Arr√™ter cluster non-authoritative
# 2. Supprimer data directory
# 3. Reconfigurer discovery.seed_hosts
# 4. Red√©marrer n≈ìuds
```

**Pr√©vention** :
```yaml
# elasticsearch.yml
discovery.zen.minimum_master_nodes: 2  # Pour 3 masters
```

---

### Template de Post-Mortem

```markdown
# Post-Mortem: [Titre Incident]

**Date**: YYYY-MM-DD
**Dur√©e Totale**: Xh XXmin
**Impact**: [Description impact utilisateurs]
**S√©v√©rit√©**: P0 / P1 / P2

## Timeline

| Heure | √âv√©nement |
|-------|-----------|
| 10:00 | Alerte d√©clench√©e : Cluster RED |
| 10:05 | √âquipe ops notifi√©e (PagerDuty) |
| 10:15 | Diagnostic identifie disk full sur node-3 |
| 10:30 | Mitigation : Augmentation watermark, suppression logs anciens |
| 11:00 | Cluster retourne √† GREEN |
| 11:30 | Validation compl√®te, monitoring normalis√© |

## Root Cause

[Description d√©taill√©e de la cause racine]

Exemple : Croissance anormale de donn√©es (3x normal) suite √† bug applicatif g√©n√©rant logs en boucle.

## Impact

- **Utilisateurs affect√©s** : 15% (r√©gion EMEA uniquement)
- **Requ√™tes √©chou√©es** : ~2,000
- **Perte de donn√©es** : Aucune (r√©pliques OK)
- **Downtime** : 1h30 (cluster RED)

## Ce Qui a Bien Fonctionn√©

- ‚úÖ Alerte d√©clench√©e rapidement (< 5 min)
- ‚úÖ Runbook suivi correctement
- ‚úÖ R√©pliques ont permis de maintenir disponibilit√© partielle
- ‚úÖ Communication stakeholders r√©guli√®re

## Ce Qui Doit √ätre Am√©lior√©

- ‚ùå Alerte disk usage n'a pas √©t√© d√©clench√©e assez t√¥t
- ‚ùå Runbook disk full incomplet (manquait proc√©dure suppression logs)
- ‚ùå Pas de capacity planning proactif

## Actions Correctives

| Action | Responsable | Deadline | Statut |
|--------|-------------|----------|--------|
| Impl√©menter alerte croissance anormale donn√©es | Ops Team | 2024-01-20 | TODO |
| Automatiser suppression logs anciens (ILM) | Platform Team | 2024-01-25 | TODO |
| Fixer bug applicatif | Dev Team | 2024-01-18 | DONE |
| Mettre √† jour runbook disk full | Ops Lead | 2024-01-17 | DONE |
| Augmenter disk watermark thresholds | Ops Team | 2024-01-16 | DONE |

## Lessons Learned

1. **Capacity planning** doit √™tre plus proactif (alertes sur tendances)
2. **Runbooks** doivent √™tre test√©s r√©guli√®rement (drill exercises)
3. **Automation** (ILM) r√©duit risques humains

## Liens

- [Incident Ticket](https://jira.company.com/INC-12345)
- [Chat Logs](https://slack.com/archives/ops/threads/...)
- [Monitoring Dashboard](https://grafana.company.com/d/elasticsearch)
```

### Points Cl√©s √† Retenir

‚úÖ **Runbooks document√©s** acc√©l√®rent r√©solution (pas de recherche pendant incident)  
‚úÖ **Diagnostics √©tape par √©tape** guident m√™me √©quipes juniors  
‚úÖ **Mitigation vs R√©solution** : Mitigation r√©duit impact, r√©solution √©limine cause  
‚úÖ **Post-Mortems** transforment incidents en apprentissages  
‚úÖ **Actions correctives** pr√©viennent r√©currences  
‚úÖ **Tests r√©guliers** (game days, chaos engineering) valident runbooks  
‚úÖ **S√©v√©rit√©s claires** (P0, P1, P2) priorisent r√©ponses  
‚úÖ **Communication** stakeholders transparente r√©duit escalations  
‚úÖ **Automation** (alertes, auto-remediation) r√©duit MTTR  
‚úÖ **Culture blameless** encourage partage et am√©lioration continue

**F√©licitations !** Vous ma√Ætrisez maintenant toutes les bonnes pratiques pour g√©rer un cluster Elasticsearch en production ! üéâüéâüéâ


---

# Jour 3 - Monitoring, S√©curit√© et APM

## Lab 4.1: Utilisation de l'API Cluster Health

**Topic**: Monitoring - APIs de Surveillance
**Pr√©requis**: Cluster Elasticsearch avec au moins 1 n≈ìud actif

### Objectif

Ma√Ætriser l'API `_cluster/health` pour diagnostiquer l'√©tat du cluster, interpr√©ter les statuts (green/yellow/red), et identifier les shards non allou√©s.

### Contexte

Vous recevez une alerte indiquant que le cluster est pass√© en statut `yellow`. Vous devez diagnostiquer la cause et comprendre l'impact sur le service.

### Exercice de Base

#### Setup

**Avant de commencer**:
1. V√©rifiez que votre cluster est accessible: `GET /`
2. Cr√©ez un index de test avec replicas:

```bash
PUT /health-test
{
  "settings": {
    "number_of_shards": 2,
    "number_of_replicas": 1
  }
}
```

#### √âtapes

**√âtape 1**: Consulter le cluster health basique

```bash
GET /_cluster/health
```

**R√©sultat attendu**:
```json
{
  "cluster_name": "elasticsearch",
  "status": "yellow",
  "timed_out": false,
  "number_of_nodes": 1,
  "number_of_data_nodes": 1,
  "active_primary_shards": 2,
  "active_shards": 2,
  "relocating_shards": 0,
  "initializing_shards": 0,
  "unassigned_shards": 2,
  "delayed_unassigned_shards": 0,
  "number_of_pending_tasks": 0,
  "number_of_in_flight_fetch": 0,
  "task_max_waiting_in_queue_millis": 0,
  "active_shards_percent_as_number": 50.0
}
```

**Interpr√©tation**:
- üü° **status: "yellow"**: Au moins un replica shard non allou√©
- ‚úÖ **active_primary_shards: 2**: Tous les primaires sont actifs (pas de perte de donn√©es)
- ‚ö†Ô∏è **unassigned_shards: 2**: 2 replicas ne peuvent pas √™tre allou√©s (cluster √† 1 n≈ìud)
- ‚ö†Ô∏è **active_shards_percent: 50%**: Seulement la moiti√© des shards sont actifs

**√âtape 2**: Obtenir des d√©tails par index

```bash
GET /_cluster/health?level=indices
```

**R√©sultat attendu**:
```json
{
  "cluster_name": "elasticsearch",
  "status": "yellow",
  "indices": {
    "health-test": {
      "status": "yellow",
      "number_of_shards": 2,
      "number_of_replicas": 1,
      "active_primary_shards": 2,
      "active_shards": 2,
      "relocating_shards": 0,
      "initializing_shards": 0,
      "unassigned_shards": 2
    }
  }
}
```

**Observation**: L'index `health-test` est responsable du statut yellow.

**√âtape 3**: Identifier les shards non allou√©s

```bash
GET /_cat/shards/health-test?v&h=index,shard,prirep,state,unassigned.reason
```

**R√©sultat attendu**:
```
index       shard prirep state      unassigned.reason
health-test 0     p      STARTED    
health-test 0     r      UNASSIGNED NODE_LEFT
health-test 1     p      STARTED    
health-test 1     r      UNASSIGNED NODE_LEFT
```

**Explication**:
- Les 2 shards primaires (p) sont STARTED ‚úÖ
- Les 2 shards replicas (r) sont UNASSIGNED avec raison "NODE_LEFT"
- **Cause**: Pas assez de n≈ìuds pour allouer les replicas (besoin de 2 n≈ìuds minimum)

**√âtape 4**: Comprendre les couleurs de statut

| Statut | Signification | Impact | Action |
|--------|---------------|--------|--------|
| üü¢ **GREEN** | Tous les shards (primaires + replicas) allou√©s | Aucun | Normal |
| üü° **YELLOW** | Tous primaires allou√©s, certains replicas manquants | Fonctionnel, mais pas de HA | Surveillance, non urgent |
| üî¥ **RED** | Au moins un primaire manquant | **PERTE DE DONN√âES** | Action imm√©diate |

**√âtape 5**: Simuler un cluster RED (optionnel, avec pr√©caution)

**Attention**: Cette manipulation peut entra√Æner une perte de donn√©es temporaire.

```bash
# Cr√©er un index
PUT /red-test
{
  "settings": {
    "number_of_shards": 2,
    "number_of_replicas": 0
  }
}

# Indexer des documents
POST /red-test/_doc/1
{"message": "Test document"}

# Fermer l'index (simule un shard primaire indisponible)
POST /red-test/_close

# V√©rifier le cluster health
GET /_cluster/health
```

**R√©sultat attendu**: `"status": "red"` (au moins un shard primaire ferm√©)

**Reopen pour restaurer**:
```bash
POST /red-test/_open
GET /_cluster/health
```

**√âtape 6**: Utiliser les param√®tres de l'API

**Attendre le statut green** (timeout 30s):
```bash
GET /_cluster/health?wait_for_status=green&timeout=30s
```

**Attendre qu'aucun shard ne soit relocating**:
```bash
GET /_cluster/health?wait_for_no_relocating_shards=true&timeout=30s
```

**Filtrer un index sp√©cifique**:
```bash
GET /_cluster/health/health-test
```

#### Validation

**Commandes de v√©rification**:

1. R√©sum√© cluster avec m√©triques cl√©s:
```bash
GET /_cluster/health?filter_path=status,number_of_nodes,active_shards,unassigned_shards
```

2. Sant√© de tous les index:
```bash
GET /_cluster/health?level=indices&filter_path=indices.*.status
```

3. Identifier tous les shards unassigned du cluster:
```bash
GET /_cat/shards?v&h=index,shard,prirep,state,unassigned.reason | grep UNASSIGNED
```

4. Expliquer pourquoi un shard est unassigned:
```bash
GET /_cluster/allocation/explain
{
  "index": "health-test",
  "shard": 0,
  "primary": false
}
```

**R√©sultat exemple**:
```json
{
  "index": "health-test",
  "shard": 0,
  "primary": false,
  "current_state": "unassigned",
  "unassigned_info": {
    "reason": "INDEX_CREATED",
    "at": "2023-11-10T10:00:00.000Z",
    "details": "not enough data nodes to allocate shard, allocation would violate shard allocation rules"
  },
  "can_allocate": "no",
  "allocate_explanation": "cannot allocate because allocation is not permitted to any of the nodes"
}
```

#### Crit√®res de Succ√®s

- ‚úÖ Comprendre les 3 statuts (green/yellow/red) et leur signification
- ‚úÖ Identifier les shards unassigned avec `_cat/shards`
- ‚úÖ Utiliser `_cluster/allocation/explain` pour diagnostiquer
- ‚úÖ Interpr√©ter `active_shards_percent` (100% = green, <100% = yellow/red)
- ‚úÖ Savoir quand un statut yellow est acceptable (dev/test avec 1 n≈ìud)

#### D√©pannage

**Probl√®me**: Cluster reste yellow m√™me avec 2 n≈ìuds
‚Üí V√©rifiez les r√®gles d'allocation: `GET /_cluster/settings`
‚Üí V√©rifiez que les n≈ìuds ont le r√¥le `data`: `GET /_cat/nodes?v&h=name,node.role`
‚Üí V√©rifiez l'espace disque: watermark flood peut bloquer l'allocation

**Probl√®me**: Cluster passe en red apr√®s suppression d'un index
‚Üí Normal temporairement, les shards doivent √™tre r√©allou√©s
‚Üí Attendez quelques secondes et rev√©rifiez: `GET /_cluster/health`
‚Üí Si reste red, v√©rifiez les logs: `tail -f /var/log/elasticsearch/elasticsearch.log`

**Probl√®me**: `active_shards_percent` bloqu√© √† un pourcentage
‚Üí Des shards sont INITIALIZING (en cours de copie)
‚Üí V√©rifiez avec: `GET /_cat/recovery?v`
‚Üí Attendez la fin de la r√©cup√©ration

---

## Lab 4.2: Monitoring des Statistiques de N≈ìuds

**Topic**: Monitoring - M√©triques Critiques
**Pr√©requis**: Cluster Elasticsearch actif

### Objectif

Utiliser l'API `_nodes/stats` pour extraire les m√©triques critiques (heap JVM, CPU, disk I/O) et surveiller la sant√© des n≈ìuds individuellement.

### Contexte

L'√©quipe infrastructure demande un rapport sur l'utilisation des ressources du cluster. Vous devez extraire les m√©triques cl√©s pour identifier les n≈ìuds surcharg√©s.

### Exercice de Base

#### Setup

**Avant de commencer**:
1. Identifiez les n≈ìuds du cluster: `GET /_cat/nodes?v`
2. Notez les noms des n≈ìuds pour les requ√™tes filtrant

#### √âtapes

**√âtape 1**: Obtenir les statistiques JVM (heap usage)

```bash
GET /_nodes/stats/jvm?filter_path=nodes.*.name,nodes.*.jvm.mem
```

**R√©sultat attendu**:
```json
{
  "nodes": {
    "abc123": {
      "name": "node-1",
      "jvm": {
        "mem": {
          "heap_used_in_bytes": 5368709120,
          "heap_used_percent": 25,
          "heap_committed_in_bytes": 21474836480,
          "heap_max_in_bytes": 21474836480,
          "non_heap_used_in_bytes": 157286400,
          "non_heap_committed_in_bytes": 164626432
        }
      }
    }
  }
}
```

**Analyse**:
```
Heap used:         5,368,709,120 bytes = 5 GB
Heap max:         21,474,836,480 bytes = 20 GB
Heap used %:      25%
```

**Interpr√©tation**:
- ‚úÖ <75%: Sain
- ‚ö†Ô∏è 75-85%: Surveiller
- ‚ùå >85%: Critique (risque OutOfMemoryError)

**√âtape 2**: V√©rifier les Garbage Collection stats

```bash
GET /_nodes/stats/jvm?filter_path=nodes.*.name,nodes.*.jvm.gc
```

**R√©sultat attendu**:
```json
{
  "nodes": {
    "abc123": {
      "name": "node-1",
      "jvm": {
        "gc": {
          "collectors": {
            "young": {
              "collection_count": 1234,
              "collection_time_in_millis": 12340
            },
            "old": {
              "collection_count": 5,
              "collection_time_in_millis": 500
            }
          }
        }
      }
    }
  }
}
```

**Calculs**:
```
Dur√©e moyenne GC young: 12,340 ms / 1,234 = 10 ms par GC
Dur√©e moyenne GC old:   500 ms / 5 = 100 ms par GC
```

**Alertes**:
- ‚ö†Ô∏è GC young > 50 ms: Heap sous pression
- ‚ùå GC old > 1000 ms: Heap critiquement plein
- ‚ùå GC fr√©quents (>10/minute): Heap trop petit

**√âtape 3**: Monitorer l'utilisation CPU et RAM (OS level)

```bash
GET /_nodes/stats/os?filter_path=nodes.*.name,nodes.*.os.cpu,nodes.*.os.mem
```

**R√©sultat attendu**:
```json
{
  "nodes": {
    "abc123": {
      "name": "node-1",
      "os": {
        "cpu": {
          "percent": 45,
          "load_average": {
            "1m": 2.5,
            "5m": 2.0,
            "15m": 1.8
          }
        },
        "mem": {
          "total_in_bytes": 68719476736,
          "free_in_bytes": 20000000000,
          "used_in_bytes": 48719476736,
          "free_percent": 29,
          "used_percent": 71
        }
      }
    }
  }
}
```

**Analyse**:
```
CPU usage:        45% (moyenne r√©cente)
Load average 1m:  2.5 (sur un serveur 16 cores ‚Üí 2.5/16 = 15.6% load)
RAM usage:        71% (incluant OS cache)
RAM free:         29%
```

**Thresholds**:
- CPU: <60% ‚úÖ, 60-80% ‚ö†Ô∏è, >80% ‚ùå
- Load avg: <cores ‚úÖ, cores-2√ócores ‚ö†Ô∏è, >2√ócores ‚ùå
- RAM: >20% free ‚úÖ, 10-20% free ‚ö†Ô∏è, <10% free ‚ùå

**√âtape 4**: V√©rifier l'utilisation disque et I/O

```bash
GET /_nodes/stats/fs?filter_path=nodes.*.name,nodes.*.fs.total,nodes.*.fs.io_stats
```

**R√©sultat attendu**:
```json
{
  "nodes": {
    "abc123": {
      "name": "node-1",
      "fs": {
        "total": {
          "total_in_bytes": 2000000000000,
          "free_in_bytes": 1200000000000,
          "available_in_bytes": 1200000000000
        },
        "io_stats": {
          "total": {
            "operations": 123456789,
            "read_operations": 98765432,
            "write_operations": 24691357,
            "read_kilobytes": 5000000,
            "write_kilobytes": 3000000
          }
        }
      }
    }
  }
}
```

**Calculs**:
```
Disque total:     2,000 GB (2 TB)
Disque utilis√©:   800 GB (40%)
Disque libre:     1,200 GB (60%)

I/O read:         5,000,000 KB = 4.88 GB
I/O write:        3,000,000 KB = 2.93 GB
```

**Thresholds disque** (watermarks):
- <85%: ‚úÖ Sain
- 85-90%: ‚ö†Ô∏è LOW watermark (pas de nouveaux shards)
- 90-95%: ‚ö†Ô∏è HIGH watermark (relocate shards)
- >95%: ‚ùå FLOOD (indices en read-only)

**√âtape 5**: Surveiller les m√©triques d'indexation et recherche

```bash
GET /_nodes/stats/indices?filter_path=nodes.*.name,nodes.*.indices.indexing,nodes.*.indices.search
```

**R√©sultat attendu**:
```json
{
  "nodes": {
    "abc123": {
      "name": "node-1",
      "indices": {
        "indexing": {
          "index_total": 10000000,
          "index_time_in_millis": 5000000,
          "index_current": 5,
          "index_failed": 10
        },
        "search": {
          "query_total": 500000,
          "query_time_in_millis": 2000000,
          "query_current": 2,
          "fetch_total": 450000,
          "fetch_time_in_millis": 500000
        }
      }
    }
  }
}
```

**Calculs de performance**:
```
Indexing:
  - Latence moyenne: 5,000,000 ms / 10,000,000 docs = 0.5 ms/doc
  - Taux d'√©chec: 10 / 10,000,000 = 0.0001% ‚úÖ

Search:
  - Latence query: 2,000,000 ms / 500,000 = 4 ms/query
  - Latence fetch: 500,000 ms / 450,000 = 1.1 ms/fetch
  - Total: ~5.1 ms par recherche ‚úÖ
```

**√âtape 6**: Cr√©er un tableau de bord synth√©tique

Combinez toutes les m√©triques dans une seule requ√™te:

```bash
GET /_nodes/stats?filter_path=nodes.*.name,nodes.*.jvm.mem.heap_used_percent,nodes.*.os.cpu.percent,nodes.*.fs.total.available_in_bytes,nodes.*.indices.search.query_time_in_millis
```

**Cr√©ez un tableau manuel**:

| Node | Heap | CPU | Disk Free | Search Latency |
|------|------|-----|-----------|----------------|
| node-1 | 25% ‚úÖ | 45% ‚úÖ | 60% ‚úÖ | 4 ms ‚úÖ |
| node-2 | 78% ‚ö†Ô∏è | 82% ‚ùå | 12% ‚ö†Ô∏è | 15 ms ‚ö†Ô∏è |

**Observations**: node-2 n√©cessite une attention (CPU et heap √©lev√©s).

#### Validation

**Commandes de v√©rification**:

1. R√©sum√© rapide de tous les n≈ìuds:
```bash
GET /_cat/nodes?v&h=name,heap.percent,ram.percent,cpu,load_1m,disk.used_percent
```

**R√©sultat**:
```
name   heap.percent ram.percent cpu load_1m disk.used_percent
node-1 25           71          45  2.5     40
```

2. Comparer les performances entre n≈ìuds:
```bash
GET /_nodes/stats/indices?filter_path=nodes.*.name,nodes.*.indices.indexing.index_time_in_millis,nodes.*.indices.search.query_time_in_millis
```

3. Identifier le n≈ìud le plus charg√©:
```bash
GET /_cat/nodes?v&h=name,cpu,load_1m&s=cpu:desc
```

#### Crit√®res de Succ√®s

- ‚úÖ Extraire heap usage avec `_nodes/stats/jvm`
- ‚úÖ Interpr√©ter les m√©triques CPU/RAM/disk
- ‚úÖ Calculer les latences moyennes (indexing, search)
- ‚úÖ Identifier les n≈ìuds surcharg√©s (CPU >80%, heap >85%)
- ‚úÖ Comprendre les watermarks disque (85%, 90%, 95%)

#### D√©pannage

**Probl√®me**: Heap usage constamment >85%
‚Üí Cluster sous-dimensionn√©, ajouter des n≈ìuds
‚Üí Ou augmenter le heap (si <32 GB et RAM disponible)
‚Üí V√©rifier les requ√™tes: certaines peuvent consommer trop de m√©moire

**Probl√®me**: CPU √©lev√© mais load average faible
‚Üí CPU utilis√© par des t√¢ches courtes (bursts)
‚Üí Normal si indexation/recherche intensive par pics
‚Üí Surveiller les thread pool rejections

**Probl√®me**: Disque plein mais cluster n'utilise pas toute la capacit√©
‚Üí V√©rifier les watermarks: `GET /_cluster/settings?include_defaults&filter_path=*.cluster.routing.allocation.disk.watermark*`
‚Üí Augmenter les watermarks si n√©cessaire (avec prudence)

---

## Lab 4.3: Configuration et Analyse des Slow Query Logs

**Topic**: Monitoring - Analyse des Logs
**Pr√©requis**: Cluster Elasticsearch, acc√®s aux fichiers de logs

### Objectif

Configurer les slow query logs pour capturer les requ√™tes lentes, ex√©cuter une requ√™te intentionnellement lente, et analyser les logs pour identifier les optimisations possibles.

### Contexte

Les utilisateurs se plaignent de lenteur sur certaines recherches. Vous devez activer les slow logs pour identifier les requ√™tes probl√©matiques et leur temps d'ex√©cution.

### Exercice de Base

#### Setup

**Avant de commencer**:
1. Cr√©ez un index de test avec des donn√©es:

```bash
PUT /slowlog-test
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0
  },
  "mappings": {
    "properties": {
      "title": { "type": "text" },
      "content": { "type": "text" },
      "category": { "type": "keyword" },
      "views": { "type": "integer" }
    }
  }
}

# Indexer des documents (1000 pour avoir du volume)
POST /slowlog-test/_bulk
{"index":{}}
{"title":"Article 1","content":"Long content here...","category":"tech","views":100}
{"index":{}}
{"title":"Article 2","content":"Another long content...","category":"science","views":200}
... (r√©p√©ter jusqu'√† 1000 docs)
```

#### √âtapes

**√âtape 1**: Configurer les seuils de slow query log

```bash
PUT /slowlog-test/_settings
{
  "index.search.slowlog.threshold.query.warn": "500ms",
  "index.search.slowlog.threshold.query.info": "250ms",
  "index.search.slowlog.threshold.query.debug": "100ms",
  "index.search.slowlog.threshold.query.trace": "50ms",
  "index.search.slowlog.level": "info"
}
```

**Explication des niveaux**:
- **WARN** (500ms): Requ√™tes tr√®s lentes
- **INFO** (250ms): Requ√™tes lentes
- **DEBUG** (100ms): Requ√™tes moyennement lentes
- **TRACE** (50ms): Toutes les requ√™tes un peu lentes

**Note**: Seuls les logs du niveau configur√© et au-dessus sont √©crits (`level: info` ‚Üí logs ‚â•250ms).

**√âtape 2**: Configurer les slow indexing logs (optionnel)

```bash
PUT /slowlog-test/_settings
{
  "index.indexing.slowlog.threshold.index.warn": "1s",
  "index.indexing.slowlog.threshold.index.info": "500ms",
  "index.indexing.slowlog.threshold.index.debug": "250ms",
  "index.indexing.slowlog.threshold.index.trace": "100ms",
  "index.indexing.slowlog.level": "info"
}
```

**√âtape 3**: Localiser les fichiers de slow logs

**Emplacement par d√©faut**:
```
/var/log/elasticsearch/<cluster_name>_index_search_slowlog.log
/var/log/elasticsearch/<cluster_name>_index_indexing_slowlog.log
```

**V√©rifier les logs existent**:
```bash
ls -lh /var/log/elasticsearch/*slowlog.log
# ou si installation par archive:
ls -lh logs/*slowlog.log
```

**√âtape 4**: Ex√©cuter une requ√™te lente

Cr√©ez une requ√™te intentionnellement co√ªteuse (wildcard sur gros volume):

```bash
GET /slowlog-test/_search
{
  "query": {
    "wildcard": {
      "content": "*long*content*"
    }
  },
  "size": 100
}
```

**Note**: Les requ√™tes wildcard sont lentes car elles ne peuvent pas utiliser l'index invers√© efficacement.

Ou utilisez une agr√©gation complexe:

```bash
GET /slowlog-test/_search
{
  "size": 0,
  "aggs": {
    "categories": {
      "terms": {
        "field": "category",
        "size": 100
      },
      "aggs": {
        "avg_views": {
          "avg": { "field": "views" }
        },
        "top_hits": {
          "top_hits": {
            "size": 10,
            "_source": ["title", "views"]
          }
        }
      }
    }
  }
}
```

**√âtape 5**: Analyser les slow logs

Consultez le fichier de slow log:

```bash
tail -f /var/log/elasticsearch/elasticsearch_index_search_slowlog.log
```

**Format d'une entr√©e slow log**:
```
[2023-11-10T10:30:15,123][INFO ][i.s.s.query] [node-1] [slowlog-test][0] 
took[312ms], took_millis[312], total_hits[100 hits], 
types[], stats[], search_type[QUERY_THEN_FETCH], total_shards[1], 
source[{"query":{"wildcard":{"content":"*long*content*"}},"size":100}]
```

**Analyse**:
- **took**: 312 ms (au-dessus du seuil INFO de 250ms)
- **total_hits**: 100 r√©sultats
- **source**: La requ√™te compl√®te (JSON)

**√âtape 6**: Identifier les patterns de requ√™tes lentes

Recherchez dans les logs les requ√™tes fr√©quemment lentes:

```bash
# Compter les occurrences de wildcard
grep "wildcard" /var/log/elasticsearch/*_index_search_slowlog.log | wc -l

# Extraire les temps took
grep -oP 'took\[\K[0-9]+ms' /var/log/elasticsearch/*_index_search_slowlog.log | sort -n

# Identifier les index les plus impact√©s
grep "slowlog-test" /var/log/elasticsearch/*_index_search_slowlog.log | wc -l
```

**√âtape 7**: Optimiser la requ√™te identifi√©e

**Avant** (wildcard lent):
```bash
GET /slowlog-test/_search
{
  "query": {
    "wildcard": {
      "content": "*long*content*"
    }
  }
}
```
Temps: ~300ms

**Apr√®s** (match query rapide):
```bash
GET /slowlog-test/_search
{
  "query": {
    "match": {
      "content": "long content"
    }
  }
}
```
Temps: ~10ms ‚úÖ (√ó30 plus rapide)

**√âtape 8**: D√©sactiver les slow logs (si n√©cessaire)

```bash
PUT /slowlog-test/_settings
{
  "index.search.slowlog.threshold.query.warn": "-1",
  "index.search.slowlog.threshold.query.info": "-1",
  "index.search.slowlog.threshold.query.debug": "-1",
  "index.search.slowlog.threshold.query.trace": "-1"
}
```

**Note**: `-1` d√©sactive le logging pour ce niveau.

#### Validation

**Commandes de v√©rification**:

1. V√©rifier la configuration slow log d'un index:
```bash
GET /slowlog-test/_settings?include_defaults&filter_path=*.index.search.slowlog*,*.index.indexing.slowlog*
```

2. Forcer une requ√™te lente et v√©rifier le log imm√©diatement:
```bash
# Ex√©cuter une requ√™te lente
GET /slowlog-test/_search?scroll=1m
{
  "size": 1000,
  "query": { "match_all": {} }
}

# V√©rifier le slow log (derni√®res 10 lignes)
tail -10 /var/log/elasticsearch/elasticsearch_index_search_slowlog.log
```

3. Statistiques des slow logs (avec script):
```bash
# Extraire tous les temps took et calculer la moyenne
grep -oP 'took\[\K[0-9]+' /var/log/elasticsearch/*_index_search_slowlog.log | \
  awk '{ sum += $1; n++ } END { if (n > 0) print "Average: " sum/n " ms" }'
```

#### Crit√®res de Succ√®s

- ‚úÖ Configurer les seuils slowlog avec `PUT /index/_settings`
- ‚úÖ Localiser les fichiers de logs slowlog
- ‚úÖ Ex√©cuter une requ√™te lente (>250ms)
- ‚úÖ Lire et interpr√©ter une entr√©e de slow log
- ‚úÖ Identifier le type de requ√™te lente (wildcard, agr√©gation complexe)
- ‚úÖ Proposer une optimisation

#### D√©pannage

**Probl√®me**: Aucun slow log g√©n√©r√© m√™me avec requ√™tes lentes
‚Üí V√©rifiez le niveau de log: `index.search.slowlog.level` doit √™tre au bon niveau
‚Üí V√©rifiez que la requ√™te d√©passe effectivement le seuil (mesurez avec `?profile=true`)
‚Üí V√©rifiez les permissions du fichier de log: `ls -l /var/log/elasticsearch/`

**Probl√®me**: Fichier de slow log devient trop volumineux
‚Üí Configurez la rotation des logs dans `log4j2.properties`:
```properties
appender.index_search_slowlog_rolling.type = RollingFile
appender.index_search_slowlog_rolling.filePattern = ${sys:es.logs.base_path}${sys:file.separator}${sys:es.logs.cluster_name}_index_search_slowlog-%d{yyyy-MM-dd}.log
appender.index_search_slowlog_rolling.policies.type = Policies
appender.index_search_slowlog_rolling.policies.time.type = TimeBasedTriggeringPolicy
appender.index_search_slowlog_rolling.policies.time.interval = 1
```

**Probl√®me**: Trop de slow logs, bruit important
‚Üí Augmentez les seuils: 500ms ‚Üí 1s, 250ms ‚Üí 500ms
‚Üí Activez uniquement le niveau WARN (requ√™tes tr√®s lentes)

---

## üåü Bonus 4.A: Cr√©ation de Dashboards Kibana pour Monitoring

**Niveau**: Avanc√©
**Pr√©requis**: Kibana install√© et accessible, Stack Monitoring activ√©

### Objectif

Cr√©er un dashboard Kibana personnalis√© pour surveiller les KPIs cl√©s du cluster (cluster health, heap usage, indexing rate, search latency) avec des visualisations en temps r√©el.

### Contexte

L'√©quipe DevOps souhaite un dashboard centralis√© pour surveiller le cluster Elasticsearch sans avoir √† ex√©cuter manuellement des requ√™tes API. Vous allez cr√©er un dashboard Kibana avec les m√©triques essentielles.

### Challenge

**Partie 1**: Activer Monitoring (si pas d√©j√† fait)

Activez la collecte de m√©triques:

```bash
PUT /_cluster/settings
{
  "persistent": {
    "xpack.monitoring.collection.enabled": true
  }
}
```

V√©rifiez dans Kibana: **Stack Monitoring** ‚Üí **Overview** devrait afficher les m√©triques.

**Partie 2**: Cr√©er des visualisations dans Kibana

1. **Cluster Health Gauge** (statut vert/jaune/rouge)
   - Type: **Gauge**
   - Index pattern: `.monitoring-es-*`
   - Metric: `cluster_stats.status` (field mapping)
   - Color ranges: Green (0-1), Yellow (1-2), Red (2-3)

2. **Heap Usage Line Chart** (√©volution temporelle)
   - Type: **Line**
   - Index pattern: `.monitoring-es-*`
   - X-axis: `@timestamp` (Date Histogram, interval: 1 minute)
   - Y-axis: `node_stats.jvm.mem.heap_used_percent` (Average)
   - Threshold line: 85% (critical)

3. **Indexing Rate Area Chart**
   - Type: **Area**
   - Index pattern: `.monitoring-es-*`
   - X-axis: `@timestamp`
   - Y-axis: `node_stats.indices.indexing.index_total` (Derivative ‚Üí docs/sec)

4. **Search Latency Bar Chart**
   - Type: **Vertical Bar**
   - Index pattern: `.monitoring-es-*`
   - X-axis: `node_stats.name` (Terms, size: 10)
   - Y-axis: `node_stats.indices.search.query_time_in_millis / node_stats.indices.search.query_total` (Scripted field)

5. **Disk Usage Metric**
   - Type: **Metric**
   - Index pattern: `.monitoring-es-*`
   - Metric: `(node_stats.fs.total.total_in_bytes - node_stats.fs.total.available_in_bytes) / node_stats.fs.total.total_in_bytes * 100` (Scripted)
   - Format: Percentage

**Partie 3**: Assembler le dashboard

1. Cr√©ez un nouveau dashboard: **Kibana ‚Üí Dashboard ‚Üí Create new dashboard**
2. Ajoutez toutes les visualisations cr√©√©es
3. Organisez en grid:
   ```
   +-------------------+-------------------+
   | Cluster Health    | Heap Usage        |
   | (Gauge)           | (Line Chart)      |
   +-------------------+-------------------+
   | Indexing Rate     | Search Latency    |
   | (Area Chart)      | (Bar Chart)       |
   +-------------------+-------------------+
   | Disk Usage        | Thread Pool       |
   | (Metric)          | Rejections (Table)|
   +-------------------+-------------------+
   ```

**Partie 4**: Configurer les alertes (Watcher)

Cr√©ez une alerte pour heap >85%:

```bash
PUT _watcher/watch/heap_alert
{
  "trigger": {
    "schedule": {
      "interval": "1m"
    }
  },
  "input": {
    "search": {
      "request": {
        "indices": [".monitoring-es-*"],
        "body": {
          "query": {
            "bool": {
              "must": [
                {
                  "range": {
                    "@timestamp": {
                      "gte": "now-2m"
                    }
                  }
                },
                {
                  "range": {
                    "node_stats.jvm.mem.heap_used_percent": {
                      "gte": 85
                    }
                  }
                }
              ]
            }
          },
          "aggs": {
            "nodes": {
              "terms": {
                "field": "node_stats.node_id"
              },
              "aggs": {
                "avg_heap": {
                  "avg": {
                    "field": "node_stats.jvm.mem.heap_used_percent"
                  }
                }
              }
            }
          }
        }
      }
    }
  },
  "condition": {
    "compare": {
      "ctx.payload.hits.total": {
        "gt": 0
      }
    }
  },
  "actions": {
    "log_action": {
      "logging": {
        "text": "Heap usage above 85% on nodes: {{ctx.payload.aggregations.nodes.buckets}}"
      }
    }
  }
}
```

**Partie 5**: Configurer le refresh automatique

Dans le dashboard Kibana:
- Cliquez sur l'horloge (en haut √† droite)
- S√©lectionnez "Auto refresh: 10 seconds"
- Time range: "Last 15 minutes"

### Validation

**Checklist dashboard complet**:

- [ ] Cluster health gauge (vert/jaune/rouge)
- [ ] Heap usage line chart avec threshold 85%
- [ ] Indexing rate area chart (docs/sec)
- [ ] Search latency bar chart par n≈ìud
- [ ] Disk usage metric avec %
- [ ] Thread pool rejections table
- [ ] Auto-refresh configur√© (10s)
- [ ] Time picker sur "Last 15 minutes"

**Questions √† r√©pondre**:

1. **Pourquoi utiliser `.monitoring-es-*` comme index pattern ?**
   - Elasticsearch stocke les m√©triques de monitoring dans ces index
   - Pattern avec wildcard pour inclure tous les index de monitoring (par jour)

2. **Comment calculer le taux (rate) √† partir d'un compteur cumulatif ?**
   - Utiliser l'agr√©gation **Derivative** dans Kibana
   - Exemple: `indexing.index_total` (compteur) ‚Üí Derivative ‚Üí docs/sec (taux)

3. **Quelle est la diff√©rence entre Average et Sum pour les m√©triques ?**
   - **Average**: Moyenne sur tous les n≈ìuds (ex: heap moyen du cluster)
   - **Sum**: Total cumul√© (ex: nombre total de documents index√©s)

**Crit√®re de succ√®s**: 
- Dashboard fonctionnel avec au moins 5 visualisations
- M√©triques en temps r√©el (auto-refresh)
- Capable d'identifier un probl√®me visuellement (heap spike, rejections)

---


---


## Lab 5.1: Cr√©ation d'une Alerte Simple avec Kibana Rules

**Objectif**: Cr√©er une alerte de surveillance de la sant√© du cluster avec Kibana Rules et tester son d√©clenchement.

**Contexte**: Les Kibana Rules offrent une interface graphique intuitive pour cr√©er des alertes sans manipuler du JSON. Vous allez cr√©er une r√®gle qui surveille la sant√© du cluster et vous notifie lorsqu'il passe en statut YELLOW ou RED.

### √âtape 1: Acc√©der √† l'Interface de Gestion des R√®gles

1. Ouvrez Kibana dans votre navigateur
2. Dans le menu lat√©ral, cliquez sur **Stack Management** (ic√¥ne d'engrenage)
3. Sous la section **Alerts and Insights**, cliquez sur **Rules**

Vous devriez voir l'interface de gestion des r√®gles avec la liste des r√®gles existantes (si disponible).

### √âtape 2: Cr√©er une Nouvelle R√®gle

1. Cliquez sur le bouton **Create rule** en haut √† droite
2. S√©lectionnez le type de r√®gle: **Elasticsearch query**
   - Ce type permet d'ex√©cuter des requ√™tes Elasticsearch et de d√©clencher des alertes selon les r√©sultats
3. Donnez un nom √† votre r√®gle: `cluster-health-monitor`
4. Ajoutez des tags pour organiser vos alertes: `cluster`, `health`, `ops`

### √âtape 3: Configurer la Requ√™te de Surveillance

Dans la section **Define your query**:

1. **Index**: S√©lectionnez `.monitoring-es-*` ou cr√©ez un index temporaire pour les tests
2. **Time field**: `@timestamp` ou le champ de temps de votre index
3. **Query**: Configurez la requ√™te pour surveiller la sant√© du cluster

```json
{
  "query": {
    "bool": {
      "must": [
        {
          "range": {
            "@timestamp": {
              "gte": "now-5m"
            }
          }
        }
      ],
      "filter": [
        {
          "terms": {
            "cluster_state.status": ["yellow", "red"]
          }
        }
      ]
    }
  }
}
```

4. **Size**: Laissez √† `100` documents
5. **Threshold**: Configurez le seuil de d√©clenchement
   - **WHEN**: `query matches`
   - **FOR THE LAST**: `5 minutes`
   - **GROUPED OVER**: `all documents`

### √âtape 4: Alternative - Utiliser l'API Cluster Health

Si vous n'avez pas d'index de monitoring, cr√©ez une r√®gle avec un type **ES query** simul√©:

1. Cr√©ez un index de test pour simuler des √©tats de sant√©:

```bash
# Cr√©er un index de test
PUT /cluster_health_logs

# Indexer un document simulant un √©tat YELLOW
POST /cluster_health_logs/_doc
{
  "@timestamp": "2024-01-15T10:00:00Z",
  "status": "yellow",
  "cluster_name": "es-ops-training",
  "number_of_nodes": 3,
  "unassigned_shards": 2
}
```

2. Configurez la r√®gle pour interroger cet index:
   - **Index**: `cluster_health_logs`
   - **Time field**: `@timestamp`
   - **Query**: Rechercher les documents avec `status: yellow` ou `status: red`

### √âtape 5: Configurer la Fr√©quence de V√©rification

Dans la section **Check every**:

1. **Check every**: `1 minute`
   - La r√®gle sera √©valu√©e toutes les minutes
2. **Notify**: `Every time alert is active`
   - Alternative: `On status change` pour ne notifier que lors des changements d'√©tat

### √âtape 6: D√©finir les Actions (Actions Simplifi√©es pour Tests)

Pour ce premier lab, nous allons utiliser une action simple de journalisation:

1. Dans la section **Actions**, cliquez sur **Add action**
2. S√©lectionnez **Server log** comme type de connecteur
   - Cette action journalise dans les logs Kibana, pratique pour les tests
3. Configurez le message:

```
Alerte: Le cluster {{context.cluster.name}} est en √©tat {{context.status}}!

D√©tails:
- Statut: {{context.status}}
- N≈ìuds: {{context.number_of_nodes}}
- Shards non assign√©s: {{context.unassigned_shards}}
- Date: {{context.date}}

Action requise: V√©rifier l'√©tat du cluster avec GET _cluster/health
```

4. **Action group**: S√©lectionnez `Alert` (d√©clench√©e quand l'alerte est active)

### √âtape 7: Sauvegarder et Activer la R√®gle

1. Cliquez sur **Save** en bas de page
2. La r√®gle est automatiquement activ√©e apr√®s sa cr√©ation
3. V√©rifiez que le statut est **Enabled** dans la liste des r√®gles

### √âtape 8: Tester le D√©clenchement de l'Alerte

Maintenant testons que l'alerte se d√©clenche correctement:

#### M√©thode 1: Simuler un √âtat YELLOW (si environnement de test)

```bash
# Cr√©er un index avec 2 r√©pliques sur un cluster √† 1 seul n≈ìud
PUT /test-yellow-alert
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 2
  }
}

# V√©rifier que le cluster passe en YELLOW
GET _cluster/health
```

**R√©sultat attendu**:
```json
{
  "cluster_name": "es-ops-training",
  "status": "yellow",
  "timed_out": false,
  "number_of_nodes": 1,
  "unassigned_shards": 2,
  ...
}
```

#### M√©thode 2: Indexer un Document de Test

Si vous utilisez l'index de simulation:

```bash
# Indexer un nouveau document YELLOW
POST /cluster_health_logs/_doc
{
  "@timestamp": "{{NOW}}",
  "status": "yellow",
  "cluster_name": "es-ops-training",
  "number_of_nodes": 3,
  "unassigned_shards": 5
}

# Forcer le refresh
POST /cluster_health_logs/_refresh
```

### √âtape 9: V√©rifier que l'Alerte s'est D√©clench√©e

1. Retournez dans **Stack Management** ‚Üí **Rules**
2. Cliquez sur votre r√®gle `cluster-health-monitor`
3. Consultez l'onglet **Alert history** ou **History**
   - Vous devriez voir les d√©clenchements r√©cents
4. V√©rifiez les logs Kibana pour voir le message journalis√©:

```bash
# Depuis votre terminal, consultez les logs Kibana
docker logs kibana | grep "cluster-health-monitor"
# OU si installation locale
tail -f /var/log/kibana/kibana.log | grep "cluster-health-monitor"
```

**R√©sultat attendu dans les logs**:
```
[ALERT] cluster-health-monitor: Le cluster es-ops-training est en √©tat yellow!
```

### √âtape 10: Tester la D√©sactivation et Modification

1. **D√©sactiver la r√®gle**:
   - Dans la liste des r√®gles, cliquez sur le switch pour d√©sactiver `cluster-health-monitor`
   - Le statut passe √† **Disabled**
   - V√©rifiez qu'aucune nouvelle alerte n'est d√©clench√©e

2. **Modifier la r√®gle**:
   - Cliquez sur le nom de la r√®gle
   - Cliquez sur **Edit rule** en haut √† droite
   - Changez la fr√©quence de v√©rification √† `5 minutes`
   - Sauvegardez

3. **R√©activer la r√®gle**:
   - R√©activez le switch pour remettre la r√®gle en √©tat **Enabled**

### Validation

V√©rifiez que vous avez r√©ussi le lab:

```bash
# 1. V√©rifier que la r√®gle existe via l'API Kibana
curl -X GET "localhost:5601/api/alerting/rules" \
  -H "kbn-xsrf: true" \
  -u elastic:votre_password | jq '.data[] | select(.name=="cluster-health-monitor")'
```

**R√©sultat attendu**:
```json
{
  "id": "abc123...",
  "name": "cluster-health-monitor",
  "tags": ["cluster", "health", "ops"],
  "enabled": true,
  "schedule": {
    "interval": "1m"
  },
  ...
}
```

```bash
# 2. V√©rifier les alertes actives
GET _kibana/api/alerting/rule/ABC123/_state
```

### Points Cl√©s √† Retenir

‚úÖ **Kibana Rules** offrent une interface graphique pour cr√©er des alertes sans JSON
‚úÖ Le type **Elasticsearch query** permet d'interroger n'importe quel index
‚úÖ La **fr√©quence de v√©rification** contr√¥le combien de fois la r√®gle est √©valu√©e
‚úÖ Les **actions** d√©finissent ce qui se passe quand l'alerte se d√©clenche
‚úÖ L'action **Server log** est id√©ale pour les tests et le debugging
‚úÖ Les r√®gles peuvent √™tre **activ√©es/d√©sactiv√©es** sans les supprimer
‚úÖ L'historique des alertes est accessible via l'interface Kibana

---

## Lab 5.2: Configuration d'Actions Avanc√©es (Webhook et Index)

**Objectif**: Configurer des actions sophistiqu√©es pour vos alertes - envoyer des webhooks vers des services externes et indexer les alertes pour analyse historique.

**Contexte**: Les alertes ne sont utiles que si elles d√©clenchent les bonnes actions. Dans ce lab, vous allez configurer deux types d'actions essentielles en production: les webhooks (pour int√©grer avec des outils externes comme Slack, PagerDuty, ou vos propres services) et l'indexation (pour garder une trace de toutes les alertes).

### Partie A: Cr√©er un Connecteur Webhook

Les connecteurs sont des configurations r√©utilisables qui d√©finissent comment se connecter √† des services externes.

#### √âtape 1: Cr√©er un Service de Test pour Recevoir les Webhooks

Nous allons utiliser **webhook.site** pour tester nos webhooks:

1. Ouvrez votre navigateur et allez sur https://webhook.site
2. Notez l'URL unique g√©n√©r√©e (format: `https://webhook.site/abc-def-123...`)
   - Cette URL affichera tous les webhooks re√ßus en temps r√©el
3. Gardez cet onglet ouvert pour voir les webhooks arriver

**Alternative locale avec Netcat**:
```bash
# Terminal 1: D√©marrer un serveur HTTP simple
while true; do echo -e "HTTP/1.1 200 OK\n\n" | nc -l 8888; done

# Votre webhook URL locale: http://localhost:8888
```

#### √âtape 2: Cr√©er le Connecteur Webhook dans Kibana

1. Dans Kibana, allez dans **Stack Management** ‚Üí **Connectors**
2. Cliquez sur **Create connector**
3. S√©lectionnez **Webhook** dans la liste des types
4. Configurez le connecteur:

**Configuration**:
- **Connector name**: `ops-webhook-notifier`
- **URL**: Collez l'URL de webhook.site ou votre URL locale
- **Method**: `POST`
- **Headers**: Ajoutez les en-t√™tes suivants

```json
{
  "Content-Type": "application/json",
  "X-Alert-Source": "elasticsearch-ops"
}
```

5. Testez le connecteur:
   - Cliquez sur **Test** en bas
   - V√©rifiez que webhook.site re√ßoit bien la requ√™te

6. Cliquez sur **Save**

#### √âtape 3: Cr√©er un Connecteur Index Action

Ce connecteur permettra d'indexer les alertes dans Elasticsearch pour analyse historique.

1. Dans **Stack Management** ‚Üí **Connectors**, cliquez sur **Create connector**
2. S√©lectionnez **Index** dans la liste
3. Configurez:

**Configuration**:
- **Connector name**: `alert-history-index`
- **Index**: `alert-history`
- **Refresh**: `true` (pour que les documents soient imm√©diatement disponibles)
- **Time field**: `@timestamp` (sera ajout√© automatiquement)

4. Cliquez sur **Save**

### Partie B: Cr√©er une Alerte avec Actions Multiples

Maintenant cr√©ons une alerte qui utilise ces deux connecteurs.

#### √âtape 4: Cr√©er l'Alerte de Monitoring de Heap

1. Allez dans **Stack Management** ‚Üí **Rules**
2. Cliquez sur **Create rule**
3. Configurez la r√®gle:

**Informations de base**:
- **Name**: `heap-usage-critical`
- **Tags**: `performance`, `heap`, `critical`
- **Rule type**: **Elasticsearch query**

**Query definition**:
- **Index**: `.monitoring-es-*` ou cr√©ez un index de simulation
- **Time field**: `@timestamp`
- **Query**:

```json
{
  "query": {
    "bool": {
      "must": [
        {
          "range": {
            "@timestamp": {
              "gte": "now-5m"
            }
          }
        },
        {
          "range": {
            "node_stats.jvm.mem.heap_used_percent": {
              "gte": 85
            }
          }
        }
      ]
    }
  },
  "aggs": {
    "max_heap": {
      "max": {
        "field": "node_stats.jvm.mem.heap_used_percent"
      }
    },
    "avg_heap": {
      "avg": {
        "field": "node_stats.jvm.mem.heap_used_percent"
      }
    }
  }
}
```

**Threshold**:
- **WHEN**: `query matches`
- **FOR THE LAST**: `5 minutes`
- **GROUPED OVER**: `top 5 'node_stats.node_id'` (pour identifier les n≈ìuds probl√©matiques)

**Schedule**:
- **Check every**: `1 minute`
- **Notify**: `On status change` (pour √©viter le spam)

#### √âtape 5: Cr√©er un Index de Simulation pour Tests

Comme nous n'avons peut-√™tre pas de donn√©es de monitoring r√©elles:

```bash
# Cr√©er l'index de simulation
PUT /heap-monitoring
{
  "mappings": {
    "properties": {
      "@timestamp": { "type": "date" },
      "node_id": { "type": "keyword" },
      "node_name": { "type": "keyword" },
      "heap_used_percent": { "type": "float" }
    }
  }
}

# Indexer des donn√©es simulant un heap critique
POST /heap-monitoring/_bulk
{"index":{}}
{"@timestamp":"2024-01-15T10:00:00Z","node_id":"node-1","node_name":"es-ops-node-1","heap_used_percent":87.5}
{"index":{}}
{"@timestamp":"2024-01-15T10:01:00Z","node_id":"node-1","node_name":"es-ops-node-1","heap_used_percent":89.2}
{"index":{}}
{"@timestamp":"2024-01-15T10:02:00Z","node_id":"node-2","node_name":"es-ops-node-2","heap_used_percent":91.8}
{"index":{}}
{"@timestamp":"2024-01-15T10:00:00Z","node_id":"node-3","node_name":"es-ops-node-3","heap_used_percent":75.3}
```

Modifiez votre r√®gle pour utiliser cet index:
- **Index**: `heap-monitoring`
- **Query**: Rechercher `heap_used_percent >= 85`

#### √âtape 6: Configurer l'Action Webhook

Dans la section **Actions** de votre r√®gle:

1. Cliquez sur **Add action**
2. S√©lectionnez le connecteur `ops-webhook-notifier`
3. Configurez le payload JSON:

```json
{
  "alert_id": "{{alertId}}",
  "alert_name": "{{alertName}}",
  "alert_type": "heap_usage",
  "severity": "critical",
  "timestamp": "{{date}}",
  "context": {
    "cluster_name": "{{context.cluster.name}}",
    "condition": "Heap usage exceeded 85%",
    "details": {
      "max_heap_percent": "{{context.max_heap}}",
      "avg_heap_percent": "{{context.avg_heap}}",
      "affected_nodes": "{{context.groupBy}}"
    }
  },
  "actions_required": [
    "Check heap usage: GET _nodes/stats/jvm",
    "Review GC activity: GET _nodes/stats/jvm?filter_path=nodes.*.jvm.gc",
    "Consider increasing heap or clearing cache"
  ],
  "links": {
    "kibana_dashboard": "https://kibana.example.com/app/monitoring",
    "runbook": "https://docs.example.com/runbooks/elasticsearch-heap"
  }
}
```

4. **Action group**: `Alert` (quand l'alerte est active)
5. **Throttle**: `15 minutes` (√©viter les alertes r√©p√©t√©es)

#### √âtape 7: Configurer l'Action Index

1. Dans la m√™me r√®gle, cliquez sur **Add action** √† nouveau
2. S√©lectionnez le connecteur `alert-history-index`
3. Configurez le document √† indexer:

```json
{
  "@timestamp": "{{date}}",
  "alert": {
    "id": "{{alertId}}",
    "name": "{{alertName}}",
    "action_group": "{{context.group}}",
    "instance_id": "{{alertInstanceId}}"
  },
  "rule": {
    "id": "{{rule.id}}",
    "name": "{{rule.name}}",
    "type": "{{rule.type}}",
    "tags": {{#toJson}}rule.tags{{/toJson}}
  },
  "metrics": {
    "heap": {
      "max_percent": {{context.max_heap}},
      "avg_percent": {{context.avg_heap}},
      "threshold": 85
    }
  },
  "nodes": {
    "affected": "{{context.groupBy}}"
  },
  "status": "triggered",
  "severity": "critical",
  "message": "Heap usage critical: {{context.max_heap}}% detected on cluster {{context.cluster.name}}"
}
```

4. **Action group**: `Alert`
5. Pas de throttle n√©cessaire (nous voulons toutes les occurrences dans l'historique)

#### √âtape 8: Sauvegarder et Activer

1. Cliquez sur **Save** pour cr√©er la r√®gle avec les deux actions
2. La r√®gle est automatiquement activ√©e

### Partie C: D√©clencher et V√©rifier les Actions

#### √âtape 9: D√©clencher l'Alerte

Indexez des donn√©es qui d√©clencheront l'alerte:

```bash
# Indexer des donn√©es avec heap > 85%
POST /heap-monitoring/_doc
{
  "@timestamp": "{{NOW}}",
  "node_id": "node-1",
  "node_name": "es-ops-node-1",
  "heap_used_percent": 92.5
}

# Forcer le refresh
POST /heap-monitoring/_refresh
```

Attendez 1-2 minutes (la fr√©quence de v√©rification de la r√®gle).

#### √âtape 10: V√©rifier l'Action Webhook

1. Retournez sur webhook.site (ou votre serveur local)
2. Vous devriez voir une requ√™te POST arriver avec le payload JSON
3. V√©rifiez que les donn√©es sont correctes:
   - `alert_name`: "heap-usage-critical"
   - `severity`: "critical"
   - `context.details.max_heap_percent`: valeur > 85

**Exemple de requ√™te re√ßue**:
```json
{
  "alert_id": "alert-123-abc",
  "alert_name": "heap-usage-critical",
  "severity": "critical",
  "timestamp": "2024-01-15T10:05:30.123Z",
  "context": {
    "condition": "Heap usage exceeded 85%",
    "details": {
      "max_heap_percent": "92.5",
      "avg_heap_percent": "88.7",
      "affected_nodes": "node-1"
    }
  }
}
```

#### √âtape 11: V√©rifier l'Action Index

Interrogez l'index d'historique des alertes:

```bash
# V√©rifier que l'index a √©t√© cr√©√©
GET alert-history

# Rechercher les alertes r√©centes
GET alert-history/_search
{
  "query": {
    "range": {
      "@timestamp": {
        "gte": "now-1h"
      }
    }
  },
  "sort": [
    { "@timestamp": "desc" }
  ]
}
```

**R√©sultat attendu**:
```json
{
  "hits": {
    "total": { "value": 1 },
    "hits": [
      {
        "_source": {
          "@timestamp": "2024-01-15T10:05:30.123Z",
          "alert": {
            "id": "alert-123-abc",
            "name": "heap-usage-critical"
          },
          "metrics": {
            "heap": {
              "max_percent": 92.5,
              "avg_percent": 88.7,
              "threshold": 85
            }
          },
          "status": "triggered",
          "severity": "critical",
          "message": "Heap usage critical: 92.5% detected..."
        }
      }
    ]
  }
}
```

#### √âtape 12: Cr√©er des Visualisations de l'Historique d'Alertes

Cr√©ons un dashboard Kibana pour visualiser l'historique:

1. Allez dans **Kibana** ‚Üí **Discover**
2. Cr√©ez un **Data View** pour `alert-history`
3. Allez dans **Dashboard** ‚Üí **Create dashboard**
4. Ajoutez des visualisations:

**Visualisation 1: Timeline des Alertes**
```
Visualization type: Line chart
X-axis: @timestamp (Date histogram)
Y-axis: Count
Break down by: alert.name.keyword
```

**Visualisation 2: R√©partition par S√©v√©rit√©**
```
Visualization type: Pie chart
Slice by: severity.keyword
```

**Visualisation 3: Top N≈ìuds Probl√©matiques**
```
Visualization type: Table
Rows: nodes.affected.keyword
Metrics: Count, Max heap_percent
```

### Validation

V√©rifiez tous les √©l√©ments:

```bash
# 1. V√©rifier les connecteurs
GET _kibana/api/actions/connectors

# 2. V√©rifier la r√®gle et ses actions
GET _kibana/api/alerting/rules

# 3. Compter les alertes dans l'index
GET alert-history/_count

# 4. Statistiques sur les alertes par s√©v√©rit√©
GET alert-history/_search
{
  "size": 0,
  "aggs": {
    "by_severity": {
      "terms": {
        "field": "severity.keyword"
      }
    },
    "by_alert_name": {
      "terms": {
        "field": "alert.name.keyword"
      }
    }
  }
}
```

### Points Cl√©s √† Retenir

‚úÖ Les **connecteurs** sont r√©utilisables entre plusieurs r√®gles
‚úÖ Les **webhooks** permettent d'int√©grer avec n'importe quel service externe
‚úÖ L'**indexation des alertes** cr√©e une base de donn√©es d'historique analysable
‚úÖ Les **actions multiples** permettent de notifier ET d'archiver simultan√©ment
‚úÖ Le **throttling** √©vite les alertes r√©p√©t√©es (alert fatigue)
‚úÖ Les **payloads personnalis√©s** incluent contexte et actions recommand√©es
‚úÖ Les **variables de contexte** (`{{context.*}}`) rendent les alertes dynamiques
‚úÖ webhook.site est un outil pratique pour tester les webhooks
‚úÖ L'historique d'alertes permet de cr√©er des dashboards et des rapports

---

## üåü Bonus Challenge 5.A: Alerte Watcher Avanc√©e avec Agr√©gations Complexes

**Niveau**: Avanc√©  
**Objectif**: Cr√©er une alerte Watcher sophistiqu√©e utilisant des agr√©gations complexes pour d√©tecter des anomalies dans les patterns d'indexation.

**Contexte**: Watcher offre plus de flexibilit√© que Kibana Rules gr√¢ce √† son mod√®le JSON programmable. Dans ce challenge, vous allez cr√©er une alerte qui d√©tecte des anomalies dans le taux d'indexation en comparant la moyenne actuelle avec la moyenne historique (d√©tection de baisse soudaine qui pourrait indiquer un probl√®me).

### Sc√©nario

Votre cluster indexe normalement ~1000 documents/minute. Vous voulez √™tre alert√© si:
1. Le taux d'indexation chute en dessous de 50% de la moyenne historique
2. Cette condition persiste pendant au moins 3 minutes
3. Le probl√®me affecte plusieurs index simultan√©ment

### √âtape 1: Cr√©er des Donn√©es de Test

Cr√©ons un index simulant des m√©triques d'indexation:

```bash
# Cr√©er l'index de m√©triques
PUT /indexing-metrics
{
  "mappings": {
    "properties": {
      "@timestamp": { "type": "date" },
      "index_name": { "type": "keyword" },
      "docs_indexed": { "type": "long" },
      "indexing_rate": { "type": "float" },
      "node_id": { "type": "keyword" }
    }
  }
}

# G√©n√©rer des donn√©es historiques normales (baseline)
POST /indexing-metrics/_bulk
{"index":{}}
{"@timestamp":"2024-01-15T09:00:00Z","index_name":"products","docs_indexed":1000,"indexing_rate":950.5,"node_id":"node-1"}
{"index":{}}
{"@timestamp":"2024-01-15T09:01:00Z","index_name":"products","docs_indexed":980,"indexing_rate":975.2,"node_id":"node-1"}
{"index":{}}
{"@timestamp":"2024-01-15T09:02:00Z","index_name":"products","docs_indexed":1020,"indexing_rate":1010.8,"node_id":"node-1"}
{"index":{}}
{"@timestamp":"2024-01-15T09:03:00Z","index_name":"products","docs_indexed":995,"indexing_rate":990.1,"node_id":"node-1"}
{"index":{}}
{"@timestamp":"2024-01-15T09:00:00Z","index_name":"orders","docs_indexed":500,"indexing_rate":485.3,"node_id":"node-2"}
{"index":{}}
{"@timestamp":"2024-01-15T09:01:00Z","index_name":"orders","docs_indexed":510,"indexing_rate":505.7,"node_id":"node-2"}
{"index":{}}
{"@timestamp":"2024-01-15T09:02:00Z","index_name":"orders","docs_indexed":490,"indexing_rate":495.2,"node_id":"node-2"}
{"index":{}}
{"@timestamp":"2024-01-15T09:03:00Z","index_name":"orders","docs_indexed":505,"indexing_rate":500.8,"node_id":"node-2"}

# G√©n√©rer des donn√©es r√©centes montrant une chute (anomalie)
{"index":{}}
{"@timestamp":"2024-01-15T10:00:00Z","index_name":"products","docs_indexed":450,"indexing_rate":445.2,"node_id":"node-1"}
{"index":{}}
{"@timestamp":"2024-01-15T10:01:00Z","index_name":"products","docs_indexed":420,"indexing_rate":415.8,"node_id":"node-1"}
{"index":{}}
{"@timestamp":"2024-01-15T10:02:00Z","index_name":"products","docs_indexed":430,"indexing_rate":425.5,"node_id":"node-1"}
{"index":{}}
{"@timestamp":"2024-01-15T10:00:00Z","index_name":"orders","docs_indexed":220,"indexing_rate":215.3,"node_id":"node-2"}
{"index":{}}
{"@timestamp":"2024-01-15T10:01:00Z","index_name":"orders","docs_indexed":210,"indexing_rate":205.7,"node_id":"node-2"}
{"index":{}}
{"@timestamp":"2024-01-15T10:02:00Z","index_name":"orders","docs_indexed":225,"indexing_rate":220.1,"node_id":"node-2"}

# Forcer le refresh
POST /indexing-metrics/_refresh
```

### √âtape 2: D√©velopper la Requ√™te d'Agr√©gation

Testons d'abord notre logique de d√©tection:

```bash
GET /indexing-metrics/_search
{
  "size": 0,
  "query": {
    "range": {
      "@timestamp": {
        "gte": "now-1h"
      }
    }
  },
  "aggs": {
    "by_index": {
      "terms": {
        "field": "index_name",
        "size": 20
      },
      "aggs": {
        "recent_rate": {
          "filter": {
            "range": {
              "@timestamp": {
                "gte": "now-5m"
              }
            }
          },
          "aggs": {
            "avg_recent": {
              "avg": {
                "field": "indexing_rate"
              }
            }
          }
        },
        "baseline_rate": {
          "filter": {
            "range": {
              "@timestamp": {
                "gte": "now-30m",
                "lt": "now-5m"
              }
            }
          },
          "aggs": {
            "avg_baseline": {
              "avg": {
                "field": "indexing_rate"
              }
            }
          }
        },
        "rate_comparison": {
          "bucket_script": {
            "buckets_path": {
              "recent": "recent_rate>avg_recent",
              "baseline": "baseline_rate>avg_baseline"
            },
            "script": "params.recent / params.baseline"
          }
        }
      }
    },
    "anomalous_indexes": {
      "filter": {
        "range": {
          "rate_comparison": {
            "lt": 0.5
          }
        }
      }
    }
  }
}
```

**Logique**:
- **recent_rate**: Moyenne des 5 derni√®res minutes
- **baseline_rate**: Moyenne des 25 minutes pr√©c√©dentes (de -30m √† -5m)
- **rate_comparison**: Ratio recent/baseline (< 0.5 signifie chute de >50%)

### √âtape 3: Cr√©er la Watch Watcher

Maintenant cr√©ons la watch compl√®te avec conditions et actions multiples:

```bash
PUT _watcher/watch/indexing-rate-anomaly
{
  "metadata": {
    "name": "Indexing Rate Anomaly Detection",
    "version": "1.0",
    "description": "D√©tecte les chutes soudaines du taux d'indexation (>50%) persistant sur plusieurs minutes",
    "team": "ops",
    "severity": "high"
  },
  "trigger": {
    "schedule": {
      "interval": "2m"
    }
  },
  "input": {
    "search": {
      "request": {
        "indices": ["indexing-metrics"],
        "body": {
          "size": 0,
          "query": {
            "range": {
              "@timestamp": {
                "gte": "now-1h"
              }
            }
          },
          "aggs": {
            "by_index": {
              "terms": {
                "field": "index_name",
                "size": 50
              },
              "aggs": {
                "recent_rate": {
                  "filter": {
                    "range": {
                      "@timestamp": {
                        "gte": "now-5m"
                      }
                    }
                  },
                  "aggs": {
                    "avg_recent": {
                      "avg": {
                        "field": "indexing_rate"
                      }
                    },
                    "count_recent": {
                      "value_count": {
                        "field": "indexing_rate"
                      }
                    }
                  }
                },
                "baseline_rate": {
                  "filter": {
                    "range": {
                      "@timestamp": {
                        "gte": "now-35m",
                        "lt": "now-5m"
                      }
                    }
                  },
                  "aggs": {
                    "avg_baseline": {
                      "avg": {
                        "field": "indexing_rate"
                      }
                    },
                    "stddev_baseline": {
                      "extended_stats": {
                        "field": "indexing_rate"
                      }
                    }
                  }
                },
                "rate_drop_percent": {
                  "bucket_script": {
                    "buckets_path": {
                      "recent": "recent_rate>avg_recent",
                      "baseline": "baseline_rate>avg_baseline"
                    },
                    "script": "((params.baseline - params.recent) / params.baseline) * 100"
                  }
                }
              }
            }
          }
        }
      }
    }
  },
  "condition": {
    "script": {
      "source": """
        def anomalies = [];
        def buckets = ctx.payload.aggregations.by_index.buckets;
        
        for (bucket in buckets) {
          def recent = bucket.recent_rate.avg_recent.value;
          def baseline = bucket.baseline_rate.avg_baseline.value;
          def count = bucket.recent_rate.count_recent.value;
          def drop_percent = bucket.rate_drop_percent.value;
          
          // Conditions:
          // 1. Au moins 3 points de donn√©es r√©centes (3 minutes)
          // 2. Chute >= 50%
          // 3. Baseline non nulle
          if (count >= 3 && drop_percent >= 50 && baseline > 0) {
            anomalies.add([
              'index': bucket.key,
              'recent_rate': Math.round(recent * 100) / 100,
              'baseline_rate': Math.round(baseline * 100) / 100,
              'drop_percent': Math.round(drop_percent * 100) / 100,
              'sample_count': count
            ]);
          }
        }
        
        // D√©clencher si au moins 1 index anomalique
        ctx.payload.anomalies = anomalies;
        return anomalies.size() > 0;
      """,
      "lang": "painless"
    }
  },
  "transform": {
    "script": {
      "source": """
        def result = [
          'alert_triggered_at': ctx.execution_time,
          'affected_indexes': ctx.payload.anomalies,
          'total_affected': ctx.payload.anomalies.size(),
          'severity': ctx.payload.anomalies.size() >= 3 ? 'critical' : 'high',
          'investigation_links': [
            'cluster_stats': 'GET _cluster/stats',
            'node_stats': 'GET _nodes/stats/indices',
            'slow_logs': 'Check slow indexing logs'
          ]
        ];
        return result;
      """,
      "lang": "painless"
    }
  },
  "actions": {
    "log_to_elasticsearch": {
      "index": {
        "index": "watcher-alerts",
        "doc_id": "indexing-anomaly-{{ctx.watch_id}}-{{ctx.execution_time}}",
        "refresh": true
      }
    },
    "notify_ops_team": {
      "throttle_period": "15m",
      "webhook": {
        "scheme": "https",
        "host": "webhook.site",
        "port": 443,
        "path": "/votre-webhook-id",
        "method": "post",
        "headers": {
          "Content-Type": "application/json",
          "X-Alert-Type": "indexing-anomaly"
        },
        "body": """
{
  "alert": "Indexing Rate Anomaly Detected",
  "severity": "{{ctx.payload.severity}}",
  "triggered_at": "{{ctx.payload.alert_triggered_at}}",
  "summary": "{{ctx.payload.total_affected}} index(es) showing >50% drop in indexing rate",
  "affected_indexes": {{#toJson}}ctx.payload.affected_indexes{{/toJson}},
  "actions_required": [
    "Check cluster health: GET _cluster/health",
    "Check node disk space: GET _cat/nodes?v&h=name,disk.avail,disk.used_percent",
    "Review indexing queues: GET _cat/thread_pool/write?v",
    "Check for network issues or slow nodes"
  ],
  "investigation": {{#toJson}}ctx.payload.investigation_links{{/toJson}}
}
        """
      }
    },
    "send_detailed_email": {
      "throttle_period": "30m",
      "email": {
        "to": ["ops-team@example.com"],
        "subject": "[{{ctx.payload.severity}}] Indexing Rate Anomaly: {{ctx.payload.total_affected}} Index(es) Affected",
        "body": {
          "html": """
<html>
<body style="font-family: Arial, sans-serif;">
  <h2 style="color: #d32f2f;">‚ö†Ô∏è Indexing Rate Anomaly Detected</h2>
  
  <table style="border-collapse: collapse; width: 100%; margin: 20px 0;">
    <tr style="background-color: #f5f5f5;">
      <td style="padding: 10px; border: 1px solid #ddd;"><strong>Severity</strong></td>
      <td style="padding: 10px; border: 1px solid #ddd;">{{ctx.payload.severity}}</td>
    </tr>
    <tr>
      <td style="padding: 10px; border: 1px solid #ddd;"><strong>Triggered At</strong></td>
      <td style="padding: 10px; border: 1px solid #ddd;">{{ctx.payload.alert_triggered_at}}</td>
    </tr>
    <tr style="background-color: #f5f5f5;">
      <td style="padding: 10px; border: 1px solid #ddd;"><strong>Affected Indexes</strong></td>
      <td style="padding: 10px; border: 1px solid #ddd;">{{ctx.payload.total_affected}}</td>
    </tr>
  </table>
  
  <h3>Affected Indexes Details:</h3>
  <table style="border-collapse: collapse; width: 100%; margin: 20px 0;">
    <thead>
      <tr style="background-color: #1976d2; color: white;">
        <th style="padding: 10px; border: 1px solid #ddd; text-align: left;">Index</th>
        <th style="padding: 10px; border: 1px solid #ddd; text-align: right;">Recent Rate</th>
        <th style="padding: 10px; border: 1px solid #ddd; text-align: right;">Baseline Rate</th>
        <th style="padding: 10px; border: 1px solid #ddd; text-align: right;">Drop %</th>
      </tr>
    </thead>
    <tbody>
      {{#ctx.payload.affected_indexes}}
      <tr>
        <td style="padding: 10px; border: 1px solid #ddd;">{{index}}</td>
        <td style="padding: 10px; border: 1px solid #ddd; text-align: right;">{{recent_rate}} docs/min</td>
        <td style="padding: 10px; border: 1px solid #ddd; text-align: right;">{{baseline_rate}} docs/min</td>
        <td style="padding: 10px; border: 1px solid #ddd; text-align: right; color: #d32f2f;"><strong>‚Üì{{drop_percent}}%</strong></td>
      </tr>
      {{/ctx.payload.affected_indexes}}
    </tbody>
  </table>
  
  <h3>Recommended Actions:</h3>
  <ol>
    <li>Check cluster health: <code>GET _cluster/health</code></li>
    <li>Check node disk space: <code>GET _cat/nodes?v&h=name,disk.avail,disk.used_percent</code></li>
    <li>Review indexing queues: <code>GET _cat/thread_pool/write?v</code></li>
    <li>Check for network issues or slow nodes</li>
    <li>Review application logs for indexing errors</li>
  </ol>
  
  <p style="margin-top: 30px; padding-top: 20px; border-top: 1px solid #ddd; color: #666;">
    <em>This is an automated alert from Elasticsearch Watcher. Do not reply to this email.</em>
  </p>
</body>
</html>
          """
        }
      }
    }
  }
}
```

### √âtape 4: Tester la Watch

#### Test 1: Ex√©cution Manuelle

```bash
# Ex√©cuter la watch manuellement pour voir le r√©sultat
POST _watcher/watch/indexing-rate-anomaly/_execute
{
  "trigger_data": {
    "triggered_time": "2024-01-15T10:05:00Z"
  }
}
```

**R√©sultat attendu**:
```json
{
  "watch_record": {
    "watch_id": "indexing-rate-anomaly",
    "state": "executed",
    "trigger_event": {
      "type": "manual"
    },
    "result": {
      "condition": {
        "type": "script",
        "status": "success",
        "met": true
      },
      "actions": [
        {
          "id": "log_to_elasticsearch",
          "type": "index",
          "status": "success"
        },
        {
          "id": "notify_ops_team",
          "type": "webhook",
          "status": "success"
        }
      ]
    }
  }
}
```

#### Test 2: V√©rifier l'Index d'Alertes

```bash
# V√©rifier que l'alerte a √©t√© index√©e
GET watcher-alerts/_search
{
  "query": {
    "match": {
      "watch_id": "indexing-rate-anomaly"
    }
  },
  "sort": [
    { "alert_triggered_at": "desc" }
  ]
}
```

#### Test 3: V√©rifier le Webhook

Consultez webhook.site pour voir le payload JSON envoy√©.

### √âtape 5: Cr√©er un Dashboard d'Analyse

Cr√©ons un dashboard pour visualiser les anomalies d√©tect√©es:

```bash
# Cr√©er un index pattern pour les alertes Watcher
# Dans Kibana: Stack Management ‚Üí Data Views ‚Üí Create data view
# Name: watcher-alerts
# Index pattern: watcher-alerts
# Time field: alert_triggered_at
```

Visualisations recommand√©es:

**Viz 1: Timeline des Anomalies**
```
Type: Line chart
X-axis: alert_triggered_at (Date histogram, interval: auto)
Y-axis: Count of alerts
Break down by: severity
```

**Viz 2: Indexes les Plus Affect√©s**
```
Type: Table
Rows: affected_indexes.index.keyword
Metrics: 
  - Count (nombre d'occurrences)
  - Avg affected_indexes.drop_percent
  - Latest alert_triggered_at
Sort by: Count (descending)
```

**Viz 3: Comparaison Rates**
```
Type: Bar chart (horizontal)
Y-axis: affected_indexes.index.keyword
X-axis: 
  - affected_indexes.recent_rate (s√©rie 1)
  - affected_indexes.baseline_rate (s√©rie 2)
```

### √âtape 6: Am√©lioration - Ajouter une Action Slack

Si vous avez un workspace Slack, ajoutez une action Slack:

1. Dans Slack, cr√©ez une Incoming Webhook: https://api.slack.com/messaging/webhooks
2. Ajoutez cette action √† votre watch:

```json
"notify_slack": {
  "throttle_period": "15m",
  "webhook": {
    "scheme": "https",
    "host": "hooks.slack.com",
    "port": 443,
    "path": "/services/YOUR/WEBHOOK/PATH",
    "method": "post",
    "headers": {
      "Content-Type": "application/json"
    },
    "body": """
{
  "text": ":warning: *Indexing Rate Anomaly Detected*",
  "blocks": [
    {
      "type": "header",
      "text": {
        "type": "plain_text",
        "text": ":rotating_light: Indexing Rate Anomaly Alert"
      }
    },
    {
      "type": "section",
      "fields": [
        {
          "type": "mrkdwn",
          "text": "*Severity:*\n{{ctx.payload.severity}}"
        },
        {
          "type": "mrkdwn",
          "text": "*Affected Indexes:*\n{{ctx.payload.total_affected}}"
        }
      ]
    },
    {
      "type": "section",
      "text": {
        "type": "mrkdwn",
        "text": "*Details:*\n{{#ctx.payload.affected_indexes}}- `{{index}}`: ‚Üì{{drop_percent}}% ({{recent_rate}} ‚Üí {{baseline_rate}} docs/min)\n{{/ctx.payload.affected_indexes}}"
      }
    },
    {
      "type": "actions",
      "elements": [
        {
          "type": "button",
          "text": {
            "type": "plain_text",
            "text": "View in Kibana"
          },
          "url": "https://your-kibana.com/app/watcher"
        }
      ]
    }
  ]
}
    """
  }
}
```

### Validation Finale

V√©rifiez tous les composants:

```bash
# 1. √âtat de la watch
GET _watcher/watch/indexing-rate-anomaly

# 2. Historique d'ex√©cution
GET .watcher-history*/_search
{
  "query": {
    "match": {
      "watch_id": "indexing-rate-anomaly"
    }
  },
  "sort": [
    { "result.execution_time": "desc" }
  ],
  "size": 10
}

# 3. Statistiques sur les alertes d√©clench√©es
GET watcher-alerts/_search
{
  "size": 0,
  "aggs": {
    "by_severity": {
      "terms": {
        "field": "severity.keyword"
      }
    },
    "total_affected_indexes": {
      "sum": {
        "field": "total_affected"
      }
    },
    "avg_drop_percent": {
      "nested": {
        "path": "affected_indexes"
      },
      "aggs": {
        "avg_drop": {
          "avg": {
            "field": "affected_indexes.drop_percent"
          }
        }
      }
    }
  }
}

# 4. D√©sactiver/Activer la watch
POST _watcher/watch/indexing-rate-anomaly/_deactivate
POST _watcher/watch/indexing-rate-anomaly/_activate

# 5. Supprimer la watch (si n√©cessaire)
DELETE _watcher/watch/indexing-rate-anomaly
```

### D√©fis Suppl√©mentaires (Si Temps Disponible)

**Challenge 1**: Ajouter une d√©tection de "surge" (augmentation soudaine du taux d'indexation > 200%)

**Challenge 2**: Impl√©menter une logique d'auto-r√©solution qui envoie une notification quand les taux reviennent √† la normale

**Challenge 3**: Cr√©er une seconde watch qui surveille le taux de r√©ussite des actions (webhooks, emails) de la premi√®re watch

### Points Cl√©s √† Retenir

‚úÖ **Watcher** offre une flexibilit√© maximale avec le scripting Painless
‚úÖ Les **agr√©gations complexes** permettent des comparaisons baseline vs recent
‚úÖ Le **bucket_script** calcule des m√©triques d√©riv√©es (ratios, pourcentages)
‚úÖ Les **scripts Painless** dans conditions permettent une logique m√©tier sophistiqu√©e
‚úÖ Les **transforms** reformatent les donn√©es avant les actions
‚úÖ Les **actions multiples** (index + webhook + email) assurent la r√©silience
‚úÖ Le **throttling** √©vite l'alert fatigue avec des p√©riodes diff√©rentes par action
‚úÖ Les **templates HTML** cr√©ent des emails riches et actionnables
‚úÖ L'**indexation des alertes** permet l'analyse historique et les dashboards
‚úÖ La **validation progressive** (test query ‚Üí execute watch ‚Üí monitor) assure la fiabilit√©

**F√©licitations!** Vous ma√Ætrisez maintenant les syst√®mes d'alertes avanc√©s d'Elasticsearch! üéâ


---


## Lab 7.1: Cr√©ation d'Utilisateurs et de R√¥les

**Objectif**: Ma√Ætriser la cr√©ation et la gestion d'utilisateurs et de r√¥les avec diff√©rents niveaux de privil√®ges, en impl√©mentant le principe du moindre privil√®ge (least privilege).

**Contexte**: Le contr√¥le d'acc√®s bas√© sur les r√¥les (RBAC) est fondamental pour s√©curiser Elasticsearch. Dans ce lab, vous allez cr√©er plusieurs r√¥les avec des privil√®ges vari√©s, cr√©er des utilisateurs, et tester les restrictions d'acc√®s.

### Pr√©requis : S√©curit√© Activ√©e

V√©rifiez que la s√©curit√© est activ√©e sur votre cluster :

```bash
GET /_xpack
```

**R√©sultat attendu** :
```json
{
  "features": {
    "security": {
      "available": true,
      "enabled": true
    }
  }
}
```

Si la s√©curit√© n'est pas activ√©e (Elasticsearch 7.x), ajoutez dans `elasticsearch.yml` :

```yaml
xpack.security.enabled: true
```

Puis red√©marrez Elasticsearch.

### √âtape 1: V√©rifier l'Utilisateur Actuel

Commencez par v√©rifier avec quel utilisateur vous √™tes connect√© :

```bash
GET /_security/_authenticate
```

**R√©sultat attendu** :
```json
{
  "username": "elastic",
  "roles": ["superuser"],
  "full_name": null,
  "email": null,
  "metadata": {
    "_reserved": true
  },
  "enabled": true,
  "authentication_realm": {
    "name": "reserved",
    "type": "reserved"
  }
}
```

Vous devriez √™tre connect√© avec l'utilisateur `elastic` (superuser).

### √âtape 2: Cr√©er un R√¥le "Lecture Seule" (Read-Only)

Cr√©ons un r√¥le qui permet uniquement la lecture des indices de logs :

```bash
POST /_security/role/logs_readonly
{
  "cluster": ["monitor"],
  "indices": [
    {
      "names": ["logs-*", "filebeat-*", "logstash-*"],
      "privileges": ["read", "view_index_metadata"]
    }
  ],
  "applications": [],
  "run_as": [],
  "metadata": {
    "version": 1,
    "description": "Read-only access to logs indices"
  }
}
```

**R√©sultat attendu** :
```json
{
  "role": {
    "created": true
  }
}
```

**Explication des privil√®ges** :
- `cluster: ["monitor"]` : Peut voir les stats du cluster (_cluster/health, _cat/*, etc.)
- `indices.privileges: ["read"]` : Peut rechercher et lire les documents
- `view_index_metadata` : Peut voir les mappings et settings

**V√©rifier le r√¥le cr√©√©** :

```bash
GET /_security/role/logs_readonly
```

### √âtape 3: Cr√©er un R√¥le "Analyste de Donn√©es"

Cr√©ons un r√¥le pour un analyste qui peut lire et cr√©er des visualisations :

```bash
POST /_security/role/data_analyst
{
  "cluster": ["monitor", "manage_index_templates"],
  "indices": [
    {
      "names": ["products", "orders", "customers"],
      "privileges": ["read", "view_index_metadata"]
    },
    {
      "names": [".kibana*", ".kibana-*"],
      "privileges": ["read", "write", "manage"]
    }
  ],
  "applications": [
    {
      "application": "kibana-.kibana",
      "privileges": ["feature_discover.all", "feature_visualize.all", "feature_dashboard.read"],
      "resources": ["*"]
    }
  ],
  "metadata": {
    "description": "Data analyst with read access to business data and Kibana visualization capabilities"
  }
}
```

**Nouveaux privil√®ges** :
- `manage_index_templates` : Peut cr√©er des index patterns dans Kibana
- Acc√®s aux indices `.kibana*` pour sauvegarder les visualisations
- Privil√®ges Kibana : `discover.all`, `visualize.all`, `dashboard.read`

### √âtape 4: Cr√©er un R√¥le "D√©veloppeur"

Cr√©ons un r√¥le pour un d√©veloppeur avec acc√®s complet √† ses indices de test :

```bash
POST /_security/role/developer
{
  "cluster": ["monitor", "manage_index_templates", "manage_ilm", "manage_pipeline"],
  "indices": [
    {
      "names": ["dev-*", "test-*"],
      "privileges": ["all"]
    },
    {
      "names": ["products", "orders"],
      "privileges": ["read", "view_index_metadata"]
    }
  ],
  "applications": [
    {
      "application": "kibana-.kibana",
      "privileges": ["all"],
      "resources": ["space:dev"]
    }
  ],
  "metadata": {
    "description": "Developer with full access to dev/test indices"
  }
}
```

**Privil√®ges √©tendus** :
- `all` sur indices `dev-*` et `test-*` : Peut tout faire
- `manage_ilm` : Peut g√©rer les Index Lifecycle Management policies
- `manage_pipeline` : Peut g√©rer les ingest pipelines
- Acc√®s complet Kibana dans le space "dev"

### √âtape 5: Cr√©er des Utilisateurs avec Ces R√¥les

**Utilisateur 1 : Lecteur de logs** :

```bash
POST /_security/user/alice_reader
{
  "password": "ReadOnlyPass123!",
  "roles": ["logs_readonly"],
  "full_name": "Alice Reader",
  "email": "alice@example.com",
  "metadata": {
    "department": "Operations",
    "hire_date": "2024-01-15"
  }
}
```

**Utilisateur 2 : Analyste** :

```bash
POST /_security/user/bob_analyst
{
  "password": "AnalystPass456!",
  "roles": ["data_analyst", "kibana_user"],
  "full_name": "Bob Analyst",
  "email": "bob@example.com",
  "metadata": {
    "department": "Data Science",
    "hire_date": "2023-05-10"
  }
}
```

**Utilisateur 3 : D√©veloppeur** :

```bash
POST /_security/user/charlie_dev
{
  "password": "DevPass789!",
  "roles": ["developer"],
  "full_name": "Charlie Developer",
  "email": "charlie@example.com",
  "metadata": {
    "department": "Engineering",
    "hire_date": "2023-03-20"
  }
}
```

**V√©rifier les utilisateurs cr√©√©s** :

```bash
GET /_security/user
```

Vous devriez voir les trois utilisateurs list√©s avec leurs r√¥les.

### √âtape 6: Cr√©er des Indices de Test

Cr√©ons des indices pour tester les permissions :

```bash
# Index de logs
PUT /logs-2024-01
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0
  }
}

POST /logs-2024-01/_bulk
{"index":{"_id":"1"}}
{"timestamp":"2024-01-15T10:00:00Z","level":"INFO","message":"Application started","service":"api"}
{"index":{"_id":"2"}}
{"timestamp":"2024-01-15T10:05:00Z","level":"WARN","message":"High memory usage","service":"api"}
{"index":{"_id":"3"}}
{"timestamp":"2024-01-15T10:10:00Z","level":"ERROR","message":"Database connection failed","service":"database"}

# Index products
PUT /products
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0
  }
}

POST /products/_bulk
{"index":{"_id":"1"}}
{"name":"Laptop","price":999,"category":"electronics","stock":50}
{"index":{"_id":"2"}}
{"name":"Mouse","price":25,"category":"electronics","stock":200}
{"index":{"_id":"3"}}
{"name":"Desk","price":299,"category":"furniture","stock":20}

# Index de d√©veloppement
PUT /dev-feature-x
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0
  }
}

POST /dev-feature-x/_doc/1
{
  "feature": "feature-x",
  "status": "in-development",
  "tests_passing": false
}
```

### √âtape 7: Tester les Permissions de alice_reader (Read-Only)

**Test 1 : Lecture autoris√©e sur logs** :

```bash
# Se connecter comme alice_reader
curl -u alice_reader:ReadOnlyPass123! "https://localhost:9200/logs-2024-01/_search?pretty"
```

**R√©sultat attendu** : Succ√®s (200 OK) avec les 3 documents

**Test 2 : Lecture NON autoris√©e sur products** :

```bash
curl -u alice_reader:ReadOnlyPass123! "https://localhost:9200/products/_search?pretty"
```

**R√©sultat attendu** : Erreur 403 Forbidden
```json
{
  "error": {
    "type": "security_exception",
    "reason": "action [indices:data/read/search] is unauthorized for user [alice_reader]"
  },
  "status": 403
}
```

**Test 3 : √âcriture NON autoris√©e sur logs** :

```bash
curl -u alice_reader:ReadOnlyPass123! -X POST "https://localhost:9200/logs-2024-01/_doc" \
  -H 'Content-Type: application/json' \
  -d '{"timestamp":"2024-01-15T11:00:00Z","level":"INFO","message":"Test"}'
```

**R√©sultat attendu** : Erreur 403 Forbidden (pas de privil√®ge `write`)

**Test 4 : Cluster health autoris√©** :

```bash
curl -u alice_reader:ReadOnlyPass123! "https://localhost:9200/_cluster/health?pretty"
```

**R√©sultat attendu** : Succ√®s (privil√®ge `monitor` permet cela)

### √âtape 8: Tester les Permissions de bob_analyst (Analyste)

**Test 1 : Lecture autoris√©e sur products et orders** :

```bash
curl -u bob_analyst:AnalystPass456! "https://localhost:9200/products/_search?pretty"
```

**R√©sultat attendu** : Succ√®s (200 OK)

**Test 2 : √âcriture NON autoris√©e sur products** :

```bash
curl -u bob_analyst:AnalystPass456! -X POST "https://localhost:9200/products/_doc" \
  -H 'Content-Type: application/json' \
  -d '{"name":"New Product","price":100}'
```

**R√©sultat attendu** : Erreur 403 Forbidden (r√¥le `data_analyst` n'a que `read`)

**Test 3 : Lecture NON autoris√©e sur dev-* (indices de dev)** :

```bash
curl -u bob_analyst:AnalystPass456! "https://localhost:9200/dev-feature-x/_search?pretty"
```

**R√©sultat attendu** : Erreur 403 Forbidden

### √âtape 9: Tester les Permissions de charlie_dev (D√©veloppeur)

**Test 1 : Acc√®s complet aux indices dev-*** :

```bash
# Lecture
curl -u charlie_dev:DevPass789! "https://localhost:9200/dev-feature-x/_search?pretty"

# √âcriture
curl -u charlie_dev:DevPass789! -X POST "https://localhost:9200/dev-feature-x/_doc" \
  -H 'Content-Type: application/json' \
  -d '{"feature":"feature-y","status":"planned"}'

# Suppression
curl -u charlie_dev:DevPass789! -X DELETE "https://localhost:9200/dev-feature-x"
```

**R√©sultat attendu** : Tous succ√®s (privil√®ge `all` sur `dev-*`)

**Test 2 : Cr√©ation d'index de test** :

```bash
curl -u charlie_dev:DevPass789! -X PUT "https://localhost:9200/test-new-feature"
```

**R√©sultat attendu** : Succ√®s (peut cr√©er des indices `test-*`)

**Test 3 : Lecture autoris√©e mais √©criture NON autoris√©e sur products** :

```bash
# Lecture : OK
curl -u charlie_dev:DevPass789! "https://localhost:9200/products/_search?pretty"

# √âcriture : FORBIDDEN
curl -u charlie_dev:DevPass789! -X POST "https://localhost:9200/products/_doc" \
  -H 'Content-Type: application/json' \
  -d '{"name":"Hacked"}'
```

**R√©sultat attendu** : 
- Lecture : Succ√®s
- √âcriture : Erreur 403 (pas de privil√®ge `write` sur `products`)

### √âtape 10: Modifier un Utilisateur

Imaginons que Bob devient "Senior Analyst" et a besoin d'acc√®s en √©criture :

**Cr√©er un nouveau r√¥le** :

```bash
POST /_security/role/senior_analyst
{
  "cluster": ["monitor", "manage_index_templates", "manage_ilm"],
  "indices": [
    {
      "names": ["products", "orders", "customers"],
      "privileges": ["read", "write", "view_index_metadata"]
    },
    {
      "names": [".kibana*"],
      "privileges": ["all"]
    }
  ]
}
```

**Mettre √† jour Bob avec le nouveau r√¥le** :

```bash
PUT /_security/user/bob_analyst
{
  "roles": ["senior_analyst", "kibana_admin"],
  "full_name": "Bob Senior Analyst",
  "email": "bob@example.com"
}
```

**Tester le nouvel acc√®s** :

```bash
# Maintenant Bob peut √©crire
curl -u bob_analyst:AnalystPass456! -X POST "https://localhost:9200/products/_doc" \
  -H 'Content-Type: application/json' \
  -d '{"name":"New Product","price":150,"category":"electronics"}'
```

**R√©sultat attendu** : Succ√®s (201 Created)

### √âtape 11: D√©sactiver Temporairement un Utilisateur

D√©sactivons Alice temporairement (ex: cong√©, investigation s√©curit√©) :

```bash
PUT /_security/user/alice_reader/_disable
```

**R√©sultat attendu** :
```json
{
  "acknowledged": true
}
```

**Tester que Alice ne peut plus se connecter** :

```bash
curl -u alice_reader:ReadOnlyPass123! "https://localhost:9200/_cluster/health"
```

**R√©sultat attendu** : Erreur 401 Unauthorized

**R√©activer Alice** :

```bash
PUT /_security/user/alice_reader/_enable
```

### √âtape 12: Changer le Mot de Passe

Changeons le mot de passe de Charlie :

```bash
POST /_security/user/charlie_dev/_password
{
  "password": "NewDevPassword2024!"
}
```

**R√©sultat attendu** :
```json
{
  "acknowledged": true
}
```

**V√©rifier que l'ancien mot de passe ne fonctionne plus** :

```bash
# Ancien password : FAIL
curl -u charlie_dev:DevPass789! "https://localhost:9200/"

# Nouveau password : SUCCESS
curl -u charlie_dev:NewDevPassword2024! "https://localhost:9200/"
```

### Validation Finale

V√©rifiez que vous avez r√©ussi le lab :

```bash
# 1. Lister tous les r√¥les personnalis√©s
GET /_security/role/logs_readonly,data_analyst,developer,senior_analyst

# 2. Lister tous les utilisateurs
GET /_security/user

# 3. V√©rifier les privil√®ges de chaque utilisateur via _authenticate
# (se connecter avec chaque utilisateur et ex√©cuter GET /_security/_authenticate)

# 4. Tester les acc√®s (matrice de tests)
```

**Matrice de tests attendus** :

| Utilisateur | Index logs-* | Index products | Index dev-* | √âcriture logs-* | √âcriture products |
|-------------|--------------|----------------|-------------|-----------------|-------------------|
| alice_reader | ‚úÖ Read | ‚ùå Denied | ‚ùå Denied | ‚ùå Denied | ‚ùå Denied |
| bob_analyst (apr√®s update) | ‚ùå Denied | ‚úÖ Read/Write | ‚ùå Denied | ‚ùå Denied | ‚úÖ Write |
| charlie_dev | ‚ùå Denied | ‚úÖ Read | ‚úÖ All | ‚ùå Denied | ‚ùå Denied |

### Points Cl√©s √† Retenir

‚úÖ **Privil√®ges cluster** vs **privil√®ges index** : Bien comprendre la diff√©rence  
‚úÖ **Principe du moindre privil√®ge** : Donner uniquement les acc√®s n√©cessaires  
‚úÖ `read` permet `_search`, `_get` mais pas `_index`, `_update`, `_delete`  
‚úÖ `write` permet `_index`, `_update`, `_delete` mais pas cr√©ation d'index  
‚úÖ `all` donne tous les privil√®ges sur les indices cibl√©s  
‚úÖ Les patterns (`logs-*`, `dev-*`) permettent de couvrir plusieurs indices  
‚úÖ Les utilisateurs peuvent avoir **plusieurs r√¥les** (cumul des privil√®ges)  
‚úÖ `_disable` / `_enable` permettent de d√©sactiver temporairement sans supprimer  
‚úÖ Tester syst√©matiquement les acc√®s apr√®s cr√©ation de r√¥les  
‚úÖ Utiliser `_security/_authenticate` pour v√©rifier l'utilisateur actuel

---

## Lab 7.2: Impl√©mentation de Document-Level Security (DLS)

**Objectif**: Mettre en ≈ìuvre la s√©curit√© au niveau des documents pour filtrer les donn√©es visibles selon le r√¥le de l'utilisateur, en utilisant des requ√™tes Elasticsearch.

**Contexte**: La Document-Level Security (DLS) permet de limiter les documents visibles √† un utilisateur selon une query Elasticsearch. C'est essentiel pour impl√©menter du multi-tenancy, s√©parer les donn√©es par d√©partement, r√©gion, ou niveau de confidentialit√©.

### Sc√©nario

Vous g√©rez un cluster Elasticsearch pour une entreprise multi-r√©gionale avec plusieurs d√©partements :
- **D√©partement Sales** : Acc√®s uniquement aux commandes de vente
- **D√©partement HR** : Acc√®s uniquement aux employ√©s
- **Managers r√©gionaux** : Acc√®s uniquement aux donn√©es de leur r√©gion

Vous allez impl√©menter des filtres DLS pour chaque cas d'usage.

### √âtape 1: Cr√©er les Indices de Test avec Donn√©es Multi-Tenant

**Index 1 : Commandes avec d√©partement** :

```bash
PUT /orders
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0
  },
  "mappings": {
    "properties": {
      "order_id": { "type": "keyword" },
      "customer": { "type": "keyword" },
      "amount": { "type": "float" },
      "department": { "type": "keyword" },
      "region": { "type": "keyword" },
      "status": { "type": "keyword" },
      "created_at": { "type": "date" }
    }
  }
}

POST /orders/_bulk
{"index":{"_id":"1"}}
{"order_id":"ORD-001","customer":"Alice Corp","amount":5000,"department":"sales","region":"EMEA","status":"completed","created_at":"2024-01-15T10:00:00Z"}
{"index":{"_id":"2"}}
{"order_id":"ORD-002","customer":"Bob LLC","amount":3000,"department":"sales","region":"AMER","status":"pending","created_at":"2024-01-16T10:00:00Z"}
{"index":{"_id":"3"}}
{"order_id":"ORD-003","customer":"Charlie Inc","amount":7500,"department":"marketing","region":"EMEA","status":"completed","created_at":"2024-01-17T10:00:00Z"}
{"index":{"_id":"4"}}
{"order_id":"ORD-004","customer":"David Co","amount":2000,"department":"sales","region":"APAC","status":"completed","created_at":"2024-01-18T10:00:00Z"}
{"index":{"_id":"5"}}
{"order_id":"ORD-005","customer":"Eve Enterprises","amount":9000,"department":"marketing","region":"AMER","status":"pending","created_at":"2024-01-19T10:00:00Z"}

# Index 2 : Employ√©s avec r√©gion et niveau de confidentialit√©
PUT /employees
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0
  },
  "mappings": {
    "properties": {
      "employee_id": { "type": "keyword" },
      "name": { "type": "keyword" },
      "department": { "type": "keyword" },
      "region": { "type": "keyword" },
      "salary": { "type": "float" },
      "confidentiality": { "type": "keyword" },
      "hire_date": { "type": "date" }
    }
  }
}

POST /employees/_bulk
{"index":{"_id":"1"}}
{"employee_id":"EMP-001","name":"Alice Johnson","department":"sales","region":"EMEA","salary":60000,"confidentiality":"public","hire_date":"2020-01-15"}
{"index":{"_id":"2"}}
{"employee_id":"EMP-002","name":"Bob Smith","department":"hr","region":"EMEA","salary":55000,"confidentiality":"restricted","hire_date":"2021-03-20"}
{"index":{"_id":"3"}}
{"employee_id":"EMP-003","name":"Charlie Brown","department":"engineering","region":"AMER","salary":85000,"confidentiality":"public","hire_date":"2019-05-10"}
{"index":{"_id":"4"}}
{"employee_id":"EMP-004","name":"David Lee","department":"sales","region":"APAC","salary":65000,"confidentiality":"public","hire_date":"2022-07-01"}
{"index":{"_id":"5"}}
{"employee_id":"EMP-005","name":"Eve Martinez","department":"hr","region":"AMER","salary":75000,"confidentiality":"confidential","hire_date":"2018-11-15"}
```

### √âtape 2: Cr√©er un R√¥le avec DLS pour le D√©partement Sales

Ce r√¥le permet de voir **uniquement** les commandes du d√©partement "sales" :

```bash
POST /_security/role/sales_team
{
  "cluster": ["monitor"],
  "indices": [
    {
      "names": ["orders"],
      "privileges": ["read", "view_index_metadata"],
      "query": {
        "term": {
          "department": "sales"
        }
      }
    }
  ],
  "metadata": {
    "description": "Sales team - can only see sales department orders"
  }
}
```

**Explication** :
- `query.term.department: "sales"` : Filtre qui n'affiche que les documents o√π `department = "sales"`
- Les documents avec `department = "marketing"` sont **invisibles** pour ce r√¥le

### √âtape 3: Cr√©er un R√¥le avec DLS pour Manager R√©gional EMEA

Ce r√¥le permet de voir **uniquement** les donn√©es de la r√©gion EMEA :

```bash
POST /_security/role/emea_manager
{
  "cluster": ["monitor"],
  "indices": [
    {
      "names": ["orders", "employees"],
      "privileges": ["read", "view_index_metadata"],
      "query": {
        "term": {
          "region": "EMEA"
        }
      }
    }
  ],
  "metadata": {
    "description": "EMEA regional manager - can only see EMEA region data"
  }
}
```

### √âtape 4: Cr√©er un R√¥le avec DLS Complexe (Plusieurs Conditions)

Ce r√¥le permet de voir les commandes "sales" **ET** statut "completed" :

```bash
POST /_security/role/sales_completed
{
  "cluster": ["monitor"],
  "indices": [
    {
      "names": ["orders"],
      "privileges": ["read"],
      "query": {
        "bool": {
          "must": [
            { "term": { "department": "sales" } },
            { "term": { "status": "completed" } }
          ]
        }
      }
    }
  ],
  "metadata": {
    "description": "Sales team - only completed sales orders"
  }
}
```

**Query DLS** : Combine plusieurs conditions avec `bool.must`

### √âtape 5: Cr√©er un R√¥le pour HR avec Filtrage par Confidentialit√©

Le d√©partement HR peut voir tous les employ√©s **sauf** les "confidential" :

```bash
POST /_security/role/hr_team
{
  "cluster": ["monitor"],
  "indices": [
    {
      "names": ["employees"],
      "privileges": ["read", "write", "view_index_metadata"],
      "query": {
        "bool": {
          "must_not": [
            { "term": { "confidentiality": "confidential" } }
          ]
        }
      }
    }
  ],
  "metadata": {
    "description": "HR team - cannot see confidential employee records"
  }
}
```

**Query DLS** : Utilise `bool.must_not` pour exclure des documents

### √âtape 6: Cr√©er des Utilisateurs avec R√¥les DLS

```bash
# Utilisateur sales team
POST /_security/user/sarah_sales
{
  "password": "SalesPass123!",
  "roles": ["sales_team"],
  "full_name": "Sarah Sales",
  "email": "sarah@example.com"
}

# Utilisateur EMEA manager
POST /_security/user/michael_emea
{
  "password": "EMEAPass456!",
  "roles": ["emea_manager"],
  "full_name": "Michael EMEA Manager",
  "email": "michael@example.com"
}

# Utilisateur sales completed
POST /_security/user/tom_audit
{
  "password": "AuditPass789!",
  "roles": ["sales_completed"],
  "full_name": "Tom Auditor",
  "email": "tom@example.com"
}

# Utilisateur HR
POST /_security/user/helen_hr
{
  "password": "HRPass321!",
  "roles": ["hr_team"],
  "full_name": "Helen HR",
  "email": "helen@example.com"
}
```

### √âtape 7: Tester le Filtrage DLS pour Sales Team

**Connexion en tant que sarah_sales** :

```bash
curl -u sarah_sales:SalesPass123! "https://localhost:9200/orders/_search?pretty"
```

**R√©sultat attendu** :
```json
{
  "hits": {
    "total": { "value": 3 },
    "hits": [
      {
        "_source": {
          "order_id": "ORD-001",
          "department": "sales",
          "region": "EMEA",
          ...
        }
      },
      {
        "_source": {
          "order_id": "ORD-002",
          "department": "sales",
          "region": "AMER",
          ...
        }
      },
      {
        "_source": {
          "order_id": "ORD-004",
          "department": "sales",
          "region": "APAC",
          ...
        }
      }
    ]
  }
}
```

**Analyse** :
- ‚úÖ Voit ORD-001, ORD-002, ORD-004 (department = "sales")
- ‚ùå Ne voit **PAS** ORD-003, ORD-005 (department = "marketing")

**Compter les documents visibles** :

```bash
curl -u sarah_sales:SalesPass123! "https://localhost:9200/orders/_count?pretty"
```

**R√©sultat attendu** : `{ "count": 3 }`

### √âtape 8: Tester le Filtrage DLS pour EMEA Manager

**Connexion en tant que michael_emea** :

```bash
curl -u michael_emea:EMEAPass456! "https://localhost:9200/orders/_search?pretty"
```

**R√©sultat attendu** :
```json
{
  "hits": {
    "total": { "value": 2 },
    "hits": [
      {
        "_source": {
          "order_id": "ORD-001",
          "region": "EMEA",
          "department": "sales",
          ...
        }
      },
      {
        "_source": {
          "order_id": "ORD-003",
          "region": "EMEA",
          "department": "marketing",
          ...
        }
      }
    ]
  }
}
```

**Analyse** :
- ‚úÖ Voit ORD-001, ORD-003 (region = "EMEA")
- ‚ùå Ne voit **PAS** ORD-002, ORD-004, ORD-005 (autres r√©gions)

**Tester sur l'index employees** :

```bash
curl -u michael_emea:EMEAPass456! "https://localhost:9200/employees/_search?pretty"
```

**R√©sultat attendu** : Employ√©s EMP-001 et EMP-002 uniquement (region = "EMEA")

### √âtape 9: Tester le Filtrage DLS avec Conditions Multiples

**Connexion en tant que tom_audit** (sales + completed) :

```bash
curl -u tom_audit:AuditPass789! "https://localhost:9200/orders/_search?pretty"
```

**R√©sultat attendu** :
```json
{
  "hits": {
    "total": { "value": 2 },
    "hits": [
      {
        "_source": {
          "order_id": "ORD-001",
          "department": "sales",
          "status": "completed",
          ...
        }
      },
      {
        "_source": {
          "order_id": "ORD-004",
          "department": "sales",
          "status": "completed",
          ...
        }
      }
    ]
  }
}
```

**Analyse** :
- ‚úÖ Voit ORD-001, ORD-004 (sales + completed)
- ‚ùå Ne voit **PAS** ORD-002 (sales mais pending)
- ‚ùå Ne voit **PAS** ORD-003, ORD-005 (marketing)

### √âtape 10: Tester le Filtrage DLS avec Exclusion (HR Team)

**Connexion en tant que helen_hr** :

```bash
curl -u helen_hr:HRPass321! "https://localhost:9200/employees/_search?pretty"
```

**R√©sultat attendu** :
```json
{
  "hits": {
    "total": { "value": 4 },
    "hits": [
      { "_source": { "employee_id": "EMP-001", "confidentiality": "public" } },
      { "_source": { "employee_id": "EMP-002", "confidentiality": "restricted" } },
      { "_source": { "employee_id": "EMP-003", "confidentiality": "public" } },
      { "_source": { "employee_id": "EMP-004", "confidentiality": "public" } }
    ]
  }
}
```

**Analyse** :
- ‚úÖ Voit EMP-001, 002, 003, 004 (public ou restricted)
- ‚ùå Ne voit **PAS** EMP-005 (confidentiality = "confidential")

### √âtape 11: V√©rifier l'Invisibilit√© Compl√®te (Get par ID)

M√™me si on conna√Æt l'ID d'un document filtr√© par DLS, il est inaccessible :

```bash
# Sarah (sales team) essaie d'acc√©der √† ORD-003 (marketing)
curl -u sarah_sales:SalesPass123! "https://localhost:9200/orders/_doc/3?pretty"
```

**R√©sultat attendu** : Erreur 404 Not Found
```json
{
  "_index": "orders",
  "_id": "3",
  "found": false
}
```

Le document existe mais est **invisible** pour sarah_sales (comme s'il n'existait pas).

### √âtape 12: Tester les Agr√©gations avec DLS

Les agr√©gations respectent √©galement le filtrage DLS :

```bash
# Agr√©ger par r√©gion (vue sarah_sales)
curl -u sarah_sales:SalesPass123! -X GET "https://localhost:9200/orders/_search?pretty" \
  -H 'Content-Type: application/json' \
  -d '{
  "size": 0,
  "aggs": {
    "by_region": {
      "terms": {
        "field": "region"
      }
    }
  }
}'
```

**R√©sultat attendu** :
```json
{
  "aggregations": {
    "by_region": {
      "buckets": [
        { "key": "EMEA", "doc_count": 1 },
        { "key": "AMER", "doc_count": 1 },
        { "key": "APAC", "doc_count": 1 }
      ]
    }
  }
}
```

**Analyse** : Uniquement les r√©gions des commandes "sales" (3 documents au total).

### Validation Finale

V√©rifiez que vous avez r√©ussi le lab :

```bash
# 1. V√©rifier les r√¥les DLS cr√©√©s
GET /_security/role/sales_team,emea_manager,sales_completed,hr_team

# 2. Pour chaque utilisateur, v√©rifier le count
curl -u sarah_sales:SalesPass123! "https://localhost:9200/orders/_count"
# Attendu: {"count": 3}

curl -u michael_emea:EMEAPass456! "https://localhost:9200/orders/_count"
# Attendu: {"count": 2}

curl -u tom_audit:AuditPass789! "https://localhost:9200/orders/_count"
# Attendu: {"count": 2}

curl -u helen_hr:HRPass321! "https://localhost:9200/employees/_count"
# Attendu: {"count": 4}

# 3. Comparer avec superuser (voit tout)
curl -u elastic:your_password "https://localhost:9200/orders/_count"
# Attendu: {"count": 5}
```

### Points Cl√©s √† Retenir

‚úÖ **DLS filtre les documents** visibles selon une query Elasticsearch  
‚úÖ La query DLS est **transparente** pour l'utilisateur (documents invisibles comme s'ils n'existaient pas)  
‚úÖ M√™me avec `GET /_doc/{id}`, un document filtr√© retourne **404 Not Found**  
‚úÖ Les **agr√©gations** et **statistiques** respectent le filtrage DLS  
‚úÖ `term` query pour filtrage exact, `bool` pour conditions complexes  
‚úÖ `must`, `must_not`, `should` permettent des filtres sophistiqu√©s  
‚úÖ DLS fonctionne avec **tous les patterns d'indices** (`orders-*`, etc.)  
‚úÖ Combiner DLS avec Field-Level Security pour protection maximale  
‚úÖ Tester syst√©matiquement avec `_count` et `_search` apr√®s cr√©ation de r√¥les DLS  
‚úÖ DLS est id√©al pour **multi-tenancy**, **s√©paration d√©partementale**, **filtrage r√©gional**

---

## üåü Bonus Challenge 7.A: Field-Level Security (FLS) pour Masquer des Champs Sensibles

**Niveau**: Avanc√©  
**Objectif**: Impl√©menter la s√©curit√© au niveau des champs (Field-Level Security) pour cacher des donn√©es sensibles selon les r√¥les, en combinant avec DLS pour une protection multicouche.

**Contexte**: Certaines donn√©es dans vos indices sont sensibles (SSN, salaires, emails personnels, donn√©es m√©dicales). La Field-Level Security permet de les masquer compl√®tement pour certains r√¥les, m√™me si l'utilisateur peut voir le document.

### Sc√©nario

Vous g√©rez un cluster avec des donn√©es d'employ√©s contenant :
- **Donn√©es publiques** : Nom, d√©partement, date d'embauche
- **Donn√©es sensibles** : SSN, salaire, adresse personnelle, num√©ro de t√©l√©phone
- **Donn√©es confidentielles** : √âvaluations de performance, notes disciplinaires

Vous allez cr√©er plusieurs niveaux d'acc√®s :
1. **Public** : Peut voir uniquement les champs publics
2. **HR Team** : Peut voir public + certaines donn√©es sensibles (pas SSN)
3. **HR Manager** : Peut voir tout (public + sensible + confidentiel)

### √âtape 1: Cr√©er un Index d'Employ√©s Enrichi

```bash
PUT /employees_full
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0
  },
  "mappings": {
    "properties": {
      "employee_id": { "type": "keyword" },
      "name": { "type": "keyword" },
      "department": { "type": "keyword" },
      "position": { "type": "keyword" },
      "hire_date": { "type": "date" },
      "email_corporate": { "type": "keyword" },
      "email_personal": { "type": "keyword" },
      "phone_work": { "type": "keyword" },
      "phone_personal": { "type": "keyword" },
      "address": {
        "properties": {
          "street": { "type": "text" },
          "city": { "type": "keyword" },
          "country": { "type": "keyword" },
          "postal_code": { "type": "keyword" }
        }
      },
      "ssn": { "type": "keyword" },
      "salary": { "type": "float" },
      "performance_review": {
        "properties": {
          "rating": { "type": "keyword" },
          "comments": { "type": "text" },
          "reviewer": { "type": "keyword" }
        }
      },
      "disciplinary_notes": { "type": "text" }
    }
  }
}
```

### √âtape 2: Indexer des Donn√©es de Test

```bash
POST /employees_full/_bulk
{"index":{"_id":"1"}}
{"employee_id":"EMP-001","name":"Alice Johnson","department":"sales","position":"Sales Manager","hire_date":"2020-01-15","email_corporate":"alice.johnson@company.com","email_personal":"alice.j@gmail.com","phone_work":"+33-1-23-45-67-89","phone_personal":"+33-6-12-34-56-78","address":{"street":"10 Rue de Rivoli","city":"Paris","country":"France","postal_code":"75001"},"ssn":"123-45-6789","salary":75000,"performance_review":{"rating":"excellent","comments":"Top performer","reviewer":"Director Sales"},"disciplinary_notes":null}
{"index":{"_id":"2"}}
{"employee_id":"EMP-002","name":"Bob Smith","department":"hr","position":"HR Specialist","hire_date":"2021-03-20","email_corporate":"bob.smith@company.com","email_personal":"bob.smith@yahoo.com","phone_work":"+33-1-98-76-54-32","phone_personal":"+33-6-98-76-54-32","address":{"street":"25 Avenue des Champs","city":"Lyon","country":"France","postal_code":"69001"},"ssn":"987-65-4321","salary":60000,"performance_review":{"rating":"good","comments":"Solid contributor","reviewer":"HR Director"},"disciplinary_notes":"Late arrival incident - 2023-05-10"}
{"index":{"_id":"3"}}
{"employee_id":"EMP-003","name":"Charlie Brown","department":"engineering","position":"Senior Engineer","hire_date":"2019-05-10","email_corporate":"charlie.brown@company.com","email_personal":"cbrown@outlook.com","phone_work":"+33-1-11-22-33-44","phone_personal":"+33-6-11-22-33-44","address":{"street":"5 Boulevard Saint-Germain","city":"Paris","country":"France","postal_code":"75005"},"ssn":"555-12-3456","salary":95000,"performance_review":{"rating":"excellent","comments":"Technical leader","reviewer":"CTO"},"disciplinary_notes":null}
```

### √âtape 3: Cr√©er un R√¥le "Public" avec FLS Restrictif

Ce r√¥le ne peut voir que les champs publics :

```bash
POST /_security/role/employee_public_view
{
  "cluster": ["monitor"],
  "indices": [
    {
      "names": ["employees_full"],
      "privileges": ["read"],
      "field_security": {
        "grant": [
          "employee_id",
          "name",
          "department",
          "position",
          "hire_date",
          "email_corporate",
          "phone_work"
        ]
      }
    }
  ],
  "metadata": {
    "description": "Public view - only non-sensitive employee data"
  }
}
```

**Champs accord√©s** : ID, nom, d√©partement, poste, date d'embauche, email pro, t√©l√©phone pro  
**Champs cach√©s** : SSN, salaire, adresse, emails/t√©l√©phones persos, √©valuations, notes disciplinaires

### √âtape 4: Cr√©er un R√¥le "HR Team" avec FLS Mod√©r√©

Ce r√¥le peut voir plus de champs mais pas les plus sensibles (SSN, notes disciplinaires) :

```bash
POST /_security/role/hr_team_view
{
  "cluster": ["monitor"],
  "indices": [
    {
      "names": ["employees_full"],
      "privileges": ["read", "write"],
      "field_security": {
        "grant": [
          "employee_id",
          "name",
          "department",
          "position",
          "hire_date",
          "email_*",
          "phone_*",
          "address.*",
          "salary",
          "performance_review.*"
        ],
        "except": [
          "ssn",
          "disciplinary_notes"
        ]
      }
    }
  ],
  "metadata": {
    "description": "HR team - can see most fields except SSN and disciplinary notes"
  }
}
```

**Utilisation de wildcards** :
- `email_*` : Accorde `email_corporate` ET `email_personal`
- `phone_*` : Accorde `phone_work` ET `phone_personal`
- `address.*` : Accorde tous les sous-champs de `address`
- `performance_review.*` : Tous les sous-champs des √©valuations

**Champs explicitement exclus** :
- `ssn` : Num√©ro de s√©curit√© sociale
- `disciplinary_notes` : Notes disciplinaires

### √âtape 5: Cr√©er un R√¥le "HR Manager" avec Acc√®s Complet

Ce r√¥le peut voir TOUS les champs sans restriction :

```bash
POST /_security/role/hr_manager_full
{
  "cluster": ["monitor", "manage"],
  "indices": [
    {
      "names": ["employees_full"],
      "privileges": ["all"],
      "field_security": {
        "grant": ["*"]
      }
    }
  ],
  "metadata": {
    "description": "HR Manager - full access to all employee data"
  }
}
```

**Grant `["*"]`** : Accorde tous les champs sans exception

### √âtape 6: Cr√©er des Utilisateurs avec Ces R√¥les

```bash
# Utilisateur public
POST /_security/user/intern_view
{
  "password": "InternPass123!",
  "roles": ["employee_public_view"],
  "full_name": "Intern Viewer"
}

# Utilisateur HR team
POST /_security/user/jane_hr
{
  "password": "HRPass456!",
  "roles": ["hr_team_view"],
  "full_name": "Jane HR Specialist"
}

# Utilisateur HR manager
POST /_security/user/susan_hrmanager
{
  "password": "ManagerPass789!",
  "roles": ["hr_manager_full"],
  "full_name": "Susan HR Manager"
}
```

### √âtape 7: Tester FLS - Vue Publique (Intern)

```bash
curl -u intern_view:InternPass123! "https://localhost:9200/employees_full/_search?pretty"
```

**R√©sultat attendu** :
```json
{
  "hits": {
    "hits": [
      {
        "_source": {
          "employee_id": "EMP-001",
          "name": "Alice Johnson",
          "department": "sales",
          "position": "Sales Manager",
          "hire_date": "2020-01-15",
          "email_corporate": "alice.johnson@company.com",
          "phone_work": "+33-1-23-45-67-89"
        }
      },
      ...
    ]
  }
}
```

**Analyse** :
- ‚úÖ Voit : `employee_id`, `name`, `department`, `position`, `hire_date`, `email_corporate`, `phone_work`
- ‚ùå Ne voit **PAS** : `email_personal`, `phone_personal`, `address`, `ssn`, `salary`, `performance_review`, `disciplinary_notes`

### √âtape 8: Tester FLS - Vue HR Team

```bash
curl -u jane_hr:HRPass456! "https://localhost:9200/employees_full/_doc/1?pretty"
```

**R√©sultat attendu** :
```json
{
  "_source": {
    "employee_id": "EMP-001",
    "name": "Alice Johnson",
    "department": "sales",
    "position": "Sales Manager",
    "hire_date": "2020-01-15",
    "email_corporate": "alice.johnson@company.com",
    "email_personal": "alice.j@gmail.com",
    "phone_work": "+33-1-23-45-67-89",
    "phone_personal": "+33-6-12-34-56-78",
    "address": {
      "street": "10 Rue de Rivoli",
      "city": "Paris",
      "country": "France",
      "postal_code": "75001"
    },
    "salary": 75000,
    "performance_review": {
      "rating": "excellent",
      "comments": "Top performer",
      "reviewer": "Director Sales"
    }
  }
}
```

**Analyse** :
- ‚úÖ Voit : Tous les champs publics + emails/t√©l√©phones persos + adresse + salaire + √©valuations
- ‚ùå Ne voit **PAS** : `ssn`, `disciplinary_notes` (exclus explicitement)

### √âtape 9: Tester FLS - Vue HR Manager (Full Access)

```bash
curl -u susan_hrmanager:ManagerPass789! "https://localhost:9200/employees_full/_doc/2?pretty"
```

**R√©sultat attendu** :
```json
{
  "_source": {
    "employee_id": "EMP-002",
    "name": "Bob Smith",
    "department": "hr",
    "position": "HR Specialist",
    "hire_date": "2021-03-20",
    "email_corporate": "bob.smith@company.com",
    "email_personal": "bob.smith@yahoo.com",
    "phone_work": "+33-1-98-76-54-32",
    "phone_personal": "+33-6-98-76-54-32",
    "address": {
      "street": "25 Avenue des Champs",
      "city": "Lyon",
      "country": "France",
      "postal_code": "69001"
    },
    "ssn": "987-65-4321",
    "salary": 60000,
    "performance_review": {
      "rating": "good",
      "comments": "Solid contributor",
      "reviewer": "HR Director"
    },
    "disciplinary_notes": "Late arrival incident - 2023-05-10"
  }
}
```

**Analyse** :
- ‚úÖ Voit **TOUT** : Tous les champs y compris `ssn` et `disciplinary_notes`

### √âtape 10: Combiner DLS + FLS

Cr√©ons un r√¥le qui combine filtrage de documents ET de champs :

```bash
POST /_security/role/sales_dept_restricted
{
  "cluster": ["monitor"],
  "indices": [
    {
      "names": ["employees_full"],
      "privileges": ["read"],
      "query": {
        "term": {
          "department": "sales"
        }
      },
      "field_security": {
        "grant": [
          "employee_id",
          "name",
          "department",
          "position",
          "email_corporate",
          "phone_work"
        ]
      }
    }
  ],
  "metadata": {
    "description": "Sales department view - only sales employees, limited fields"
  }
}
```

**Double protection** :
- **DLS** : Filtre les documents (`department = "sales"` uniquement)
- **FLS** : Filtre les champs (champs publics uniquement)

### √âtape 11: Tester DLS + FLS Combin√©s

```bash
# Cr√©er l'utilisateur
POST /_security/user/sales_viewer
{
  "password": "SalesView123!",
  "roles": ["sales_dept_restricted"]
}

# Tester la recherche
curl -u sales_viewer:SalesView123! "https://localhost:9200/employees_full/_search?pretty"
```

**R√©sultat attendu** :
```json
{
  "hits": {
    "total": { "value": 1 },
    "hits": [
      {
        "_source": {
          "employee_id": "EMP-001",
          "name": "Alice Johnson",
          "department": "sales",
          "position": "Sales Manager",
          "email_corporate": "alice.johnson@company.com",
          "phone_work": "+33-1-23-45-67-89"
        }
      }
    ]
  }
}
```

**Analyse** :
- ‚úÖ Voit uniquement EMP-001 (seul employ√© "sales")
- ‚ùå Ne voit **PAS** EMP-002 (hr) ni EMP-003 (engineering) ‚Üí DLS
- ‚úÖ Champs limit√©s aux publics ‚Üí FLS

### √âtape 12: Tester FLS avec Agr√©gations

Les agr√©gations respectent √©galement FLS :

```bash
# Avec intern_view (pas acc√®s √† salary)
curl -u intern_view:InternPass123! -X GET "https://localhost:9200/employees_full/_search?pretty" \
  -H 'Content-Type: application/json' \
  -d '{
  "size": 0,
  "aggs": {
    "avg_salary": {
      "avg": {
        "field": "salary"
      }
    }
  }
}'
```

**R√©sultat attendu** : Erreur ou r√©sultat vide (le champ `salary` est invisible)

```bash
# Avec jane_hr (acc√®s √† salary)
curl -u jane_hr:HRPass456! -X GET "https://localhost:9200/employees_full/_search?pretty" \
  -H 'Content-Type: application/json' \
  -d '{
  "size": 0,
  "aggs": {
    "avg_salary": {
      "avg": {
        "field": "salary"
      }
    }
  }
}'
```

**R√©sultat attendu** :
```json
{
  "aggregations": {
    "avg_salary": {
      "value": 76666.67
    }
  }
}
```

### Validation Finale

```bash
# 1. V√©rifier les r√¥les FLS
GET /_security/role/employee_public_view,hr_team_view,hr_manager_full,sales_dept_restricted

# 2. Comparer les champs visibles pour chaque utilisateur
# intern_view : 7 champs
# jane_hr : ~13 champs (sauf ssn, disciplinary_notes)
# susan_hrmanager : TOUS les champs

# 3. V√©rifier la combinaison DLS + FLS
curl -u sales_viewer:SalesView123! "https://localhost:9200/employees_full/_count"
# Attendu: {"count": 1} (seulement Alice de sales)
```

### Points Cl√©s √† Retenir

‚úÖ **FLS cache compl√®tement les champs** (comme s'ils n'existaient pas dans le document)  
‚úÖ `grant` liste les champs **autoris√©s**, `except` liste les champs **exclus**  
‚úÖ **Wildcards** (`email_*`, `address.*`) permettent des patterns flexibles  
‚úÖ **Nested fields** utilisent la notation point (`performance_review.rating`)  
‚úÖ **DLS + FLS combin√©s** offrent une protection multicouche  
‚úÖ Les **agr√©gations** sur champs cach√©s √©chouent ou retournent vide  
‚úÖ M√™me avec `GET /_doc/{id}`, les champs cach√©s sont **absents du _source**  
‚úÖ FLS est **appliqu√© au niveau du shard** pour performance optimale  
‚úÖ Utiliser `grant: ["*"]` pour acc√®s complet √† tous les champs  
‚úÖ Tester syst√©matiquement avec diff√©rents r√¥les pour valider les restrictions

**F√©licitations !** Vous ma√Ætrisez maintenant la s√©curit√© avanc√©e d'Elasticsearch avec RBAC, DLS, et FLS ! üéâ


---


